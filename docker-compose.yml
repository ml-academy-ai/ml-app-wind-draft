# =============================================================================
# PRODUCTION DOCKER COMPOSE
# =============================================================================
# This file is for production deployment and uses images from DockerHub.
# For local development, use docker-compose.local.yml which builds from source.
#
# Usage: 
#   - Set DOCKERHUB_USERNAME environment variable
#   - Set KEDRO_VIZ_URI and MLFLOW_UI_URI environment variables (or create .env file)
#   - docker compose pull
#   - docker compose up -d

services:
  # =============================================================================
  # DATA STREAMING SERVICE
  # =============================================================================
  # This service streams data point-by-point to the database
  # It simulates real-time data ingestion for inference
  app-stream-data:
    image: ${DOCKERHUB_USERNAME}/ml-app-wind-draft:latest
    command: ["python", "entrypoint/app_stream_data.py"]
    volumes:
      - ./data:/app/data
      - ./conf:/app/conf
    networks:
      - ml-app-network
    restart: unless-stopped
    depends_on:
      - mlflow

  # =============================================================================
  # MLFLOW TRACKING SERVER
  # =============================================================================
  # This service provides MLflow for experiment tracking and model registry
  # Uses local file storage only (no database, stores everything in files)
  mlflow:
    image: ${DOCKERHUB_USERNAME}/ml-app-wind-draft:latest
    command: >
      sh -c "mlflow server 
      --host 0.0.0.0 
      --port 5001 
      --default-artifact-root file:///app/mlflow/mlartifacts
      --allowed-hosts '*'
      --cors-allowed-origins '*'"
    ports:
      - "5001:5001"
    volumes:
      - ./mlflow:/app/mlflow
      - ./data:/app/data
      - ./conf:/app/conf
    networks:
      - ml-app-network
    restart: unless-stopped

  # =============================================================================
  # WEB USER INTERFACE SERVICE
  # =============================================================================
  # This service provides the web dashboard for visualizing predictions
  # It displays real-time predictions, error metrics, and model information
  app-ui:
    image: ${DOCKERHUB_USERNAME}/ml-app-wind-draft:latest
    command: ["python", "entrypoint/app_ui.py"]
    ports:
      - "8050:8050"
    volumes:
      - ./data:/app/data
      - ./conf:/app/conf
      - ./mlflow:/app/mlflow
    environment:
      - KEDRO_ENV=local
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow:5001}
      - MLFLOW_UI_URI=${MLFLOW_UI_URI}
      - DEBUG=False
      - KEDRO_VIZ_URI=${KEDRO_VIZ_URI}
    networks:
      - ml-app-network
    restart: unless-stopped
    depends_on:
      - mlflow

  # =============================================================================
  # ML MODEL TRAINING SERVICE
  # =============================================================================
  # This service trains the ML model using the training data
  # It runs once to create the model, then the inference service uses it
  app-ml-train:
    image: ${DOCKERHUB_USERNAME}/ml-app-wind-draft:latest
    command: ["python", "entrypoint/training.py"]
    volumes:
      - ./data:/app/data
      - ./conf:/app/conf
      - ./mlflow:/app/mlflow
    environment:
      - KEDRO_ENV=local
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow:5001}
    networks:
      - ml-app-network
    restart: "no"
    depends_on:
      - mlflow
      - app-ui

  # =============================================================================
  # ML INFERENCE SERVICE
  # =============================================================================
  # This service runs inference on new data after training completes
  # It monitors for new data and runs inference pipeline automatically
  app-ml-inference:
    image: ${DOCKERHUB_USERNAME}/ml-app-wind-draft:latest
    command: ["python", "entrypoint/inference_real_time.py"]
    volumes:
      - ./data:/app/data
      - ./conf:/app/conf
      - ./mlflow:/app/mlflow
    environment:
      - KEDRO_ENV=local
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow:5001}
    networks:
      - ml-app-network
    restart: unless-stopped
    depends_on:
      app-ml-train:
        condition: service_completed_successfully

  # =============================================================================
  # KEDRO VISUALIZATION SERVER
  # =============================================================================
  # This service provides Kedro Viz for pipeline visualization
  # It shows the data pipeline structure and dependencies
  kedro-viz:
    image: ${DOCKERHUB_USERNAME}/ml-app-wind-draft:latest
    command: ["kedro", "viz", "--host", "0.0.0.0", "--port", "4141"]
    ports:
      - "4141:4141"
    networks:
      - ml-app-network
    restart: unless-stopped
    depends_on:
      - app-ml-inference

# =============================================================================
# NETWORK CONFIGURATION
# =============================================================================
networks:
  ml-app-network:
    driver: bridge
