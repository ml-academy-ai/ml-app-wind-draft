{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a512b9ec-3873-4da8-a16a-605ea755808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from typing import Tuple, Union, List, Dict, Mapping, Any, Literal, Optional\n",
    "from catboost import CatBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import torch\n",
    "import optuna\n",
    "from pathlib import Path\n",
    "from utils import plot_errors, eval_model\n",
    "import utils\n",
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bb33af-4272-43a0-9a74-1367ee0ffb78",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Setting up MLflow](#Setting-up-MLflow)\n",
    "2. [First experiment: RF baseline](#First-experiment:-RF-baseline)\n",
    "3. [Cleaned data: Random Forest](#Cleaned-data:-Random-Forest)\n",
    "4. [Cleaned data: CatBoost](#Cleaned-data:-CatBoost)\n",
    "5. [Cleaned Data: MLP Neural Network](#Cleaned-Data:-MLP-Neural-Network)\n",
    "6. [Cleaned Data: LSTM](#Cleaned-Data:-LSTM)\n",
    "7. [Child runs intro](#Child-runs-intro)\n",
    "8. [CatBoost Bayesian Hyperparameter Tuning with Child Runs](#CatBoost-Bayesian-Hyperparameter-Tuning-with-Child-Runs)\n",
    "9. [Logging important artifacts](#Logging-important-artifacts)\n",
    "10. [Logging and loading the best CatBoost Optuna model with X_scaler](#Logging-and-loading-the-best-CatBoost-Optuna-model-with-X_scaler)\n",
    "11. [Testing the best model on Production Data](#Testing-the-best-model-on-Production-Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20830f84-f705-4775-ac43-f8478124a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e780b763-1a97-4110-87f4-2378ef32eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diff_outliers(df, column, diff_threshold):\n",
    "    \"\"\"\n",
    "    Remove outliers based on absolute first-order diff and forward-fill the gaps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    column : str\n",
    "        Column to clean.\n",
    "    diff_threshold : float\n",
    "        Absolute diff threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_clean : pd.DataFrame\n",
    "        Cleaned dataframe with forward fill.\n",
    "    outlier_idx : pd.Index\n",
    "        Indices of removed outliers.\n",
    "    \"\"\"\n",
    "\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. Compute absolute diff\n",
    "    diff_vals = df_clean[column].diff(1).abs()\n",
    "\n",
    "    # 2. Outlier mask\n",
    "    outlier_mask = diff_vals > diff_threshold\n",
    "    outlier_idx = df_clean.index[outlier_mask]\n",
    "\n",
    "    # 3. Remove outliers\n",
    "    df_clean.loc[outlier_idx, column] = np.nan\n",
    "\n",
    "    # 4. Forward fill (and backfill if needed)\n",
    "    df_clean[column] = df_clean[column].ffill().bfill()\n",
    "\n",
    "    return df_clean, outlier_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb7bc2ac-29a9-4d35-a1f3-e5202430d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_signal(df, column, window, method=\"median\"):\n",
    "    \"\"\"\n",
    "    Smooth a time-series column using rolling mean or median.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    column : str\n",
    "        Column to smooth.\n",
    "    window : int\n",
    "        Rolling window size.\n",
    "    method : str\n",
    "        \"mean\"  -> rolling mean filter\n",
    "        \"median\" -> rolling median filter (robust smoothing)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_smoothed : pd.DataFrame\n",
    "        DataFrame with smoothed column.\n",
    "    \"\"\"\n",
    "\n",
    "    df_smoothed = df.copy()\n",
    "\n",
    "    if method == \"mean\":\n",
    "        df_smoothed[column] = df_smoothed[column].rolling(\n",
    "            window=window, min_periods=1, center=False\n",
    "        ).mean()\n",
    "\n",
    "    elif method == \"median\":\n",
    "        df_smoothed[column] = df_smoothed[column].rolling(\n",
    "            window=window, min_periods=1, center=False\n",
    "        ).median()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'mean' or 'median'\")\n",
    "\n",
    "    return df_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016d3d18-398e-4240-85b3-6d1cb58312f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(df: pd.DataFrame, columns:List[str]=None, lags: List[int]=[1], drop_na=True):\n",
    "    \"\"\"\n",
    "    Add lag features to DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - columns: list of column names (default: all numeric)\n",
    "    - lags: int or list of lag periods (default: 1)\n",
    "    - drop_na: bool, drop NaN rows (default: True)\n",
    "    \n",
    "    Returns: DataFrame with lag features\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for col in columns:\n",
    "        for lag in lags:\n",
    "            df_result[f\"{col}_lag{lag}\"] = df_result[col].shift(lag)\n",
    "    \n",
    "    if drop_na:\n",
    "        return df_result.dropna()\n",
    "    else:\n",
    "        return df_result.bfill()  # Backward fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c05c462c-292c-420e-ad48-780041dcb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    abs_diff_thresholds: Dict[str, float],\n",
    "    smooth_window: int\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Clean and enrich train/test turbine datasets:\n",
    "    - remove outliers using absolute-difference thresholds,\n",
    "    - denoise selected signals via FFT low-pass filter,\n",
    "    - add lag features,\n",
    "    - add rolling statistical features.\n",
    "\n",
    "    Assumptions\n",
    "    ----------\n",
    "    - `df_train` and `df_test` contain a target column named 'Power'.\n",
    "    - `remove_diff_outliers` does NOT drop rows (it forward-fills / smooths values).\n",
    "    - Columns required for denoising and feature engineering are present:\n",
    "      ['GenRPM', 'GenPh1Temp', 'WindSpeed', 'WindDirAbs',\n",
    "       'WindDirRel', 'Pitch', 'RotorRPM'].\n",
    "    - Helper functions `remove_diff_outliers`, `fft_lowpass_filter`,\n",
    "      `add_lag_features`, and `add_rolling_features` are defined elsewhere.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pd.DataFrame\n",
    "        Training dataset with features and target ('Power').\n",
    "    df_test : pd.DataFrame\n",
    "        Test dataset with features and target ('Power').\n",
    "    abs_diff_thresholds : Dict[str, float]\n",
    "        Mapping from column name to absolute-difference threshold used by\n",
    "        `remove_diff_outliers` to smooth outliers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : pd.DataFrame\n",
    "        Cleaned and feature-engineered training features (without 'Power').\n",
    "    y_train : pd.Series\n",
    "        Cleaned training target ('Power').\n",
    "    X_test : pd.DataFrame\n",
    "        Cleaned and feature-engineered test features (without 'Power').\n",
    "    y_test : pd.Series\n",
    "        Original test target ('Power'), not filtered by outlier logic.\n",
    "    \"\"\"\n",
    "    df_filtered_train = df_train.copy()\n",
    "\n",
    "    # Applying filter by difference\n",
    "    # Remove outliers on the train set (features + target)\n",
    "    for col, thr in abs_diff_thresholds.items():\n",
    "        df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)\n",
    "\n",
    "    # Remove outliers on the test set (features only)\n",
    "    filt_cols = [col for col in df_test.columns if col != \"Power\"]\n",
    "    x_filtered_test = df_test[filt_cols].copy()\n",
    "\n",
    "    for col, thr in abs_diff_thresholds.items():\n",
    "        if col != \"Power\":\n",
    "            x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)\n",
    "\n",
    "    # Denoising\n",
    "    smooth_cols = [col for col in df_filtered_train.columns if col != 'Power']\n",
    "\n",
    "    for col in smooth_cols:\n",
    "        df_filtered_train = smooth_signal(df_filtered_train, col, window=smooth_window, method=\"mean\")\n",
    "    \n",
    "    for col in smooth_cols:\n",
    "        x_filtered_test = smooth_signal(x_filtered_test, col, window=smooth_window, method=\"mean\")\n",
    "\n",
    "\n",
    "    # Add Lag Features in train and test datasets\n",
    "    df_filtered_train = add_lag_features(\n",
    "        df_filtered_train,\n",
    "        columns=[\"GenRPM\", \"GenPh1Temp\", 'WindSpeed'],\n",
    "        lags=[1, 2, 3],\n",
    "        drop_na=False,\n",
    "    )\n",
    "    x_filtered_test = add_lag_features(\n",
    "        x_filtered_test,\n",
    "        columns=[\"GenRPM\", \"GenPh1Temp\", 'WindSpeed'],\n",
    "        lags=[1, 2, 3],\n",
    "        drop_na=False,\n",
    "    )\n",
    "\n",
    "    # Add Statistical Features in the train dataset\n",
    "    df_filtered_train = add_rolling_features(\n",
    "        df_filtered_train,\n",
    "        window_sizes=[3, 6],  # e.g. [5, 10, 15]\n",
    "        columns=[\"GenRPM\", \"GenPh1Temp\", \"WindSpeed\",\n",
    "                 \"WindDirAbs\", \"WindDirRel\", \"Pitch\", \"RotorRPM\"],\n",
    "        stats=[\"max\"],\n",
    "        drop_na=False,\n",
    "    )\n",
    "\n",
    "    # Add Statistical Features in the test dataset\n",
    "    x_filtered_test = add_rolling_features(\n",
    "        x_filtered_test,\n",
    "        window_sizes=[3, 6],  # e.g. [5, 10, 15]\n",
    "        columns=[\"GenRPM\", \"GenPh1Temp\", \"WindSpeed\",\n",
    "                 \"WindDirAbs\", \"WindDirRel\", \"Pitch\", \"RotorRPM\"],\n",
    "        stats=[\"max\"],\n",
    "        drop_na=False,\n",
    "    )\n",
    "\n",
    "    X_train = df_filtered_train.drop(columns=\"Power\")\n",
    "    y_train = df_filtered_train[\"Power\"]\n",
    "    X_test = x_filtered_test\n",
    "    y_test = df_test[\"Power\"]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d825579b-d36b-4d40-9449-54be25d49bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(df, columns, window_sizes=7, stats=['mean', 'median'], drop_na=True):\n",
    "    \"\"\"\n",
    "    Add rolling features to DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - columns: list of column names\n",
    "    - window_sizes: int or list of window sizes (default: 7)\n",
    "    - stats: list of statistics ['mean', 'median', 'std', 'min', 'max', 'skew', 'kurt']\n",
    "    - drop_na: bool, drop NaN rows (default: True)\n",
    "    \n",
    "    Returns: DataFrame with rolling features\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Convert single values to lists\n",
    "    if isinstance(window_sizes, int):\n",
    "        window_sizes = [window_sizes]\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    \n",
    "    # Create rolling features\n",
    "    for col in columns:\n",
    "        for window in window_sizes:\n",
    "            rolling = df_result[col].rolling(window)\n",
    "            \n",
    "            for stat in stats:\n",
    "                if stat == 'mean':\n",
    "                    df_result[f\"{col}_roll{window}_mean\"] = rolling.mean()\n",
    "                elif stat == 'median':\n",
    "                    df_result[f\"{col}_roll{window}_median\"] = rolling.median()\n",
    "                elif stat == 'std':\n",
    "                    df_result[f\"{col}_roll{window}_std\"] = rolling.std()\n",
    "                elif stat == 'min':\n",
    "                    df_result[f\"{col}_roll{window}_min\"] = rolling.min()\n",
    "                elif stat == 'max':\n",
    "                    df_result[f\"{col}_roll{window}_max\"] = rolling.max()\n",
    "                elif stat == 'skew':\n",
    "                    df_result[f\"{col}_roll{window}_skew\"] = rolling.skew()\n",
    "                elif stat == 'kurt':\n",
    "                    df_result[f\"{col}_roll{window}_kurt\"] = rolling.kurt()\n",
    "    if drop_na:\n",
    "        return df_result.dropna()\n",
    "    else:\n",
    "        return df_result.bfill()  # Backward fill NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a064223-10c7-475e-aecd-9bae90b237ea",
   "metadata": {},
   "source": [
    "**In the terminal, navigate to project root folder and run in the terminal (command line):**\n",
    "1. mkdir mlflow\n",
    "2. cd mlflow\n",
    "3. mlflow server --host 127.0.0.1 --port 8080\n",
    "\n",
    "**This is what it does:**\n",
    "\n",
    "1. **mkdir mlflow** creates mlflow directory where we will store the models and experiments.\n",
    "\n",
    "2. **cd mlflow** changes the directory to mlflow\n",
    "\n",
    "3. Starts the MLflow Tracking Server\n",
    "A dedicated process that manages and serves your MLflow experiments.\n",
    "\n",
    "4. Provides a Web UI\n",
    "Accessible at http://127.0.0.1:8080 (or localhost:8080), where you can browse experiments, runs, parameters, metrics, and artifacts.\n",
    "\n",
    "5. Exposes a Tracking API Endpoint\n",
    "Other scripts or notebooks can log directly to this server if you set - **mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")**\n",
    "\n",
    "6. MLflow automatically creates a folder mlruns/ in your working directory the first time you log something.\n",
    "Inside mlruns/, it creates subfolders for:\n",
    "\n",
    "- each experiment (default is 0)\n",
    "\n",
    "- each run within that experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd6522-0245-4583-accf-1b27c58ea77f",
   "metadata": {},
   "source": [
    "# Setting up MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c8aa0-0b1a-471e-9d0b-39a09ce395fc",
   "metadata": {},
   "source": [
    "### Setting URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7384755-4370-4e02-8c07-e2170704aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tracking uri\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed6c30",
   "metadata": {},
   "source": [
    "What it does:\n",
    "\n",
    "1. It tells your MLflow client (your script/notebook) where to send all logging data (experiments, runs, params, metrics, artifacts).\n",
    "\n",
    "3. Since our host is local, it will still write to mlruns, but here you could configure a remote host URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf7ea4-2c2d-4ec1-9ee5-82152447c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current Tracking URI:\", mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928ce83",
   "metadata": {},
   "source": [
    "# First experiment: RF baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae1084",
   "metadata": {},
   "source": [
    "To create an experiment, we need to run the command:\n",
    "\n",
    "mlflow.set_experiment(\"Baseline Anomaly Model\")\n",
    "\n",
    "If we do so, in the UI (http://127.0.0.1:8080), we will see that the experiment is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb376f49-e4ce-4309-8acd-eb45553dd430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name - it also creates an experiment if it doesn't exist\n",
    "mlflow.set_experiment(\"Baseline Anomaly Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adb697a-fa2f-49fb-b6df-c14495453793",
   "metadata": {},
   "source": [
    "Let's log the very first model that we got on the raw data and log our first experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36880761-4af8-4375-afd0-669d0e5d8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "# df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05610e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamps</th>\n",
       "      <th>WindSpeed</th>\n",
       "      <th>WindDirAbs</th>\n",
       "      <th>WindDirRel</th>\n",
       "      <th>Power</th>\n",
       "      <th>Pitch</th>\n",
       "      <th>GenRPM</th>\n",
       "      <th>RotorRPM</th>\n",
       "      <th>EnvirTemp</th>\n",
       "      <th>NacelTemp</th>\n",
       "      <th>GearOilTemp</th>\n",
       "      <th>GearBearTemp</th>\n",
       "      <th>GenPh1Temp</th>\n",
       "      <th>GenBearTemp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamps</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-07-29 03:10:00</th>\n",
       "      <td>2007-07-29 03:10:00</td>\n",
       "      <td>4.790142</td>\n",
       "      <td>307.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>520.300000</td>\n",
       "      <td>-1.483813</td>\n",
       "      <td>1078.376427</td>\n",
       "      <td>16.229237</td>\n",
       "      <td>17.866438</td>\n",
       "      <td>59.882623</td>\n",
       "      <td>58.059329</td>\n",
       "      <td>49.079118</td>\n",
       "      <td>79.193312</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-29 03:20:00</th>\n",
       "      <td>2007-07-29 03:20:00</td>\n",
       "      <td>4.285207</td>\n",
       "      <td>312.5</td>\n",
       "      <td>2.2</td>\n",
       "      <td>564.700000</td>\n",
       "      <td>1.831067</td>\n",
       "      <td>841.497573</td>\n",
       "      <td>16.686210</td>\n",
       "      <td>21.404675</td>\n",
       "      <td>15.233595</td>\n",
       "      <td>46.598905</td>\n",
       "      <td>45.249571</td>\n",
       "      <td>95.670331</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-29 03:30:00</th>\n",
       "      <td>2007-07-29 03:30:00</td>\n",
       "      <td>7.143066</td>\n",
       "      <td>304.4</td>\n",
       "      <td>-4.9</td>\n",
       "      <td>1632.407112</td>\n",
       "      <td>-4.402055</td>\n",
       "      <td>1022.534164</td>\n",
       "      <td>15.149165</td>\n",
       "      <td>16.315175</td>\n",
       "      <td>30.932959</td>\n",
       "      <td>49.956838</td>\n",
       "      <td>55.738552</td>\n",
       "      <td>102.969851</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-29 03:40:00</th>\n",
       "      <td>2007-07-29 03:40:00</td>\n",
       "      <td>9.469090</td>\n",
       "      <td>299.5</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>571.600000</td>\n",
       "      <td>-8.720774</td>\n",
       "      <td>867.724340</td>\n",
       "      <td>12.674883</td>\n",
       "      <td>17.626694</td>\n",
       "      <td>17.668583</td>\n",
       "      <td>45.713568</td>\n",
       "      <td>46.011476</td>\n",
       "      <td>91.759158</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-07-29 03:50:00</th>\n",
       "      <td>2007-07-29 03:50:00</td>\n",
       "      <td>3.997540</td>\n",
       "      <td>313.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>553.000000</td>\n",
       "      <td>-2.014152</td>\n",
       "      <td>1048.110983</td>\n",
       "      <td>18.668083</td>\n",
       "      <td>17.984032</td>\n",
       "      <td>31.084279</td>\n",
       "      <td>50.049534</td>\n",
       "      <td>67.344350</td>\n",
       "      <td>90.274146</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-22 10:20:00</th>\n",
       "      <td>2008-02-22 10:20:00</td>\n",
       "      <td>2.168136</td>\n",
       "      <td>301.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>524.900000</td>\n",
       "      <td>-2.657893</td>\n",
       "      <td>929.297145</td>\n",
       "      <td>13.759142</td>\n",
       "      <td>8.362282</td>\n",
       "      <td>38.902156</td>\n",
       "      <td>49.957331</td>\n",
       "      <td>49.694407</td>\n",
       "      <td>76.873304</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-22 10:30:00</th>\n",
       "      <td>2008-02-22 10:30:00</td>\n",
       "      <td>8.533812</td>\n",
       "      <td>301.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>531.300000</td>\n",
       "      <td>1.474806</td>\n",
       "      <td>904.893779</td>\n",
       "      <td>18.096816</td>\n",
       "      <td>13.002330</td>\n",
       "      <td>15.161773</td>\n",
       "      <td>50.281284</td>\n",
       "      <td>64.106411</td>\n",
       "      <td>81.280686</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-22 10:40:00</th>\n",
       "      <td>2008-02-22 10:40:00</td>\n",
       "      <td>3.595898</td>\n",
       "      <td>299.7</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>558.200000</td>\n",
       "      <td>-4.092066</td>\n",
       "      <td>1002.941924</td>\n",
       "      <td>29.516623</td>\n",
       "      <td>18.171560</td>\n",
       "      <td>29.842036</td>\n",
       "      <td>43.268336</td>\n",
       "      <td>49.106053</td>\n",
       "      <td>85.158141</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-22 10:50:00</th>\n",
       "      <td>2008-02-22 10:50:00</td>\n",
       "      <td>-1.077070</td>\n",
       "      <td>290.6</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>540.300000</td>\n",
       "      <td>-0.432372</td>\n",
       "      <td>955.865234</td>\n",
       "      <td>16.403157</td>\n",
       "      <td>12.481199</td>\n",
       "      <td>32.796415</td>\n",
       "      <td>56.342692</td>\n",
       "      <td>71.150101</td>\n",
       "      <td>92.161368</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-02-22 11:00:00</th>\n",
       "      <td>2008-02-22 11:00:00</td>\n",
       "      <td>4.213347</td>\n",
       "      <td>278.7</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>558.100000</td>\n",
       "      <td>-0.665640</td>\n",
       "      <td>1049.301208</td>\n",
       "      <td>14.551783</td>\n",
       "      <td>15.045643</td>\n",
       "      <td>42.727626</td>\n",
       "      <td>67.260356</td>\n",
       "      <td>73.737341</td>\n",
       "      <td>73.790747</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Timestamps  WindSpeed  WindDirAbs  WindDirRel  \\\n",
       "Timestamps                                                                   \n",
       "2007-07-29 03:10:00 2007-07-29 03:10:00   4.790142       307.5         1.8   \n",
       "2007-07-29 03:20:00 2007-07-29 03:20:00   4.285207       312.5         2.2   \n",
       "2007-07-29 03:30:00 2007-07-29 03:30:00   7.143066       304.4        -4.9   \n",
       "2007-07-29 03:40:00 2007-07-29 03:40:00   9.469090       299.5        -3.1   \n",
       "2007-07-29 03:50:00 2007-07-29 03:50:00   3.997540       313.8         6.7   \n",
       "...                                 ...        ...         ...         ...   \n",
       "2008-02-22 10:20:00 2008-02-22 10:20:00   2.168136       301.2        -0.3   \n",
       "2008-02-22 10:30:00 2008-02-22 10:30:00   8.533812       301.5         0.2   \n",
       "2008-02-22 10:40:00 2008-02-22 10:40:00   3.595898       299.7        -1.1   \n",
       "2008-02-22 10:50:00 2008-02-22 10:50:00  -1.077070       290.6        -3.1   \n",
       "2008-02-22 11:00:00 2008-02-22 11:00:00   4.213347       278.7        -3.0   \n",
       "\n",
       "                           Power     Pitch       GenRPM   RotorRPM  EnvirTemp  \\\n",
       "Timestamps                                                                      \n",
       "2007-07-29 03:10:00   520.300000 -1.483813  1078.376427  16.229237  17.866438   \n",
       "2007-07-29 03:20:00   564.700000  1.831067   841.497573  16.686210  21.404675   \n",
       "2007-07-29 03:30:00  1632.407112 -4.402055  1022.534164  15.149165  16.315175   \n",
       "2007-07-29 03:40:00   571.600000 -8.720774   867.724340  12.674883  17.626694   \n",
       "2007-07-29 03:50:00   553.000000 -2.014152  1048.110983  18.668083  17.984032   \n",
       "...                          ...       ...          ...        ...        ...   \n",
       "2008-02-22 10:20:00   524.900000 -2.657893   929.297145  13.759142   8.362282   \n",
       "2008-02-22 10:30:00   531.300000  1.474806   904.893779  18.096816  13.002330   \n",
       "2008-02-22 10:40:00   558.200000 -4.092066  1002.941924  29.516623  18.171560   \n",
       "2008-02-22 10:50:00   540.300000 -0.432372   955.865234  16.403157  12.481199   \n",
       "2008-02-22 11:00:00   558.100000 -0.665640  1049.301208  14.551783  15.045643   \n",
       "\n",
       "                     NacelTemp  GearOilTemp  GearBearTemp  GenPh1Temp  \\\n",
       "Timestamps                                                              \n",
       "2007-07-29 03:10:00  59.882623    58.059329     49.079118   79.193312   \n",
       "2007-07-29 03:20:00  15.233595    46.598905     45.249571   95.670331   \n",
       "2007-07-29 03:30:00  30.932959    49.956838     55.738552  102.969851   \n",
       "2007-07-29 03:40:00  17.668583    45.713568     46.011476   91.759158   \n",
       "2007-07-29 03:50:00  31.084279    50.049534     67.344350   90.274146   \n",
       "...                        ...          ...           ...         ...   \n",
       "2008-02-22 10:20:00  38.902156    49.957331     49.694407   76.873304   \n",
       "2008-02-22 10:30:00  15.161773    50.281284     64.106411   81.280686   \n",
       "2008-02-22 10:40:00  29.842036    43.268336     49.106053   85.158141   \n",
       "2008-02-22 10:50:00  32.796415    56.342692     71.150101   92.161368   \n",
       "2008-02-22 11:00:00  42.727626    67.260356     73.737341   73.790747   \n",
       "\n",
       "                     GenBearTemp  \n",
       "Timestamps                        \n",
       "2007-07-29 03:10:00         51.0  \n",
       "2007-07-29 03:20:00         52.0  \n",
       "2007-07-29 03:30:00         53.0  \n",
       "2007-07-29 03:40:00         53.0  \n",
       "2007-07-29 03:50:00         53.0  \n",
       "...                          ...  \n",
       "2008-02-22 10:20:00         76.0  \n",
       "2008-02-22 10:30:00         76.0  \n",
       "2008-02-22 10:40:00         76.0  \n",
       "2008-02-22 10:50:00         76.0  \n",
       "2008-02-22 11:00:00         76.0  \n",
       "\n",
       "[30000 rows x 14 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c77e64-fe04-487c-8028-a3841fbab739",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "feats = [col for col in df_train.columns if col != 'Power']\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name='rf_baseline'):\n",
    "    # Train the model and compute the metrics\n",
    "    eval_results  = eval_model(\n",
    "        df_train[feats],\n",
    "        df_train['Power'],\n",
    "        df_test[feats], \n",
    "        df_test['Power'], \n",
    "        3, \n",
    "        'RF', \n",
    "        params\n",
    "    )\n",
    "    # Log (store) the hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    # Log the Cross Validation metrics\n",
    "    mlflow.log_metric(\"cv_mae\", eval_results['cv_mae'])\n",
    "    mlflow.log_metric(\"cv_rmse\", eval_results['cv_rmse'])\n",
    "    mlflow.log_metric(\"cv_mape\", eval_results['cv_mape'])\n",
    "\n",
    "    # Log the Test metrics\n",
    "    mlflow.log_metric(\"test_mae\", eval_results['test_mae'])\n",
    "    mlflow.log_metric(\"test_rmse\", eval_results['test_rmse'])\n",
    "    mlflow.log_metric(\"test_mape\", eval_results['test_mape'])\n",
    "    \n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"model_version\", \"baseline\")\n",
    "\n",
    "    model_name = \"baseline_rf\"\n",
    "    \n",
    "    # Log (store) the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=eval_results['model'],\n",
    "        name=model_name,\n",
    "        input_example=None, # We will add the input example later\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5594999",
   "metadata": {},
   "source": [
    "Now, all we need to do is log the parameters, metrics, and the model.\n",
    "\n",
    "We can go to http://127.0.0.1:8080/ and see the run and experiment in the Tracking Server UI.\n",
    "\n",
    "We can also find the stored data in our file system in the ./mlflow directory.\n",
    "\n",
    "If we check the model size, it's about 250 MB, so Random Forest models are quite heavy, so be careful when storing many of experimental models locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca3d99",
   "metadata": {},
   "source": [
    "# Cleaned data: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439dbaae",
   "metadata": {},
   "source": [
    "Now, let's make a function that logs the most important artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3413c92a-21f2-419f-b700-0e36ad973a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_diff_thresholds = {\n",
    "    'WindSpeed': 12,\n",
    "    'WindDirAbs': 70,\n",
    "    'Power': 200,\n",
    "    'Pitch': 12,\n",
    "    'GenRPM': 450,\n",
    "    'WindDirRel': 9,\n",
    "    'NacelTemp': 40,\n",
    "    'GenPh1Temp': 40,\n",
    "    'RotorRPM': 20,\n",
    "    'EnvirTemp': 17,\n",
    "    'GearOilTemp':  20,  \n",
    "    'GearBearTemp': 35,\n",
    "    'GenBearTemp': 8,\n",
    "    'GenPh1Temp':   35, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63cc02-b8a4-45ec-a240-4e3f6eb07d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bab26d-778b-4971-9e35-6019f78594f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_clean_data(df_train, df_test, abs_diff_thresholds, smooth_window=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e29d46-dd90-4e88-8102-0318b695aa70",
   "metadata": {},
   "source": [
    "To log the models in a similar manner and avoid code repetition, let's create a function that logs the run parameters, models and other meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc38325-7bfc-4ce9-b1b1-faa0de65b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_run(\n",
    "    params: Mapping[str, Any],\n",
    "    metrics: Mapping[str, float],\n",
    "    tags: Mapping[str, Any],\n",
    "    trained_model: Any,\n",
    "    model_type: Literal[\"RF\", \"CatBoost\", \"MLP\", \"LSTM\"] = \"RF\",\n",
    "    input_example: Optional[Any] = None,\n",
    "    registered_model_name: Optional[str] = None,\n",
    "    model_name: str = \"model\",\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Log a single training run to MLflow: params, metrics, tags and the trained model.\n",
    "\n",
    "    This function assumes that an MLflow run has already been started\n",
    "    (e.g. via ``mlflow.start_run()`` or an MLflow context manager).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : Mapping[str, Any]\n",
    "        Hyperparameters and configuration of the run to log via ``mlflow.log_params``.\n",
    "    metrics : Mapping[str, float]\n",
    "        Metric values (e.g., RMSE, MAE) to log via ``mlflow.log_metrics``.\n",
    "    tags : Mapping[str, Any]\n",
    "        Arbitrary metadata tags (e.g., dataset name, model family, experiment info).\n",
    "    trained_model : Any\n",
    "        Fitted model instance to be logged. Must be compatible with the chosen\n",
    "        MLflow flavor (`sklearn`, `catboost`, or `pytorch`).\n",
    "    model_type : {\"RF\", \"CatBoost\", \"MLP\", \"LSTM\"}, default=\"RF\"\n",
    "        High-level model family used to pick the appropriate MLflow logging flavor:\n",
    "        - \"RF\"       → ``mlflow.sklearn.log_model``\n",
    "        - \"CatBoost\" → ``mlflow.catboost.log_model``\n",
    "        - \"MLP\"      → ``mlflow.pytorch.log_model``\n",
    "        - \"LSTM\"     → ``mlflow.pytorch.log_model``\n",
    "    input_example : Any, optional\n",
    "        Example input passed to MLflow for model signature inference and UI preview.\n",
    "    registered_model_name : str, optional\n",
    "        If provided, the model will be registered in the MLflow Model Registry\n",
    "        under this name.\n",
    "    model_name : str, default=\"model\"\n",
    "        Name of the model artifact within the run (e.g., \"model\", \"rf_model\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        The MLflow ``ModelInfo`` object returned by the underlying\n",
    "        ``mlflow.<flavor>.log_model`` call. Can be used to inspect the\n",
    "        logged model's URI and other metadata.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If ``model_type`` is not one of the supported values.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- metadata logging ----\n",
    "    mlflow.log_params(dict(params))\n",
    "    mlflow.log_metrics(dict(metrics))\n",
    "    mlflow.set_tags(dict(tags))\n",
    "\n",
    "    # ---- model logging ----\n",
    "    if model_type == \"RF\":\n",
    "        model_info = mlflow.sklearn.log_model(\n",
    "            sk_model=trained_model,\n",
    "            name=model_name,\n",
    "            input_example=input_example,\n",
    "            registered_model_name=registered_model_name,\n",
    "        )\n",
    "\n",
    "    elif model_type == \"CatBoost\":\n",
    "        model_info = mlflow.catboost.log_model(\n",
    "            cb_model=trained_model,\n",
    "            name=model_name,\n",
    "            input_example=input_example,\n",
    "            registered_model_name=registered_model_name,\n",
    "        )\n",
    "\n",
    "    elif model_type in (\"MLP\", \"LSTM\"):\n",
    "        model_info = mlflow.pytorch.log_model(\n",
    "            pytorch_model=trained_model,\n",
    "            name=model_name,\n",
    "            input_example=input_example,\n",
    "            registered_model_name=registered_model_name,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaaadb6-9762-49f7-a4bb-b4a287ab3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "\n",
    "eval_results  = eval_model(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_test, \n",
    "        y_test, \n",
    "        3, \n",
    "        'RF', \n",
    "        params\n",
    "    )\n",
    "\n",
    "metrics = {\n",
    "    \"cv_mae\": eval_results[\"cv_mae\"],\n",
    "    \"cv_rmse\": eval_results[\"cv_rmse\"],\n",
    "    \"cv_mape\": eval_results[\"cv_mape\"],\n",
    "    \"test_mae\": eval_results[\"test_mae\"],\n",
    "    \"test_rmse\": eval_results[\"test_rmse\"],\n",
    "    \"test_mape\": eval_results[\"test_mape\"],\n",
    "}\n",
    "\n",
    "run_name = 'rf_cleaned_data'\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    tags = {\n",
    "        'model_version': 'cleaned_data',\n",
    "    }\n",
    "    \n",
    "    # Log everything using our function\n",
    "    model_info = log_run(\n",
    "        params=params,\n",
    "        metrics=metrics,\n",
    "        tags=tags,\n",
    "        trained_model=eval_results['model'],\n",
    "        input_example=None,\n",
    "        model_type=\"RF\",\n",
    "        model_name=\"rf_cleaned_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f0368-dba9-4154-b221-e2c03e127f73",
   "metadata": {},
   "source": [
    "# Cleaned data: CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ac6c3-b1af-43cb-8473-44c68d75432d",
   "metadata": {},
   "source": [
    "Now, let's run CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a27b2a-6a25-46f7-8878-d313ac1c6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"iterations\": 200,       # number of trees\n",
    "    \"learning_rate\": 0.05,    \n",
    "    \"depth\": 6,               \n",
    "    \"l2_leaf_reg\": 1.0,      \n",
    "    \"random_seed\": SEED,      \n",
    "    \"loss_function\": \"RMSE\",  \n",
    "    \"verbose\": False       \n",
    "}\n",
    "\n",
    "eval_results = eval_model(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    3,\n",
    "    'CatBoost',\n",
    "    params\n",
    ")\n",
    "\n",
    "metrics = {\n",
    "    \"cv_mae\": eval_results[\"cv_mae\"],\n",
    "    \"cv_rmse\": eval_results[\"cv_rmse\"],\n",
    "    \"cv_mape\": eval_results[\"cv_mape\"],\n",
    "    \"test_mae\": eval_results[\"test_mae\"],\n",
    "    \"test_rmse\": eval_results[\"test_rmse\"],\n",
    "    \"test_mape\": eval_results[\"test_mape\"],\n",
    "}\n",
    "\n",
    "run_name = 'catboost_cleaned_data'\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    tags = {\n",
    "        'model_version': 'cleaned_data',\n",
    "    }\n",
    "    \n",
    "    # Log everything using our function\n",
    "    model_info = log_run(\n",
    "        params=params,\n",
    "        metrics=metrics,\n",
    "        tags=tags,\n",
    "        trained_model=eval_results['model'],\n",
    "        input_example=None,\n",
    "        model_type=\"CatBoost\",\n",
    "        model_name=\"catboost_cleaned_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781ec773-83f2-410a-92e3-aa9cab9827b9",
   "metadata": {},
   "source": [
    "# Cleaned Data: MLP Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526f516-3fa3-412e-89c8-a60df1f24cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_params = {\n",
    "    \"hidden_sizes\": [128, 128, 128],\n",
    "    \"dropout\": 0.2,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 100,\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "eval_results = eval_model(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    n_splits=3,\n",
    "    model_name=\"MLP\",\n",
    "    model_params=mlp_params,\n",
    ")\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    \"cv_mae\": eval_results[\"cv_mae\"],\n",
    "    \"cv_rmse\": eval_results[\"cv_rmse\"],\n",
    "    \"cv_mape\": eval_results[\"cv_mape\"],\n",
    "    \"test_mae\": eval_results[\"test_mae\"],\n",
    "    \"test_rmse\": eval_results[\"test_rmse\"],\n",
    "    \"test_mape\": eval_results[\"test_mape\"],\n",
    "}\n",
    "\n",
    "run_name = 'mlp_cleaned_data'\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    tags = {\n",
    "        'model_version': 'cleaned_data',\n",
    "    }\n",
    "    \n",
    "    # Log everything using our function\n",
    "    model_info = log_run(\n",
    "        params=params,\n",
    "        metrics=metrics,\n",
    "        tags=tags,\n",
    "        trained_model=eval_results['model'],\n",
    "        input_example=None,\n",
    "        model_type=\"MLP\",\n",
    "        model_name=\"mlp_cleaned_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b8d0e-f93e-41b3-8d80-e00d2eee95d6",
   "metadata": {},
   "source": [
    "# Cleaned Data: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc4efe-b7e0-40cd-90bb-1853eeed0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lstm_params = {\n",
    "    \"seq_len\": 48,\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.0,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 2,\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "\n",
    "eval_results = eval_model(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    n_splits=3,\n",
    "    model_name=\"LSTM\",\n",
    "    model_params=lstm_params,\n",
    ")\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    \"cv_mae\": eval_results[\"cv_mae\"],\n",
    "    \"cv_rmse\": eval_results[\"cv_rmse\"],\n",
    "    \"cv_mape\": eval_results[\"cv_mape\"],\n",
    "    \"test_mae\": eval_results[\"test_mae\"],\n",
    "    \"test_rmse\": eval_results[\"test_rmse\"],\n",
    "    \"test_mape\": eval_results[\"test_mape\"],\n",
    "}\n",
    "\n",
    "run_name = 'lstm_cleaned_data'\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    tags = {\n",
    "        'model_version': 'cleaned_data',\n",
    "    }\n",
    "    \n",
    "    # Log everything using our function\n",
    "    model_info = log_run(\n",
    "        params=params,\n",
    "        metrics=metrics,\n",
    "        tags=tags,\n",
    "        trained_model=eval_results['model'],\n",
    "        input_example=None, # we used later, the sliced input must be used here\n",
    "        model_type=\"LSTM\",\n",
    "        model_name=\"LSTM_cleaned_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa1a5e-036d-4482-8fdf-3abee47c2901",
   "metadata": {},
   "source": [
    "# Child runs intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd28a75",
   "metadata": {},
   "source": [
    "MLflow tracks experiments as named groups where all your related runs live. \n",
    "\n",
    "As we have seen up to now, each \"run\" is one training session where you log parameters, metrics, and artifacts. \n",
    "\n",
    "Parent and Child Runs add a hierarchical layer to this setup.\n",
    "\n",
    "**How does it help?**\n",
    "\n",
    "With a parent-child structure, related runs are automatically grouped together. When you're running a hyperparameter search using a Bayesian approach on a particular model architecture, every iteration gets logged as a child run,\n",
    "\n",
    "As your experiments grow in number and complexity, having a nested structure ensures your tracking remains manageable. \n",
    "\n",
    "Navigating through a structured hierarchy is much more efficient than scrolling through a flat list of hundreds or thousands of runs. \n",
    "\n",
    "This becomes particularly valuable as projects scale up.\n",
    "\n",
    "Now, as the concept of Parent and Child runs is clear, let's see how we can implement this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6bca19-e60a-4418-aea5-e5cbac6413a0",
   "metadata": {},
   "source": [
    "**Let  see how this works with CatBoost as an example.**\n",
    "\n",
    "Say you're testing a CatBoost model with different three depth - from 5 to 10.\n",
    "\n",
    "To do that, we create a function log_run_child.\n",
    "\n",
    "This is a very similar function to log_run, but here we change the names of the run at each run and also specify parameter nested=True. This parameter indicates to MLflow that the runs are child runs.\n",
    "\n",
    "Then we specify the parent run, and in a loop we run the child runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea4206-d591-4a98-baa3-5307d7c78b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_run_child(\n",
    "    run_name: str,\n",
    "    params: Mapping[str, Any],\n",
    "    metrics: Mapping[str, float],\n",
    "    tags: Mapping[str, Any],\n",
    "    trained_model: Any,\n",
    "    model_type: Literal[\"RF\", \"CatBoost\", \"MLP\", \"LSTM\"] = \"RF\",\n",
    "    input_example: Optional[Any] = None,\n",
    "    registered_model_name: Optional[str] = None,\n",
    "    iteration: Optional[int] = None,\n",
    ") -> Any:\n",
    "    \"\"\"\n",
    "    Log a nested (child) MLflow run representing one iteration of an experiment.\n",
    "\n",
    "    This function is used inside hyperparameter tuning loops,\n",
    "    model selection experiments, or any parent run where each iteration\n",
    "    should be tracked as its own nested MLflow run.\n",
    "\n",
    "    A unique child run name and model artifact name are created automatically\n",
    "    using the parent run name, iteration index, and model type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    run_name : str\n",
    "        Base name of the parent run. Used to derive the child run identifier.\n",
    "    params : Mapping[str, Any]\n",
    "        Hyperparameters for this specific iteration.\n",
    "    metrics : Mapping[str, float]\n",
    "        Evaluation metrics for this iteration.\n",
    "    tags : Mapping[str, Any]\n",
    "        Additional metadata to attach to the child run.\n",
    "    trained_model : Any\n",
    "        Fitted model instance to be logged. Must match the chosen MLflow flavor.\n",
    "    model_type : {\"RF\", \"CatBoost\", \"MLP\", \"LSTM\"}, default=\"RF\"\n",
    "        Determines which MLflow flavor is used for model logging.\n",
    "        - RF       → ``mlflow.sklearn.log_model``\n",
    "        - CatBoost → ``mlflow.catboost.log_model``\n",
    "        - MLP      → ``mlflow.pytorch.log_model``\n",
    "        - LSTM     → ``mlflow.pytorch.log_model``\n",
    "    input_example : Any, optional\n",
    "        Example input for MLflow signature inference and UI preview.\n",
    "    registered_model_name : str, optional\n",
    "        If provided, registers the logged model under this name.\n",
    "    iteration : int, optional\n",
    "        Iteration index used to generate a unique child run name. If None,\n",
    "        the run name still works but uniqueness is not guaranteed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Any\n",
    "        The MLflow ``ModelInfo`` object returned by the corresponding\n",
    "        MLflow log_model flavor.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the ``model_type`` is unknown or unsupported.\n",
    "    \"\"\"\n",
    "\n",
    "    child_run_name = f\"{run_name}_iteration_{iteration}\"\n",
    "    model_name = f\"{child_run_name}_{model_type}\"\n",
    "\n",
    "    with mlflow.start_run(run_name=child_run_name, nested=True):\n",
    "        # ---- metadata ----\n",
    "        mlflow.log_params(dict(params))\n",
    "        mlflow.log_metrics(dict(metrics))\n",
    "        mlflow.set_tags(dict(tags))\n",
    "        mlflow.set_tag(\"model_version\", run_name)\n",
    "\n",
    "        # ---- model logging ----\n",
    "        if model_type == \"RF\":\n",
    "            model_info = mlflow.sklearn.log_model(\n",
    "                sk_model=trained_model,\n",
    "                name=model_name,\n",
    "                input_example=input_example,\n",
    "                registered_model_name=registered_model_name,\n",
    "            )\n",
    "\n",
    "        elif model_type == \"CatBoost\":\n",
    "            model_info = mlflow.catboost.log_model(\n",
    "                cb_model=trained_model,\n",
    "                name=model_name,\n",
    "                input_example=input_example,\n",
    "                registered_model_name=registered_model_name,\n",
    "            )\n",
    "\n",
    "        elif model_type in (\"MLP\", \"LSTM\"):\n",
    "            model_info = mlflow.pytorch.log_model(\n",
    "                pytorch_model=trained_model,\n",
    "                name=model_name,\n",
    "                input_example=input_example,\n",
    "                registered_model_name=registered_model_name,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f450fd9-0fa5-4cdb-9b65-a0d9e72bf46e",
   "metadata": {},
   "source": [
    "Let's try to create parent-child runs over a range of max_depth values of CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0d0ad-36c0-4855-b39c-ef687bc05e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth values you want to try\n",
    "depth_values = [4, 6, 8, 10]\n",
    "\n",
    "run_name = \"catboost_depth_tuning\"\n",
    "\n",
    "# Common tags for all child runs\n",
    "tags = {\n",
    "    \"model_family\": \"CatBoost\",\n",
    "    \"dataset_version\": \"cleaned_data\",\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    for idx, depth in enumerate(depth_values):\n",
    "        # 1) Set params for this depth\n",
    "        params = {\n",
    "            \"iterations\": 200,       \n",
    "            \"learning_rate\": 0.03,    \n",
    "            \"depth\": depth,           # depth iteration\n",
    "            \"l2_leaf_reg\": 3.0,       \n",
    "            \"random_seed\": SEED,      \n",
    "            \"loss_function\": \"RMSE\",  \n",
    "            \"verbose\": False          \n",
    "        }\n",
    "\n",
    "        # 2) Train & evaluate CatBoost for this depth\n",
    "        eval_results = eval_model(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            x_test,\n",
    "            y_test,\n",
    "            3,              \n",
    "            \"CatBoost\",     \n",
    "            params\n",
    "        )\n",
    "\n",
    "        # 3) Prepare metrics for logging\n",
    "        metrics = {\n",
    "            \"cv_mae\": eval_results[\"cv_mae\"],\n",
    "            \"cv_rmse\": eval_results[\"cv_rmse\"],\n",
    "            \"cv_mape\": eval_results[\"cv_mape\"],\n",
    "            \"test_mae\": eval_results[\"test_mae\"],\n",
    "            \"test_rmse\": eval_results[\"test_rmse\"],\n",
    "            \"test_mape\": eval_results[\"test_mape\"],\n",
    "            \"depth\": depth,\n",
    "        }\n",
    "\n",
    "        # 4) Log child run with your helper\n",
    "        model_info = log_run_child(\n",
    "            run_name=run_name,                 # parent run name\n",
    "            params=params,\n",
    "            metrics=metrics,\n",
    "            tags=tags,\n",
    "            trained_model=eval_results[\"model\"],\n",
    "            model_type=\"CatBoost\",\n",
    "            input_example=None,     \n",
    "            registered_model_name=None,        \n",
    "            iteration=idx                      \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235bc562-ff73-4055-8814-dbd1a014472d",
   "metadata": {},
   "source": [
    "Now, we can go to http://127.0.0.1:8080/ and see the child runs inside the parent run. We can also easily compare the metrics right in the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f9146-a7aa-4889-83e9-ae2bb9d4f2f5",
   "metadata": {},
   "source": [
    "# CatBoost Bayesian Hyperparameter Tuning with Child Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742012b7-be66-4cd8-90fb-2efe3aa3cf20",
   "metadata": {},
   "source": [
    "One of the best use cases for child runs are hyperparmaeter optimization runs.\n",
    "\n",
    "Often, to select hyperparameters, we might need to run hundreds of runs and if we log every run as a separate \"parent-like\" run, the UI will become messy very quickly.\n",
    "\n",
    "Let's see how we can use child runs and Bayesian Hyperparameter Tuning together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823c703-1010-42e0-81c4-715bb17a558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(\n",
    "    trial: optuna.Trial, \n",
    "    x_train: np.ndarray, \n",
    "    y_train: np.ndarray, \n",
    "    x_test: np.ndarray, \n",
    "    y_test: np.ndarray\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Optuna objective for CatBoost using eval_model and MLflow child runs.\n",
    "    Minimizes cross-validated MAE (cv_mae).\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # ----- 1. Sample CatBoost hyperparameters -----\n",
    "    params: Dict[str, Union[int, float, bool]] = {\n",
    "        \"iterations\": 100,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 0.1, 10.0, log=True),\n",
    "        \"random_seed\": SEED,\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    # ----- 2. Evaluate using your CV evaluator -----\n",
    "    eval_results = eval_model(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        n_splits=3,\n",
    "        model_name=\"CatBoost\",\n",
    "        model_params=params,\n",
    "    )\n",
    "\n",
    "    # ----- 3. Log as child MLflow run -----\n",
    "    metrics = {\n",
    "        \"cv_mae\": eval_results[\"cv_mae\"],\n",
    "        \"cv_rmse\": eval_results[\"cv_rmse\"],\n",
    "        \"cv_mape\": eval_results[\"cv_mape\"],\n",
    "        \"test_mae\": eval_results[\"test_mae\"],\n",
    "        \"test_rmse\": eval_results[\"test_rmse\"],\n",
    "        \"test_mape\": eval_results[\"test_mape\"],\n",
    "        \"trial_number\": trial.number,\n",
    "    }\n",
    "\n",
    "    tags = {\n",
    "        \"model_version\": \"catboost_optuna\",\n",
    "        \"trial\": trial.number,\n",
    "    }\n",
    "\n",
    "    # Log the run\n",
    "    log_run_child(\n",
    "        run_name=run_name,\n",
    "        params=params,\n",
    "        metrics=metrics,\n",
    "        tags=tags,\n",
    "        trained_model=eval_results[\"model\"],\n",
    "        model_type=\"CatBoost\",\n",
    "        input_example=None,\n",
    "        iteration=trial.number,\n",
    "    )\n",
    "    # ----- 4. Optuna minimizes this -----\n",
    "    return eval_results[\"cv_mape\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3402ae21-f306-44f3-93c3-1c7cfe099776",
   "metadata": {},
   "source": [
    "**Note that after the best set of hyperparameters is selected, we re-fit the model and log the parameters in the parent run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4c499-e4a6-4316-81b9-8d3df4f2989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"Catboost_optuna\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # 1) Create the study and optimize CV MAE via objective()\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    )\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, x_train, y_train, x_test, y_test),\n",
    "        n_trials=5,\n",
    "    )\n",
    "\n",
    "    # 2) Rebuild CatBoost params from best Optuna params\n",
    "    best_params = study.best_params.copy()\n",
    "    params = {\n",
    "        \"iterations\": 500,\n",
    "        \"learning_rate\": best_params[\"learning_rate\"],\n",
    "        \"depth\": best_params[\"depth\"],\n",
    "        \"l2_leaf_reg\": best_params[\"l2_leaf_reg\"],\n",
    "        \"random_seed\": 42,\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    # 3) Refit the best model params\n",
    "    eval_results = eval_model(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        n_splits=3,\n",
    "        model_name=\"CatBoost\",\n",
    "        model_params=params,\n",
    "    )\n",
    "\n",
    "    # 4) Combine metrics directly into a dict\n",
    "    metrics = {\n",
    "        \"cv_mae\": eval_results[\"cv_mae\"],\n",
    "        \"cv_rmse\": eval_results[\"cv_rmse\"],\n",
    "        \"cv_mape\": eval_results[\"cv_mape\"],\n",
    "        \"test_mae\": eval_results[\"test_mae\"],\n",
    "        \"test_rmse\": eval_results[\"test_rmse\"],\n",
    "        \"test_mape\": eval_results[\"test_mape\"],\n",
    "    }\n",
    "\n",
    "    # 5) Log metrics & params in parent run\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.set_tag(\"best_model\", \"true\")\n",
    "\n",
    "    # 6) Log the final best model    \n",
    "    model_info = mlflow.catboost.log_model(\n",
    "        cb_model=eval_results[\"model\"],\n",
    "        name=\"best_catboost_model\",\n",
    "        input_example=x_train[:5],\n",
    "        tags={\"best_model\": \"true\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46797757-6b5e-4620-88f1-2ce6815df029",
   "metadata": {},
   "source": [
    "# Logging important artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949297d8-a3d8-42eb-9a8e-20252ac4249d",
   "metadata": {},
   "source": [
    "Often, especially for the best selected models, we want to reproduce the results.\n",
    "\n",
    "To do that, we need to make sure:\n",
    "- We know what kind of input the model requires\n",
    "- How to create this input (aka how raw data is preprocessed)\n",
    "\n",
    "Let's first learn how to save the input examples.\n",
    "\n",
    "To do that, we deifne the model signature.\n",
    "\n",
    "A model signature is MLflow’s way of recording the input and output schema of your model when you log it.\n",
    "\n",
    "It defines:\n",
    "- input column names\n",
    "- input types (string, double, integer, tensor shapes, etc.)\n",
    "- output types\n",
    "- shapes (fixed or variable)\n",
    "\n",
    "Note that we define the signature and the input example for the final (best) model only.\n",
    "\n",
    "We can also save utils.py to save the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b56be5-c355-4d5d-8448-da12061df666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_name = \"Catboost_optuna\"\n",
    "\n",
    "MODELS_DIR = Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # 1) Create the study and optimize CV MAE via objective()\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    )\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, x_train, y_train, x_test, y_test),\n",
    "        n_trials=10,\n",
    "    )\n",
    "\n",
    "    # 2) Rebuild CatBoost params from best Optuna params\n",
    "    best_params = study.best_params.copy()\n",
    "    params = {\n",
    "        \"iterations\": 500,\n",
    "        \"learning_rate\": best_params[\"learning_rate\"],\n",
    "        \"depth\": best_params[\"depth\"],\n",
    "        \"l2_leaf_reg\": best_params[\"l2_leaf_reg\"],\n",
    "        \"random_seed\": SEED,\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    # 3) Refit using ONLY eval_model\n",
    "    eval_results = eval_model(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        n_splits=3,\n",
    "        model_name=\"CatBoost\",\n",
    "        model_params=params,\n",
    "    )\n",
    "\n",
    "    print(\"METRICS----------\")\n",
    "    print(eval_results[\"cv_mae\"])\n",
    "\n",
    "    # 4) Metrics\n",
    "    metrics = {\n",
    "        \"cv_mae\": eval_results[\"cv_mae\"],\n",
    "        \"cv_rmse\": eval_results[\"cv_rmse\"],\n",
    "        \"cv_mape\": eval_results[\"cv_mape\"],\n",
    "        \"test_mae\": eval_results[\"test_mae\"],\n",
    "        \"test_rmse\": eval_results[\"test_rmse\"],\n",
    "        \"test_mape\": eval_results[\"test_mape\"],\n",
    "        \"best_trial_number\": study.best_trial.number,\n",
    "    }\n",
    "\n",
    "    # 5) Log metrics & params\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.set_tag(\"best_model\", \"true\")\n",
    "\n",
    "    # 6) Best model\n",
    "    best_model = eval_results[\"model\"]\n",
    "\n",
    "    # Save preprocessing code snapshot as plain .py\n",
    "    utils_path = Path(utils.__file__).resolve()\n",
    "    preprocess_path = MODELS_DIR / \"preprocessing.py\"\n",
    "    preprocess_path.write_text(\n",
    "        utils_path.read_text(encoding=\"utf-8\"),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    artifacts = {\n",
    "        \"preprocessing_code\": str(preprocess_path),\n",
    "    }\n",
    "\n",
    "    # 7) Input example + signature\n",
    "    input_example = x_train.iloc[:5].copy()\n",
    "    signature = infer_signature(\n",
    "        input_example,\n",
    "        best_model.predict(input_example),\n",
    "    )\n",
    "\n",
    "    # 8) Log CatBoost model\n",
    "    model_info = mlflow.catboost.log_model(\n",
    "        cb_model=best_model,\n",
    "        name=\"best_catboost_model\",\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=\"Catboost_model_candidate\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bef71-a80c-4d1d-9eb1-a8cec8abe04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_test['Power'].values[:100], label='True value')\n",
    "plt.plot(best_model.predict(x_test)[:100], label='Predicted value')\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ff00d0-bb86-41dc-a050-f86230c01dba",
   "metadata": {},
   "source": [
    "This happened because we have not stored the X_scaler (StandardScaler) for the best model.\n",
    "\n",
    "Let's fix it.\n",
    "\n",
    "The easiest and most convinient way is to create a model wrapper on top of the model.\n",
    "\n",
    "This allows us to create any custom predict method.\n",
    "\n",
    "In this case, we will transform the features using the saved scaler object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b144d4-f2fd-41da-996f-a6e3d4b3aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatBoostCustom(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"\n",
    "    PyFunc CatBoost model with bundled X-scaler.\n",
    "    Expects feature-engineered, unscaled input DataFrame.\n",
    "    \"\"\"\n",
    "    def load_context(self, context: mlflow.pyfunc.PythonModelContext) -> None:\n",
    "        self.x_scaler = joblib.load(context.artifacts[\"x_scaler\"])\n",
    "        self.model = CatBoostRegressor()\n",
    "        self.model.load_model(context.artifacts[\"catboost_model\"])\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        context: mlflow.pyfunc.PythonModelContext,\n",
    "        model_input: pd.DataFrame,\n",
    "    ) -> np.ndarray:\n",
    "        x_scaled = self.x_scaler.transform(model_input)\n",
    "        return self.model.predict(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66d0882-e62d-4350-8cf6-79cf497b4ffc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_name = \"Catboost_optuna_with_scaler\"\n",
    "\n",
    "MODELS_DIR = Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # 1) Optuna optimization\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    )\n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, x_train, y_train, x_test, y_test),\n",
    "        n_trials=10,\n",
    "    )\n",
    "\n",
    "    # 2) Best params\n",
    "    best_params = study.best_params.copy()\n",
    "    params = {\n",
    "        \"iterations\": 500,\n",
    "        \"learning_rate\": best_params[\"learning_rate\"],\n",
    "        \"depth\": best_params[\"depth\"],\n",
    "        \"l2_leaf_reg\": best_params[\"l2_leaf_reg\"],\n",
    "        \"random_seed\": SEED,\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "    # 3) Train + evaluate\n",
    "    eval_results = eval_model(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        n_splits=3,\n",
    "        model_name=\"CatBoost\",\n",
    "        model_params=params,\n",
    "    )\n",
    "\n",
    "    best_model = eval_results[\"model\"]\n",
    "    x_scaler = eval_results[\"x_scaler\"]\n",
    "\n",
    "    # 4) Metrics\n",
    "    metrics = {\n",
    "        \"cv_mae\": eval_results[\"cv_mae\"],\n",
    "        \"cv_rmse\": eval_results[\"cv_rmse\"],\n",
    "        \"cv_mape\": eval_results[\"cv_mape\"],\n",
    "        \"test_mae\": eval_results[\"test_mae\"],\n",
    "        \"test_rmse\": eval_results[\"test_rmse\"],\n",
    "        \"test_mape\": eval_results[\"test_mape\"],\n",
    "        \"best_trial_number\": study.best_trial.number,\n",
    "    }\n",
    "\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.set_tag(\"best_model\", \"true\")\n",
    "\n",
    "    # 5) Signature\n",
    "    input_example = x_train.iloc[:5].copy()\n",
    "    y_example = best_model.predict(x_scaler.transform(input_example))\n",
    "    signature = infer_signature(input_example, y_example)\n",
    "\n",
    "    # 6) Save artifacts locally \n",
    "    cb_path = MODELS_DIR / \"catboost_model.cbm\"\n",
    "    scaler_path = MODELS_DIR / \"x_scaler.joblib\"\n",
    "    preprocess_path = MODELS_DIR / \"preprocessing.py\"\n",
    "\n",
    "    best_model.save_model(cb_path)\n",
    "    joblib.dump(x_scaler, scaler_path)\n",
    "\n",
    "    # Save preprocessing code snapshot as plain .py\n",
    "    utils_path = Path(utils.__file__).resolve()\n",
    "    preprocess_path.write_text(\n",
    "        utils_path.read_text(encoding=\"utf-8\"),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    artifacts = {\n",
    "        \"catboost_model\": str(cb_path),\n",
    "        \"x_scaler\": str(scaler_path),\n",
    "        \"preprocessing_code\": str(preprocess_path),\n",
    "    }\n",
    "\n",
    "    # 7) Log PyFunc model\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        name=\"best_catboost_pyfunc\",\n",
    "        python_model=CatBoostCustom(),\n",
    "        artifacts=artifacts,\n",
    "        signature=signature,\n",
    "        input_example=input_example,\n",
    "        registered_model_name=\"Catboost_model_candidate\",\n",
    "        tags={\n",
    "        \"best_model\": \"true\",\n",
    "        \"logged_model_name\": \"best_catboost_pyfunc\",\n",
    "        \"run_name\": run_name,\n",
    "    },\n",
    "    )\n",
    "\n",
    "    print(\"Logged model:\", model_info.model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1516fdbc-885e-45e0-bdc2-06e87b0821d4",
   "metadata": {},
   "source": [
    "# Logging and loading the best CatBoost Optuna model with X_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18568d-b801-4c86-8bda-220a97584291",
   "metadata": {},
   "source": [
    "Now, note that when we run the parent run, we re-train the best optuna model on the entire training set and log a tag - best_model: true. This allows us to easily load the best model, use it for predictions and consider moving the model to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bf80e-f628-486c-bf31-24a52bd0d0d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find latest  model\n",
    "top_models = mlflow.search_logged_models(\n",
    "    filter_string=(\n",
    "        \"tag.best_model = 'true' \"\n",
    "        \"AND tag.logged_model_name = 'best_catboost_pyfunc' \"\n",
    "        \"AND tag.run_name = 'Catboost_optuna_with_scaler'\"\n",
    "    ),\n",
    "    order_by=[{\"field_name\": \"last_updated_timestamp\", \"ascending\": False}],\n",
    "    max_results=1,\n",
    ")\n",
    "\n",
    "best_model = top_models.iloc[0]\n",
    "\n",
    "model_id = best_model[\"model_id\"]\n",
    "name = best_model[\"name\"]\n",
    "\n",
    "# Load model\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"models:/{model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6f3f8-4109-48ea-bdbd-44e6d55c30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3df5f3-4f13-43f0-a128-e04b570cd8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loaded_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60455fb2-9f6c-4efd-9aba-9aecab7d4361",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_test['Power'].values[:100], label='True value')\n",
    "plt.plot(y_pred[:100], label='Predicted value')\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd28c7e2-4cf4-4cec-abd3-d27356c55082",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_errors(x_test, df_test['Power'], y_pred, error='mae', error_threshold=50, rolling_window=151)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621b5bb-6bf7-4ef9-b934-e1f36b2de362",
   "metadata": {},
   "source": [
    "# Testing the best model on Production Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040b6d3-cadb-478d-90b9-e24d32af76e0",
   "metadata": {},
   "source": [
    "Even though I are NOT supposed to be able to test the model on production data, since we are doing an educational project for a portfolio, it's useful to know what kind of performance we can expect the model to have on production data.\n",
    "\n",
    "Let's test the best selected model on prod data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34a543-006d-4709-8c79-c73b68803a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df_prod = pd.read_parquet('../data/01_raw/df_prod.parquet')\n",
    "df_prod = df_prod[df_prod['Power'] > 20].copy()\n",
    "df_prod.index = pd.to_datetime(df_prod['Timestamps'])\n",
    "df_prod.drop(columns=['Timestamps'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508b7ba-fa2c-4ac1-9e0e-f25cf91a6f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prod, y_prod, _, _ = get_clean_data(df_prod[:-1], df_prod[-1:], abs_diff_thresholds, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e141d6e-e43e-4b90-93e3-2447c3b8ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prod = loaded_model.predict(x_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c28fbc-e7f1-452c-9523-76d12db15955",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_test_prod = np.mean(np.abs(y_prod.values.ravel() - y_pred_prod.ravel())/y_prod.values.ravel()*100)\n",
    "print(f\"MAPE on prod set: {mape_test_prod}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba1e46-c44a-4d60-a450-7937b8cc7577",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_prod, y_prod, y_pred_prod, error='mape', error_threshold=8.5, rolling_window=288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e9548e-5d5e-4cc0-9af6-08b2aa5518b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
