{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140e80b-ef71-4b56-a656-95c663f63d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from typing import Tuple, Union, Literal, List, Dict, Optional, Any, Mapping\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import butter, lfilter, lfilter_zi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils import plot_time_series, compute_metrics, plot_predictions, remove_outliers_zscore, plot_errors\n",
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea5c3ec-9526-4312-b50d-8b54d950d3a8",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Baseline pipeline recap](#Baseline-pipeline-recap)\n",
    "2. [Filter-by-difference cleaning](#Filter-by-difference-cleaning)\n",
    "3. [Denoising: Low-Pass FFT Filter](#Denoising:-Low-Pass-FFT-Filter)\n",
    "4. [Low-pass-FFT-data-leakage](#Low-pass-FFT-data-leakage)\n",
    "5. [Denoising: Butterworth filter](#Denoising:-Butterworth-filter)\n",
    "6. [Denoising: Mean filter](#Denoising:-Mean-filter)\n",
    "7. [Domain Feature Engineering](#Domain-Feature-Engineering)\n",
    "8. [Statistical Features](#Statistical-Features)\n",
    "9. [Lag features](#Lag-features)\n",
    "10. [Conclusions from deep data cleaning](#Conclusions-from-deep-data-cleaning)\n",
    "11. [Model Selection: Random Forest](#Model-Selection:-Random-Forest)\n",
    "12. [Model Selection: CatBoost](#Model-Selection:-CatBoost)\n",
    "13. [Model Selection: LSTM](#Model-Selection:-LSTM)\n",
    "14. [Model Selection: MLP](#Model-Selection:-MLP)\n",
    "15. [Conclusions](#Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52353250-ed58-4b0a-b013-0a23b75c923e",
   "metadata": {},
   "source": [
    "# Baseline pipeline recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130db5a-2d83-47ae-a215-e00e5aaad6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(df: pd.DataFrame, columns:List[str]=None, lags: List[int]=[1], drop_na=True):\n",
    "    \"\"\"\n",
    "    Add lag features to DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - columns: list of column names (default: all numeric)\n",
    "    - lags: int or list of lag periods (default: 1)\n",
    "    - drop_na: bool, drop NaN rows (default: True)\n",
    "    \n",
    "    Returns: DataFrame with lag features\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for col in columns:\n",
    "        for lag in lags:\n",
    "            df_result[f\"{col}_lag{lag}\"] = df_result[col].shift(lag)\n",
    "    \n",
    "    if drop_na:\n",
    "        return df_result.dropna()\n",
    "    else:\n",
    "        return df_result.bfill()  # Backward fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad7678c-0eff-48e7-94d1-9354fbaf91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(df, columns, window_sizes=7, stats=['mean', 'median'], drop_na=True):\n",
    "    \"\"\"\n",
    "    Add rolling features to DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - columns: list of column names\n",
    "    - window_sizes: int or list of window sizes (default: 7)\n",
    "    - stats: list of statistics ['mean', 'median', 'std', 'min', 'max', 'skew', 'kurt']\n",
    "    - drop_na: bool, drop NaN rows (default: True)\n",
    "    \n",
    "    Returns: DataFrame with rolling features\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Convert single values to lists\n",
    "    if isinstance(window_sizes, int):\n",
    "        window_sizes = [window_sizes]\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    \n",
    "    # Create rolling features\n",
    "    for col in columns:\n",
    "        for window in window_sizes:\n",
    "            rolling = df_result[col].rolling(window)\n",
    "            \n",
    "            for stat in stats:\n",
    "                if stat == 'mean':\n",
    "                    df_result[f\"{col}_roll{window}_mean\"] = rolling.mean()\n",
    "                elif stat == 'median':\n",
    "                    df_result[f\"{col}_roll{window}_median\"] = rolling.median()\n",
    "                elif stat == 'std':\n",
    "                    df_result[f\"{col}_roll{window}_std\"] = rolling.std()\n",
    "                elif stat == 'min':\n",
    "                    df_result[f\"{col}_roll{window}_min\"] = rolling.min()\n",
    "                elif stat == 'max':\n",
    "                    df_result[f\"{col}_roll{window}_max\"] = rolling.max()\n",
    "                elif stat == 'skew':\n",
    "                    df_result[f\"{col}_roll{window}_skew\"] = rolling.skew()\n",
    "                elif stat == 'kurt':\n",
    "                    df_result[f\"{col}_roll{window}_kurt\"] = rolling.kurt()\n",
    "    if drop_na:\n",
    "        return df_result.dropna()\n",
    "    else:\n",
    "        return df_result.bfill()  # Backward fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57324418-c4cd-4b7e-a230-7eea292ecef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    x_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    n_splits: int = 3,\n",
    "    model_name: Literal[\"RF\", \"LinReg\", \"CatBoost\"] = \"RF\",\n",
    "    model_params: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a time series model using TimeSeriesSplit cross-validation\n",
    "    on the training set, then refit on the full train data and evaluate on test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train : pd.DataFrame\n",
    "        Training features.\n",
    "    y_train : pd.Series\n",
    "        Training target.\n",
    "    x_test : pd.DataFrame\n",
    "        Test features.\n",
    "    y_test : pd.Series\n",
    "        Test target.\n",
    "    n_splits : int, default=3\n",
    "        Number of time-series CV folds.\n",
    "    model_name : {'RF','LinReg','CatBoost'}, default='RF'\n",
    "        Model identifier.\n",
    "    model_params : dict or None\n",
    "        Keyword arguments for the selected model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        - 'cv_mae'   : average MAE over CV folds\n",
    "        - 'cv_rmse'  : average RMSE over CV folds\n",
    "        - 'cv_mape'  : average MAPE over CV folds\n",
    "        - 'test_mae' : MAE on the final test set\n",
    "        - 'test_rmse': RMSE on the final test set\n",
    "        - 'test_mape': MAPE on the final test set\n",
    "        - 'y_pred_test': model predictions on test set\n",
    "        - 'model'      : fitted final model\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_mae_list: List[float] = []\n",
    "    cv_rmse_list: List[float] = []\n",
    "    cv_mape_list: List[float] = []\n",
    "\n",
    "    # --- Cross-validation ---\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(x_train), 1):\n",
    "        x_train_cv = x_train.iloc[train_idx, :].copy()\n",
    "        x_val_cv = x_train.iloc[val_idx, :].copy()\n",
    "        y_train_cv = y_train.iloc[train_idx].copy().values.ravel()\n",
    "        y_val_cv = y_train.iloc[val_idx].copy().values.ravel()\n",
    "\n",
    "        # Scale features\n",
    "        x_scaler = StandardScaler()\n",
    "        x_scaled_cv_train = x_scaler.fit_transform(x_train_cv)\n",
    "        x_scaled_cv_val = x_scaler.transform(x_val_cv)\n",
    "\n",
    "        # Construct model\n",
    "        if model_name == \"RF\":\n",
    "            model = RF(**model_params)\n",
    "        elif model_name == \"LinReg\":\n",
    "            model = LinearRegression(**model_params)\n",
    "        elif model_name == \"CatBoost\":\n",
    "            params = dict(model_params)\n",
    "            params.setdefault(\"verbose\", False)\n",
    "            params.setdefault(\"random_seed\", SEED)\n",
    "            model = CatBoostRegressor(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "        # Fit and predict\n",
    "        model.fit(x_scaled_cv_train, y_train_cv)\n",
    "        y_pred_cv = model.predict(x_scaled_cv_val)\n",
    "\n",
    "        # Metrics\n",
    "        mae_err, rmse_err, mape_err = compute_metrics(y_val_cv, y_pred_cv)\n",
    "        cv_mae_list.append(float(mae_err))\n",
    "        cv_rmse_list.append(float(rmse_err))\n",
    "        cv_mape_list.append(float(mape_err))\n",
    "\n",
    "    cv_mae = float(np.mean(cv_mae_list))\n",
    "    cv_rmse = float(np.mean(cv_rmse_list))\n",
    "    cv_mape = float(np.mean(cv_mape_list))\n",
    "\n",
    "    # --- Final model training on full training set ---\n",
    "    if model_name == \"RF\":\n",
    "        model = RF(**model_params)\n",
    "    elif model_name == \"LinReg\":\n",
    "        model = LinearRegression(**model_params)\n",
    "    elif model_name == \"CatBoost\":\n",
    "        params = dict(model_params)\n",
    "        params.setdefault(\"verbose\", False)\n",
    "        params.setdefault(\"random_seed\", SEED)\n",
    "        model = CatBoostRegressor(**params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "    x_scaler = StandardScaler()\n",
    "    x_scaled_train = x_scaler.fit_transform(x_train)\n",
    "    x_scaled_test = x_scaler.transform(x_test)\n",
    "\n",
    "    model.fit(x_scaled_train, y_train.values.ravel())\n",
    "\n",
    "    y_pred_test = model.predict(x_scaled_test)\n",
    "    mae_err_test, rmse_err_test, mape_err_test = compute_metrics(y_test, y_pred_test)\n",
    "\n",
    "    return {\n",
    "        \"cv_mae\": round(cv_mae, 2),\n",
    "        \"cv_rmse\": round(cv_rmse, 2),\n",
    "        \"cv_mape\": round(cv_mape, 2),\n",
    "        \"test_mae\": round(float(mae_err_test), 2),\n",
    "        \"test_rmse\": round(float(rmse_err_test), 2),\n",
    "        \"test_mape\": round(float(mape_err_test), 2),\n",
    "        \"y_pred_test\": y_pred_test,\n",
    "        \"model\": model,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64820a-944c-4d6c-a097-fb791c32f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_baseline(\n",
    "    df: pd.DataFrame,\n",
    "    split_index: int,\n",
    "    target: str,\n",
    "    z_threshold: float\n",
    ") -> Tuple[Dict[str, Any], pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Run the full baseline pipeline:\n",
    "    - time-based train/test split\n",
    "    - z-score based outlier cleaning (train + test features)\n",
    "    - time-series feature engineering (lags + rolling stats)\n",
    "    - Random Forest training + evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Full input dataset with features and target.\n",
    "    split_index : int\n",
    "        Index position used to split df into train ([:split_index])\n",
    "        and test ([split_index:]) in time order.\n",
    "    target : str\n",
    "        Name of the target column to predict.\n",
    "    z_threshold : float\n",
    "        Z-score threshold for outlier removal in numeric columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    eval_results : dict\n",
    "        Metrics and artifacts from eval_model (CV + test metrics, model, preds, etc.).\n",
    "    x_clean_test : pd.DataFrame\n",
    "        Final engineered test feature matrix used for inference.\n",
    "    y_test : pd.Series\n",
    "        Raw test-set target values (not cleaned), aligned with x_clean_test.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Time-based train/test split\n",
    "    df_train = df[:split_index].copy()\n",
    "    df_test = df[split_index:].copy()\n",
    "\n",
    "    # 2) Z-score cleaning on train (learn stats) and test (apply train stats)\n",
    "    df_clean_train, z_score_stats = remove_outliers_zscore(\n",
    "        df_train,\n",
    "        threshold=z_threshold,\n",
    "        nan_treatment=\"ffill\"\n",
    "    )\n",
    "\n",
    "    features = [col for col in df.columns if col != target]\n",
    "\n",
    "    x_clean_test, _ = remove_outliers_zscore(\n",
    "        df_test[features],\n",
    "        threshold=z_threshold,\n",
    "        nan_treatment=\"ffill\",\n",
    "        stats=z_score_stats\n",
    "    )\n",
    "\n",
    "    # Keep original target on test; only clean features\n",
    "    df_clean_test = pd.concat([x_clean_test, df_test[target]], axis=1)\n",
    "\n",
    "    # 3) Lag features (example: GenRPM, GenPh1Temp)\n",
    "    df_train_lag = add_lag_features(\n",
    "        df_clean_train,\n",
    "        columns=[\"GenRPM\", \"GenPh1Temp\"],\n",
    "        lags=[1, 2, 3],\n",
    "        drop_na=False\n",
    "    )\n",
    "    df_test_lag = add_lag_features(\n",
    "        df_clean_test,\n",
    "        columns=[\"GenRPM\", \"GenPh1Temp\"],\n",
    "        lags=[1, 2, 3],\n",
    "        drop_na=False\n",
    "    )\n",
    "\n",
    "    # 4) Rolling statistics (median, std, min, max)\n",
    "    df_train_feat = add_rolling_features(\n",
    "        df_train_lag,\n",
    "        window_sizes=[5, 10, 15],\n",
    "        columns=[\"GenRPM\", \"WindSpeed\", \"GenPh1Temp\"],\n",
    "        stats=[\"median\", \"std\", \"min\", \"max\"],\n",
    "        drop_na=False\n",
    "    )\n",
    "    df_test_feat = add_rolling_features(\n",
    "        df_test_lag,\n",
    "        window_sizes=[5, 10, 15],\n",
    "        columns=[\"GenRPM\", \"WindSpeed\", \"GenPh1Temp\"],\n",
    "        stats=[\"median\", \"std\", \"min\", \"max\"],\n",
    "        drop_na=False\n",
    "    )\n",
    "\n",
    "    # If you want to test \"pure filtering\" without feature engineering:\n",
    "    # df_train_feat = df_clean_train.copy()\n",
    "    # df_test_feat = df_clean_test.copy()\n",
    "\n",
    "    # 5) Final feature matrices (exclude target column)\n",
    "    feature_cols = [col for col in df_train_feat.columns if col != target]\n",
    "\n",
    "    x_clean_train = df_train_feat[feature_cols].copy()\n",
    "    y_clean_train = df_train_feat[target].copy()\n",
    "\n",
    "    x_clean_test = df_test_feat[feature_cols].copy()\n",
    "    y_test = df_test[target].copy()   # use original (uncleaned) target for evaluation\n",
    "\n",
    "    # 6) Run baseline Random Forest model\n",
    "    params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"random_state\": SEED,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    eval_results = eval_model(\n",
    "        x_clean_train,\n",
    "        y_clean_train,\n",
    "        x_clean_test,\n",
    "        y_test,\n",
    "        n_splits=3,\n",
    "        model_name=\"RF\",\n",
    "        model_params=params,\n",
    "    )\n",
    "\n",
    "    return eval_results, x_clean_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262bd602-a4e6-4462-92a1-5bc785f0a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f79716-8a35-4865-9417-1a4fc05fd031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval_results, x_test, y_test = run_baseline(df, 30_000, 'Power', 3)\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d369dcd-6db1-4c87-b499-6a54a88fc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=8.5, rolling_window=288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b572310d-10e5-4258-adfd-63a52dd0c546",
   "metadata": {},
   "source": [
    "# Filter-by-difference cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e072762-387b-4fba-b535-1536b3252a40",
   "metadata": {},
   "source": [
    "### Target cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b8b057-3489-474a-bc5e-e27715aeb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cef1ff-2d9b-49ce-b257-1f1689a19135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train, _ = remove_outliers_zscore(df_train, threshold=3, nan_treatment='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6778fb9-34b1-4b47-8585-48746cb9121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = [col for col in df_clean_train.columns]\n",
    "plot_time_series(df_clean_train, cols_to_plot, step=1, rolling_window=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead50ef6-7d02-4de0-8e58-92beabcaf84d",
   "metadata": {},
   "source": [
    "Despite that this data is cleaned data using z-score filter, we still see quite a lot of suddent spikes that can be considered as outliers.\n",
    "\n",
    "A very good way to see the if there are suddent outlying spikes is to compute the differences between the neighboring points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60c553-2b1b-45d9-9cb7-1e52f1c01359",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 2, figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df_clean_train['Power'].diff(1))\n",
    "plt.ylim(0, 20)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df_clean_train['WindDirAbs'].diff(1))\n",
    "plt.ylim(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22afee4e-f0eb-469e-aa18-071ec2cf6d80",
   "metadata": {},
   "source": [
    "Instead of specifying the cut-off differences from positive and negative sides, let's compute and plot the absolute differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d86ab4-e8a6-4d68-8195-729ed6641268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = df_train.columns  # or pick a subset\n",
    "n_features = len(cols)\n",
    "\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(12, 3 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    ax = axes[i]\n",
    "    diffs = df_clean_train[col].diff(1).abs().dropna() # compute the absolute differences\n",
    "\n",
    "    sns.histplot(diffs, ax=ax)\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel(\"abs(diff(1))\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "# remove any unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027cc13-7af0-4d03-aedb-3db50458c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diff_outliers(df, column, diff_threshold):\n",
    "    \"\"\"\n",
    "    Remove outliers based on absolute first-order diff and forward-fill the gaps.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    column : str\n",
    "        Column to clean.\n",
    "    diff_threshold : float\n",
    "        Absolute diff threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_clean : pd.DataFrame\n",
    "        Cleaned dataframe with forward fill.\n",
    "    outlier_idx : pd.Index\n",
    "        Indices of removed outliers.\n",
    "    \"\"\"\n",
    "\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1. Compute absolute diff\n",
    "    diff_vals = df_clean[column].diff(1).abs()\n",
    "\n",
    "    # 2. Outlier mask\n",
    "    outlier_mask = diff_vals > diff_threshold\n",
    "    outlier_idx = df_clean.index[outlier_mask]\n",
    "\n",
    "    # 3. Remove outliers\n",
    "    df_clean.loc[outlier_idx, column] = np.nan\n",
    "\n",
    "    # 4. Forward fill (and backfill if needed)\n",
    "    df_clean[column] = df_clean[column].ffill().bfill()\n",
    "\n",
    "    return df_clean, outlier_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01773f5d-a280-4d65-8501-7606e71cb35e",
   "metadata": {},
   "source": [
    "From the figure or quantiles, we can identify the cut-off differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9454b-09a4-4681-b9b3-82c5f8ed19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_diff_thresholds = {\n",
    "    'WindSpeed': 12,\n",
    "    'WindDirAbs': 70,\n",
    "    'Power': 200,\n",
    "    'Pitch': 12,\n",
    "    'GenRPM': 450,\n",
    "    'WindDirRel': 9,\n",
    "    'NacelTemp': 40,\n",
    "    'GenPh1Temp': 40,\n",
    "    'RotorRPM': 20,\n",
    "    'EnvirTemp': 17,\n",
    "    'NacelTemp':    30,   \n",
    "    'GearOilTemp':  20,  \n",
    "    'GearBearTemp': 35,\n",
    "    'GenBearTemp': 8,\n",
    "    'GenPh1Temp':   35, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e0a86-bc42-4ff4-afa6-d26e77d33b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_train = df_train.copy()\n",
    "removed_dict = {}\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4029eea-3790-4d3f-ab1d-ebbe95368828",
   "metadata": {},
   "source": [
    "Let's plot the raw and filtered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc840cb-9821-4425-99c4-d9d8d6db29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are NOT predictions, but we can use the same functions\n",
    "plot_predictions(df_train['Power'], df_filtered_train['Power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2473f-a709-47c6-bac6-7ab97026e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = [col for col in df_filtered_train.columns]\n",
    "plot_time_series(df_filtered_train, cols_to_plot, step=1, rolling_window=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d5bf7-d44a-4ca2-9694-1b66d9b9feeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select numeric columns only\n",
    "numeric_cols = df_filtered_train.select_dtypes(include=np.number).columns\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(numeric_cols) / n_cols))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.histplot(df_filtered_train[col].dropna(), bins=20, kde=True)\n",
    "    plt.title(col, fontsize=10)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbea873-fc59-4dfd-8c5f-a4738242c550",
   "metadata": {},
   "source": [
    "It seems that this removed the outliers a little bit better.\n",
    "\n",
    "However, we see that there are still some outliers exist.\n",
    "\n",
    "Let's see how the model performes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377b393-745c-4b08-83a4-504fff1468cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the test set\n",
    "filt_cols = [col for col in df_test.columns if col != 'Power']\n",
    "\n",
    "x_filtered_test = df_test[filt_cols].copy()\n",
    "removed_dict = {}\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    if col != 'Power':\n",
    "        x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d753f222-610e-43c0-b7b6-975c0e0ef55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    df_filtered_train[filt_cols],\n",
    "    df_filtered_train['Power'], \n",
    "    x_filtered_test, \n",
    "    df_test['Power'], \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38902300-f1d1-47f3-a587-3e5edb479930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 66.37,\n",
    "#  'cv_rmse': 96.43,\n",
    "#  'cv_mape': 7.76,\n",
    "#  'test_mae': 72.35,\n",
    "#  'test_rmse': 121.48,\n",
    "#  'test_mape': 9.34,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8f65e-44d6-40a0-9505-b28d4d059365",
   "metadata": {},
   "source": [
    "We see that this cleaning works a bit better that z-score leaning.\n",
    "\n",
    "Let's apply z-score cleaning ON TOP if filtering by difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a29d1-485d-42a8-8aff-bd808a667cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_clean, _ = remove_outliers_zscore(pd.DataFrame(df_filtered_train['Power']), nan_treatment='ffill', threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd31051-dd52-408b-a7c8-884d3b8e3300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Demonstrate drop as well\n",
    "x_train_clean, z_score_stats = remove_outliers_zscore(df_filtered_train[filt_cols], nan_treatment='ffill', threshold=4)\n",
    "x_test_clean, _ = remove_outliers_zscore(x_filtered_test, nan_treatment='ffill', threshold=3, stats=z_score_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458bdfc8-b00f-47a0-96b6-0e02c4b203fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    x_train_clean,\n",
    "    y_train_clean, \n",
    "    x_test_clean, \n",
    "    df_test['Power'], \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ae998-4d4c-44ae-83c6-2aa1feb77cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 63.89,\n",
    "#  'cv_rmse': 91.89,\n",
    "#  'cv_mape': 7.58,\n",
    "#  'test_mae': 72.15,\n",
    "#  'test_rmse': 121.68,\n",
    "#  'test_mape': 9.31,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da715ffe-0506-4b38-aa0c-b021b4f8d927",
   "metadata": {},
   "source": [
    "We see that we don't get a better performance.\n",
    "\n",
    "In fact, at z_threshold = 3, it seems we are cutting the values that contain useful information because the score gets worse.\n",
    "\n",
    "In production, it's easy to maintain the \"Filter by difference\" method because we do NOT need to keep train of z_score_stats.\n",
    "\n",
    "Good improvement and less production headache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708198bb-4005-4146-8b68-802ca1c0aa6c",
   "metadata": {},
   "source": [
    "# Denoising: Low-Pass FFT Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80edefda-e887-4572-860b-2e5b93be555f",
   "metadata": {},
   "source": [
    "In the EDA stage, we have observed that the data has a lot of noise.\n",
    "\n",
    "Potentially, cutting the noise can give us a good model improvement.\n",
    "\n",
    "Let's check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b7ea7-d68a-490e-8a5b-dd6b5eb2c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_lowpass_filter(x, dt, cutoff):\n",
    "    \"\"\"\n",
    "    Apply a low-pass FFT filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        Raw time-series signal.\n",
    "    dt : float\n",
    "        Sampling interval in chosen time units (e.g. 10/60 for cycles/hour).\n",
    "    cutoff : float\n",
    "        Cutoff frequency in same units as FFT output (e.g. cycles/hour).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_filtered : np.array\n",
    "        Filtered time-series (mean added back).\n",
    "    freqs : np.array\n",
    "        Frequency axis.\n",
    "    fft_filtered : np.array\n",
    "        Filtered FFT values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure numpy array\n",
    "    x_clean = np.asarray(x, dtype=float)\n",
    "\n",
    "    # Fill NaNs if needed\n",
    "    if np.isnan(x_clean).any():\n",
    "        nans = np.isnan(x_clean)\n",
    "        x_clean[nans] = np.interp(np.flatnonzero(nans),\n",
    "                                  np.flatnonzero(~nans),\n",
    "                                  x_clean[~nans])\n",
    "\n",
    "    # Store original mean\n",
    "    mean_val = np.mean(x_clean)\n",
    "\n",
    "    # Detrend (remove mean for FFT)\n",
    "    x_detrended = x_clean - mean_val\n",
    "\n",
    "    N = len(x_detrended)\n",
    "\n",
    "    # FFT\n",
    "    fft_vals = np.fft.rfft(x_detrended)\n",
    "    freqs = np.fft.rfftfreq(N, d=dt)\n",
    "\n",
    "    # Low-pass mask\n",
    "    mask = freqs <= cutoff\n",
    "    fft_filtered = fft_vals * mask\n",
    "\n",
    "    # Inverse FFT + ADD MEAN BACK\n",
    "    x_filtered = np.fft.irfft(fft_filtered, n=N) + mean_val\n",
    "\n",
    "    return x_filtered, freqs, fft_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d4d36a-8a9a-478a-927c-51900984312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc0e49-3d5c-42d7-a36c-b95da36337cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abs difference filter\n",
    "# Train set\n",
    "df_filtered_train = df_train.copy()\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)\n",
    "\n",
    "# Test set\n",
    "filt_cols = [col for col in df_test.columns if col != 'Power']\n",
    "x_filtered_test = df_test[filt_cols].copy()\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    if col != 'Power':\n",
    "        x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09dd8c-e442-4d45-bb0c-fd74840d9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denoise data with low pass filter\n",
    "cutoff = 1\n",
    "dt = 10/60  # 10 minutes in hours (cycles/hour)\n",
    "\n",
    "# We filter only these columns\n",
    "feats = [col for col in ['GenRPM', 'GenPh1Temp', 'WindSpeed', 'WindDirAbs', 'WindDirRel', 'Pitch', 'RotorRPM']]\n",
    "\n",
    "for col in feats:\n",
    "    x_filt, freqs, fft_filt = fft_lowpass_filter(df_filtered_train[col], dt, cutoff)\n",
    "    df_filtered_train.loc[:, col] = x_filt\n",
    "\n",
    "for col in feats:\n",
    "    x_filt, freqs, fft_filt = fft_lowpass_filter(x_filtered_test[col], dt, cutoff)\n",
    "    x_filtered_test.loc[:, col] = x_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d033a6a3-097d-4625-bcc0-41ffd509670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_train['RotorRPM'][:200], label='Raw signal')\n",
    "plt.plot(df_filtered_train['RotorRPM'][:200], label='Filtered signal')\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d07db3e-990c-4cb1-969a-11178de1b9a9",
   "metadata": {},
   "source": [
    "This looks promising.\n",
    "\n",
    "At first, it seems that we smooth too much, but this is a hyperparmaeter we can tune based on the model performance.\n",
    "\n",
    "Let's see what the performance looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16b6e9-d87a-4c7c-ac0d-d86bc85ceb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    df_filtered_train[filt_cols],\n",
    "    df_filtered_train['Power'], \n",
    "    x_filtered_test, \n",
    "    df_test['Power'], \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e7dbb-e909-484f-a2e1-51171fb8b78d",
   "metadata": {},
   "source": [
    "Now, we have Filter-by-difference + FFT filter.\n",
    "\n",
    "Let's compare with the resuls when we cleaned with only z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205094ad-cba7-4381-8b05-07af68bc5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 66.37,\n",
    "#  'cv_rmse': 96.43,\n",
    "#  'cv_mape': 7.76,\n",
    "#  'test_mae': 72.35,\n",
    "#  'test_rmse': 121.48,\n",
    "#  'test_mape': 9.34,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b697f1-7a46-49a6-9c54-5d57959bb220",
   "metadata": {},
   "source": [
    "We clealry see a huge improvement!\n",
    "\n",
    "And we have not yet engineered any features.\n",
    "\n",
    "From here, we can conclude that noise reduction is a good idea.\n",
    "\n",
    "But "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebba5a0-9afb-41db-b4b4-12855f4e0689",
   "metadata": {},
   "source": [
    "# Low-pass-FFT-data-leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e26799-e3be-4b63-8d81-8f8bd262449f",
   "metadata": {},
   "source": [
    "**Now, we also got a problem....**\n",
    "\n",
    "We just intorduced a data leakage.\n",
    "\n",
    "This is because we have fitted the filter on the ENTIRE test set which is not correct.\n",
    "\n",
    "In production, we will not have information about the future data points.\n",
    "\n",
    "So, we need to modify our filter and see how it would work in real production scenarion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e103998-4844-4371-b5a0-89150946fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_fft_lowpass(df: pd.DataFrame,\n",
    "                      cols,\n",
    "                      dt: float,\n",
    "                      cutoff: float,\n",
    "                      window_size: int = 500,\n",
    "                      online: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply fft_lowpass_filter to selected columns.\n",
    "\n",
    "    - online=False  → full-series filtering (good for TRAIN).\n",
    "    - online=True   → past-only sliding window (good for TEST/production).\n",
    "\n",
    "    Returns a new DataFrame with filtered columns.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    for col in cols:\n",
    "        x = df[col].to_numpy(dtype=float)\n",
    "\n",
    "        if not online:\n",
    "            # Full-series offline filter (simple train-time version)\n",
    "            x_filt, _, _ = fft_lowpass_filter(x, dt, cutoff)\n",
    "            df_out[col] = x_filt\n",
    "\n",
    "        else:\n",
    "            # Online-style: use only last `window_size` past points\n",
    "            y = np.full_like(x, np.nan, dtype=float)\n",
    "\n",
    "            for i in range(window_size - 1, len(x)):\n",
    "                window = x[i - window_size + 1 : i + 1]\n",
    "                x_filt_window, _, _ = fft_lowpass_filter(window, dt, cutoff)\n",
    "                y[i] = x_filt_window[-1]\n",
    "\n",
    "            df_out[col] = y\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd748d4-8d1f-481c-9f62-fbcbdcbf9f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a206a84-9581-4a91-8000-e627586de2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abs difference filter\n",
    "# Train set\n",
    "df_filtered_train = df_train.copy()\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)\n",
    "\n",
    "# Test set\n",
    "filt_cols = [col for col in df_test.columns if col != 'Power']\n",
    "x_filtered_test = df_test[filt_cols].copy()\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    if col != 'Power':\n",
    "        x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b55f16-3448-4166-afb7-89f071fcc99b",
   "metadata": {},
   "source": [
    "**First, let's apply the filter on the training set as a whole and sliding filter on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25af2d3-88a9-4d30-8555-7aa86f33467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 1\n",
    "dt = 10/60  # 10 minutes in hours (cycles/hour)\n",
    "window_size = 500\n",
    "\n",
    "feats = ['GenRPM', 'GenPh1Temp', 'WindSpeed',\n",
    "         'WindDirAbs', 'WindDirRel', 'Pitch', 'RotorRPM']\n",
    "\n",
    "# ---- TRAIN: full-series filtering ----\n",
    "df_filtered_train = apply_fft_lowpass(\n",
    "    df=df_filtered_train,\n",
    "    cols=feats,\n",
    "    dt=dt,\n",
    "    cutoff=cutoff,\n",
    "    online=False,          # full-series\n",
    ")\n",
    "\n",
    "# ---- TEST: sliding-window (production-like) ----\n",
    "x_filtered_test = apply_fft_lowpass(\n",
    "    df=x_filtered_test,\n",
    "    cols=feats,\n",
    "    dt=dt,\n",
    "    cutoff=cutoff,\n",
    "    window_size=window_size,\n",
    "    online=True,           # past-only window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d6da5-8538-4b8a-92be-7d0f7b6c3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    df_filtered_train[filt_cols],\n",
    "    df_filtered_train['Power'], \n",
    "    x_filtered_test, \n",
    "    df_test['Power'], \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fdf99c-bbd4-4e2b-a786-96afdd935caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is low pass with data leackage\n",
    "# 'cv_mae': 50.52,\n",
    "#  'cv_rmse': 72.21,\n",
    "#  'cv_mape': 6.08,\n",
    "#  'test_mae': 63.54,\n",
    "#  'test_rmse': 114.19,\n",
    "#  'test_mape': 8.18,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714578a-4068-4a20-a6ec-2e65a3490e3f",
   "metadata": {},
   "source": [
    "We see that on the test set we get a MUCH worse performance.\n",
    "\n",
    "Let's look at the error over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ff819f-f2e6-45a1-ae65-b5de1ccb2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=8.5, rolling_window=288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a83da-3d7a-405c-adaf-a89f72ab425a",
   "metadata": {},
   "source": [
    "**Let's apply the same way on the train and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8ebfe-3988-44ce-a799-ed2cdb2d1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdee319-f6e9-4000-a204-fddf2bd89f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abs difference filter\n",
    "# Train set\n",
    "df_filtered_train = df_train.copy()\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)\n",
    "\n",
    "# Test set\n",
    "filt_cols = [col for col in df_test.columns if col != 'Power']\n",
    "x_filtered_test = df_test[filt_cols].copy()\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    if col != 'Power':\n",
    "        x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c0122-931c-4bf7-99e9-7db51c714af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 1\n",
    "dt = 10/60      # 10-minute sampling in hours\n",
    "window_size = 200\n",
    "\n",
    "feats = ['GenRPM', 'GenPh1Temp', 'WindSpeed',\n",
    "         'WindDirAbs', 'WindDirRel', 'Pitch', 'RotorRPM']\n",
    "\n",
    "\n",
    "# ---- TRAIN: sliding window (same as production) ----\n",
    "df_filtered_train = apply_fft_lowpass(\n",
    "    df=df_filtered_train,\n",
    "    cols=feats,\n",
    "    dt=dt,\n",
    "    cutoff=cutoff,\n",
    "    window_size=window_size,\n",
    "    online=True     # <--- use sliding window in TRAIN too\n",
    ")\n",
    "\n",
    "# ---- TEST: sliding window (production-like) ----\n",
    "x_filtered_test = apply_fft_lowpass(\n",
    "    df=x_filtered_test,\n",
    "    cols=feats,\n",
    "    dt=dt,\n",
    "    cutoff=cutoff,\n",
    "    window_size=window_size,\n",
    "    online=True     # <--- same logic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8036ef-4339-4f0a-b028-4306a7e4feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    df_filtered_train[filt_cols],\n",
    "    df_filtered_train['Power'], \n",
    "    x_filtered_test, \n",
    "    df_test['Power'], \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c9b59-92c6-4772-b976-10679c1f927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=8.5, rolling_window=288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6823b-e1a9-4837-b712-af7b17f2a02a",
   "metadata": {},
   "source": [
    "We see that the performance is really poor, so we need to do something else.\n",
    "\n",
    "There are cut-off frequency filter alternatives that do not require information from the future.\n",
    "\n",
    "One of the, is a Butterworth filter. Let's try it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0e388-2d66-46c1-9fe9-7631a3781760",
   "metadata": {},
   "source": [
    "# Denoising: Butterworth filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5a91f-3005-465a-a6a3-7fcbf5a75394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_butter_lowpass(dt: float, cutoff: float, order: int = 4):\n",
    "    \"\"\"\n",
    "    Design a causal Butterworth low-pass filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dt : float\n",
    "        Sampling interval (e.g. 10/60 hours).\n",
    "    cutoff : float\n",
    "        Cutoff frequency in same units as 1/dt (e.g. cycles/hour).\n",
    "    order : int\n",
    "        Filter order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    b, a : np.ndarray\n",
    "        Filter coefficients.\n",
    "    \"\"\"\n",
    "    fs = 1.0 / dt          # sampling frequency\n",
    "    nyq = 0.5 * fs\n",
    "    wn = cutoff / nyq      # normalized cutoff in (0, 1)\n",
    "    b, a = butter(order, wn, btype=\"low\", analog=False)\n",
    "    return b, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0b6cc2-43f8-4d2b-9796-9d99d1dcfd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_butter_lowpass_causal(\n",
    "    df: pd.DataFrame,\n",
    "    cols,\n",
    "    b: np.ndarray,\n",
    "    a: np.ndarray,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a causal Butterworth low-pass filter to selected columns.\n",
    "\n",
    "    This simulates production-like streaming:\n",
    "    - processes samples in time order\n",
    "    - uses only past values (causal)\n",
    "    - uses lfilter with steady-state initial condition per column\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    for col in cols:\n",
    "        x = df[col].to_numpy(dtype=float)\n",
    "\n",
    "        # init state close to steady state at x[0]\n",
    "        zi = lfilter_zi(b, a) * x[0]\n",
    "\n",
    "        # causal filtering over the whole series\n",
    "        y, _ = lfilter(b, a, x, zi=zi)\n",
    "\n",
    "        df_out[col] = y\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ebf80-504c-44f7-8f67-916ac838358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abbfb3d-759f-4b22-b48b-872d9f67fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abs difference filter\n",
    "# Train set\n",
    "df_filtered_train = df_train.copy()\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)\n",
    "\n",
    "# Test set\n",
    "filt_cols = [col for col in df_test.columns if col != 'Power']\n",
    "x_filtered_test = df_test[filt_cols].copy()\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    if col != 'Power':\n",
    "        x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0b7e8-b671-4777-ba95-6a1f67375486",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 1\n",
    "dt = 10/60\n",
    "order = 1\n",
    "\n",
    "feats = ['GenRPM', 'GenPh1Temp', 'WindSpeed',\n",
    "         'WindDirAbs', 'WindDirRel', 'Pitch', 'RotorRPM']\n",
    "\n",
    "# 1) Design filter ONCE using train settings\n",
    "b, a = design_butter_lowpass(dt=dt, cutoff=cutoff, order=order)\n",
    "\n",
    "# 2) TRAIN: production-like causal filtering\n",
    "df_filtered_train = apply_butter_lowpass_causal(\n",
    "    df=df_filtered_train,\n",
    "    cols=feats,\n",
    "    b=b,\n",
    "    a=a,\n",
    ")\n",
    "\n",
    "# 3) TEST: same filter, same causal logic\n",
    "x_filtered_test = apply_butter_lowpass_causal(\n",
    "    df=x_filtered_test,\n",
    "    cols=feats,\n",
    "    b=b,\n",
    "    a=a,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98ab76-2db2-46e8-a0b6-4a3ad44ab880",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_train['RotorRPM'][\"2007-08-20\":\"2007-08-21\"], label='Raw signal')\n",
    "plt.plot(df_filtered_train['RotorRPM'][\"2007-08-20\":\"2007-08-21\"], label='Filtered signal')\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fd36f-b081-4e3b-8602-0c7fb660b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    df_filtered_train[filt_cols],\n",
    "    df_filtered_train['Power'], \n",
    "    x_filtered_test, \n",
    "    df_test['Power'], \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08646f91-bf11-4e01-adea-9366a5dd9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=8.5, rolling_window=288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274504ca-39d8-4fe3-aeba-ed06f389d963",
   "metadata": {},
   "source": [
    "We see that the predictions are way better now.\n",
    "\n",
    "With some hyperparameter tuning, this filter can be applied in production.\n",
    "\n",
    "However, we went the hard way. The easiest way to reduce noise would be to apply mean or median filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d87fb-bd43-47f5-afdf-365661c953bb",
   "metadata": {},
   "source": [
    "# Denoising: Mean filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e9957b-767a-4ebe-bce1-adf0db31c7d8",
   "metadata": {},
   "source": [
    "If these filters work with similar performance, it's better to use them instead because they are MCH easier to use.\n",
    "\n",
    "We can POTENTIALLY apply other filters like Gaussian filter, however, in production we would not be able to apply it in real-time.\n",
    "\n",
    "Because it requires information from the future relative to the current prediciton point (similar to the low-pass FFT filter)\n",
    "\n",
    "So, let's apply the mean and median sliding filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769ced12-94af-4ada-8fb6-af28fd5d5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ebab8c-f7b0-4c3d-bb76-9c1160478593",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_train = df_train.copy()\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be250209-4822-4699-a3ee-9fd1d36a7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_cols = [col for col in df_test.columns if col != 'Power']\n",
    "x_filtered_test = df_test[filt_cols].copy()\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    if col != 'Power':\n",
    "        x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed0140-0979-4955-82c3-b78bae224bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_signal(df, column, window, method=\"median\"):\n",
    "    \"\"\"\n",
    "    Smooth a time-series column using rolling mean or median.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe.\n",
    "    column : str\n",
    "        Column to smooth.\n",
    "    window : int\n",
    "        Rolling window size.\n",
    "    method : str\n",
    "        \"mean\"  -> rolling mean filter\n",
    "        \"median\" -> rolling median filter (robust smoothing)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_smoothed : pd.DataFrame\n",
    "        DataFrame with smoothed column.\n",
    "    \"\"\"\n",
    "\n",
    "    df_smoothed = df.copy()\n",
    "\n",
    "    if method == \"mean\":\n",
    "        df_smoothed[column] = df_smoothed[column].rolling(\n",
    "            window=window, min_periods=1, center=False\n",
    "        ).mean()\n",
    "\n",
    "    elif method == \"median\":\n",
    "        df_smoothed[column] = df_smoothed[column].rolling(\n",
    "            window=window, min_periods=1, center=False\n",
    "        ).median()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'mean' or 'median'\")\n",
    "\n",
    "    return df_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d717e8-0849-436c-aabc-dd5d27989d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_cols = [col for col in df_filtered_train.columns if col != 'Power']\n",
    "\n",
    "for col in smooth_cols:\n",
    "    df_filtered_train = smooth_signal(df_filtered_train, col, window=3, method=\"mean\") # test with 2, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5fe377-9df3-47c7-8413-ab61f040d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in smooth_cols:\n",
    "    x_filtered_test = smooth_signal(x_filtered_test, col, window=3, method=\"mean\") # test with 2, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba536ec0-3f0b-4861-808a-0ee97acdadb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_train['RotorRPM'][\"2007-08-20\":\"2007-08-21\"], label='Raw signal')\n",
    "plt.plot(df_filtered_train['RotorRPM'][\"2007-08-20\":\"2007-08-21\"], label='Filtered signal')\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae931c7-ac68-4b2c-b172-5c1988187923",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    df_filtered_train[filt_cols],\n",
    "    df_filtered_train['Power'], \n",
    "    x_filtered_test, \n",
    "    df_test['Power'], \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b6cb3-bd3c-4dec-9c68-04e3bffdeb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 55.36,\n",
    "#  'cv_rmse': 78.61,\n",
    "#  'cv_mape': 6.66,\n",
    "#  'test_mae': 67.9,\n",
    "#  'test_rmse': 118.16,\n",
    "#  'test_mape': 8.78,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f329a5db-bc5f-4460-b74e-b958172de2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=8.5, rolling_window=288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb06d49-350b-44c9-8f77-37c5797a0245",
   "metadata": {},
   "source": [
    "We see that average filter works worse and the Butterworth filter.\n",
    "\n",
    "However, it still works better that only z-score filter wich means in general noise reduction is required in this case.\n",
    "\n",
    "**If selecting between Butterwoth and Mean Filter, from the production perspective it's better to choose the Mean filter because it's way easier to use AND tune if required.** \n",
    "\n",
    "**Yes, we sacrifice some accuracy but gain confidence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60801f4e-9170-48d7-a269-7135921f1228",
   "metadata": {},
   "source": [
    "# Domain Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e7b54-dd3d-4938-969a-e1abeddb641c",
   "metadata": {},
   "source": [
    "Now, let's try to engineer some features based on basic physical relatonships.\n",
    "\n",
    "Note that before feature engineering, we clean the data not to propagate the noise and outliers into the engineered features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb6780f-9eb1-457d-9895-28e594118968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d048a-ce67-4628-9fcf-2956a401eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_train = df_train.copy()\n",
    "removed_dict = {}\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee7935-7513-44db-813e-6fdb956d37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_cols = [col for col in df_test.columns if col != 'Power']\n",
    "\n",
    "x_filtered_test = df_test[filt_cols].copy()\n",
    "removed_dict = {}\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    if col != 'Power':\n",
    "        x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7f6e4-4adc-4d23-8ff7-14d94b8062c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_cols = [col for col in df_filtered_train.columns if col != 'Power']\n",
    "\n",
    "for col in smooth_cols:\n",
    "    df_filtered_train = smooth_signal(df_filtered_train, col, window=3, method=\"mean\") # test with 2, 4, 5\n",
    "\n",
    "for col in smooth_cols:\n",
    "    x_filtered_test = smooth_signal(x_filtered_test, col, window=3, method=\"mean\") # test with 2, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed7a6e-86cf-4d6e-a156-4e4644e594d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create domain-specific engineered features for wind turbine data.\n",
    "\n",
    "    This function constructs physically meaningful combinations of signals\n",
    "    (e.g., wind speed, rotor RPM, pitch, thermal indicators) that often help\n",
    "    ML models capture turbine aerodynamics and thermal behavior.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe containing at least the following columns:\n",
    "        - WindSpeed, RotorRPM, Pitch, GenPh1Temp, GenBearTemp\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Copy of the input dataframe with additional engineered columns:\n",
    "        - WindSpeed3, TSR_proxy, RPM_WS2, RPM2_WS,\n",
    "          PitchFactor, WS3_pitch, ThermalLoad, ThermalProxy\n",
    "    \"\"\"\n",
    "    df_feat = df.copy()\n",
    "\n",
    "    # Aerodynamic proxies\n",
    "    df_feat[\"WindSpeed3\"] = df_feat[\"WindSpeed\"] ** 3\n",
    "    df_feat[\"TSR_proxy\"] = (df_feat[\"RotorRPM\"] / df_feat[\"WindSpeed\"]).replace(0, np.nan)\n",
    "    df_feat[\"RPM_WS2\"] = df_feat[\"RotorRPM\"] * df_feat[\"WindSpeed\"] ** 2\n",
    "    df_feat[\"RPM2_WS\"] = df_feat[\"RotorRPM\"] ** 2 * df_feat[\"WindSpeed\"]\n",
    "\n",
    "    # Pitch effect (proxy for aerodynamic efficiency)\n",
    "    df_feat[\"PitchFactor\"] = np.exp(-0.1 * df_feat[\"Pitch\"])\n",
    "    df_feat[\"WS3_pitch\"] = df_feat[\"WindSpeed3\"] * df_feat[\"PitchFactor\"]\n",
    "\n",
    "    # Thermal proxies\n",
    "    df_feat[\"ThermalLoad\"] = df_feat[\"GenPh1Temp\"] + df_feat[\"GenBearTemp\"]\n",
    "    df_feat[\"ThermalProxy\"] = df_feat[\"RPM_WS2\"] * df_feat[\"ThermalLoad\"]\n",
    "\n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941afca2-b7c2-486c-af71-f5cc91d2eafd",
   "metadata": {},
   "source": [
    "### Engineered Feature Explanations\n",
    "\n",
    "### WindSpeed3\n",
    "`WindSpeed^3`  \n",
    "Power generation is roughly proportional to the cube of wind speed.  \n",
    "**Why useful:** captures nonlinear aerodynamic effects related to power output.\n",
    "\n",
    "### TSR_proxy\n",
    "`RotorRPM / WindSpeed`  \n",
    "Approximate Tip-Speed Ratio (TSR), a key aerodynamic efficiency measure.  \n",
    "**Why useful:** indicates whether the turbine is operating in an optimal aerodynamic regime.\n",
    "\n",
    "### RPM_WS2\n",
    "`RotorRPM * WindSpeed^2`  \n",
    "Combined aerodynamic/rotational loading term.  \n",
    "**Why useful:** highlights high-load operational conditions.\n",
    "\n",
    "### RPM2_WS\n",
    "`RotorRPM^2 * WindSpeed`  \n",
    "Places more emphasis on rotor behavior relative to wind.  \n",
    "**Why useful:** detects abnormal RPM behavior under specific wind speeds.\n",
    "\n",
    "### PitchFactor\n",
    "`exp(-0.1 * Pitch)`  \n",
    "Approximates how pitching the blades reduces aerodynamic efficiency.  \n",
    "**Why useful:** captures control effects that reduce effective power capture.\n",
    "\n",
    "### WS3_pitch\n",
    "`WindSpeed3 * PitchFactor`  \n",
    "Couples aerodynamic potential with pitch-induced efficiency loss.  \n",
    "**Why useful:** represents effective aerodynamic power rather than theoretical.\n",
    "\n",
    "### ThermalLoad\n",
    "`GenPh1Temp + GenBearTemp`  \n",
    "Simple indicator of thermal stress in generator and bearing.  \n",
    "**Why useful:** overheating is a common early sign of mechanical degradation.\n",
    "\n",
    "### ThermalProxy\n",
    "`RPM_WS2 * ThermalLoad`  \n",
    "Thermal–mechanical load interaction term.  \n",
    "**Why useful:** sensitive to slow degradation such as increasing friction or bearing wear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef9a2fd-623d-4eb1-ad14-cd00a2d5bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_train = get_domain_features(df_filtered_train)\n",
    "x_filtered_test = get_domain_features(x_filtered_test)\n",
    "filt_cols = [col for col in df_filtered_train if col != 'Power']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb738eb-6fa8-4db6-8eee-8d7ed3bed003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of all columns with the target\n",
    "corr = df_filtered_train.corr()['Power'].drop('Power')\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "correlations_sorted = corr.reindex(corr.abs().sort_values(ascending=False).index)\n",
    "correlations_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf040eb1-ac65-4cf2-a86b-f7c38ab61187",
   "metadata": {},
   "source": [
    "We see that most of the enginered features have good correlation with the target, better than many of the raw features.\n",
    "\n",
    "This is promising to get better model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5ea737-f910-4588-88d8-21ac13ee5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    df_filtered_train[filt_cols],\n",
    "    df_filtered_train['Power'], \n",
    "    x_filtered_test, \n",
    "    df_test['Power'], \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baea91d-b914-4683-b6bc-575a5d23360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 57.05,\n",
    "#  'cv_rmse': 80.15,\n",
    "#  'cv_mape': 6.89,\n",
    "#  'test_mae': 69.15,\n",
    "#  'test_rmse': 119.36,\n",
    "#  'test_mape': 8.94,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782ceb27-98f4-4e6e-b436-8adf95178a56",
   "metadata": {},
   "source": [
    "We see the it has not given us almost any improvement, even though the engineered featured look good.\n",
    "\n",
    "Let's check if the model used the features at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cd148-2072-4e8d-9c47-05d3944f9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values\n",
    "importances = eval_results['model'].feature_importances_\n",
    "cols = df_filtered_train[filt_cols].columns\n",
    "\n",
    "# Combine into DataFrame\n",
    "feat_imp = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": cols,\n",
    "        \"importance\": importances\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4f40c-534b-4bad-bebc-2a2d64ce6273",
   "metadata": {},
   "source": [
    "We see that these features, despite being well correlated with the target, are not used much by the model.\n",
    "\n",
    "Overall, it can happen because:\n",
    "- Random Forest already models nonlinear relations, so engineered features may be redundant.\n",
    "- High correlation doesn’t mean the feature adds new information beyond existing inputs.\n",
    "- Strong raw predictors (e.g., GenRPM) dominate splits, reducing importance of derived features.\n",
    "\n",
    "Let's see if statistical features are able to improve the model. For the baseline mode, they did."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe4dd3-c812-4b4e-b02a-3a1d7d81e672",
   "metadata": {},
   "source": [
    "# Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f7146c-0a78-4934-a771-6cb1c1829c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdba444-0afa-426a-beb3-72e7f6e4773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "df_filtered_train = df_train.copy()\n",
    "removed_dict = {}\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)\n",
    "\n",
    "filt_cols = [col for col in df_test.columns if col != 'Power']\n",
    "\n",
    "# Test\n",
    "x_filtered_test = df_test[filt_cols].copy()\n",
    "removed_dict = {}\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    if col != 'Power':\n",
    "        x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6f45e-73d4-4cfe-b373-434addd223d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_cols = [col for col in df_filtered_train.columns if col != 'Power']\n",
    "\n",
    "for col in smooth_cols:\n",
    "    df_filtered_train = smooth_signal(df_filtered_train, col, window=3, method=\"mean\") # test with 2, 4, 5\n",
    "\n",
    "for col in smooth_cols:\n",
    "    x_filtered_test = smooth_signal(x_filtered_test, col, window=3, method=\"mean\") # test with 2, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78072194-4dcf-499f-b717-3ff5778ce31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_train = add_rolling_features(\n",
    "    df_filtered_train,\n",
    "    window_sizes=[3, 6], # window_sizes=[5, 10, 15],\n",
    "    columns=['GenRPM', 'GenPh1Temp', 'WindSpeed', 'WindDirAbs', 'WindDirRel', 'Pitch', 'RotorRPM'], \n",
    "    stats=['std','max', 'skew'], # 'min', 'mean' - we already have mean\n",
    "    drop_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5351adc-7058-426a-b25c-9e683766ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_filtered_test = add_rolling_features(\n",
    "    x_filtered_test,\n",
    "    window_sizes=[3, 6], # window_sizes=[5, 10, 15],\n",
    "    columns=['GenRPM', 'GenPh1Temp', 'WindSpeed', 'WindDirAbs', 'WindDirRel', 'Pitch', 'RotorRPM'], \n",
    "    stats=['std','max', 'skew'], #'min',  'mean' - we already have mean\n",
    "    drop_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3978e240-e543-4a0b-98e7-5c4c2c4e57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_cols = [col for col in df_filtered_train if col != 'Power']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e9d0dd-6ce4-4e85-b47d-314db7153351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    df_filtered_train[filt_cols],\n",
    "    df_filtered_train['Power'], \n",
    "    x_filtered_test, \n",
    "    df_test['Power'], \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de5a0aa-76b6-427e-9dcd-538f3a44b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## These are the metrics with Engineered features\n",
    "# 'cv_mae': 56.74,\n",
    "#  'cv_rmse': 79.68,\n",
    "#  'cv_mape': 6.86,\n",
    "#  'test_mae': 69.2,\n",
    "#  'test_rmse': 119.4,\n",
    "#  'test_mape': 8.95,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75011de-02a0-4cf8-888d-cb558439c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=8.5, rolling_window=288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db657b8-6a99-40a6-a10a-ce3be9614d91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract values\n",
    "importances = eval_results['model'].feature_importances_\n",
    "cols = df_filtered_train[filt_cols].columns\n",
    "\n",
    "# Combine into DataFrame\n",
    "feat_imp = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": cols,\n",
    "        \"importance\": importances\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8330b-8641-4463-9279-10720d243da6",
   "metadata": {},
   "source": [
    "We see that we have not got much of improvement.\n",
    "\n",
    "This can be the case because by filtering the noise using averaging, we already extrated the main info.\n",
    "\n",
    "The rest of the infromation can be bettter extracted by cleaning the noise better (as we say with data leacked low-pass FFT filter).\n",
    "\n",
    "This is definitely the room for improvement here.\n",
    "\n",
    "From the feature importance we see that some of the max features are the importance TOP, but they are way less important that the raw GenRPM feature.\n",
    "\n",
    "**To make out pipeline general, we can use some of the statistical features, we will create the transformation fuctions, so that later in production and after deeper research more features can be added if required.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d9601-fbaa-4ceb-b9ee-39555846a274",
   "metadata": {},
   "source": [
    "# Lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af7f0e-9476-45ea-af35-e1c14047081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951e834-06a2-4776-9fb4-5526c689ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering by difference\n",
    "# Train\n",
    "df_filtered_train = df_train.copy()\n",
    "removed_dict = {}\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)\n",
    "\n",
    "filt_cols = [col for col in df_test.columns if col != 'Power']\n",
    "\n",
    "# Test\n",
    "x_filtered_test = df_test[filt_cols].copy()\n",
    "removed_dict = {}\n",
    "\n",
    "for col, thr in abs_diff_thresholds.items():\n",
    "    if col != 'Power':\n",
    "        x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2778d01-7450-4697-8326-9e9febf0a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_cols = [col for col in df_filtered_train.columns if col != 'Power']\n",
    "\n",
    "for col in smooth_cols:\n",
    "    df_filtered_train = smooth_signal(df_filtered_train, col, window=3, method=\"mean\") # test with 2, 4, 5\n",
    "\n",
    "for col in smooth_cols:\n",
    "    x_filtered_test = smooth_signal(x_filtered_test, col, window=3, method=\"mean\") # test with 2, 4, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3590eff3-1aab-4dff-a968-39869bf2676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's keep max features for now\n",
    "df_filtered_train = add_rolling_features(\n",
    "    df_filtered_train,\n",
    "    window_sizes=[3, 6],\n",
    "    columns=['GenRPM', 'GenPh1Temp', 'WindSpeed'], \n",
    "    stats=['max'],\n",
    "    drop_na=False\n",
    ")\n",
    "\n",
    "x_filtered_test = add_rolling_features(\n",
    "    x_filtered_test,\n",
    "    window_sizes=[3, 6],\n",
    "    columns=['GenRPM', 'GenPh1Temp', 'WindSpeed'], \n",
    "    stats=['max'],\n",
    "    drop_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2570a-6c25-404f-a3c7-094ea7884a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding lagged features\n",
    "df_filtered_train = add_lag_features(df_filtered_train, columns=['GenRPM', 'GenPh1Temp'], lags=[1, 2, 3], drop_na=False)\n",
    "x_filtered_test = add_lag_features(x_filtered_test, columns=['GenRPM', 'GenPh1Temp'], lags=[1, 2, 3], drop_na=False)\n",
    "filt_cols = [col for col in df_filtered_train if col != 'Power']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689f825-b9c2-4e5c-ad98-dc7e907a72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    df_filtered_train[filt_cols],\n",
    "    df_filtered_train['Power'], \n",
    "    x_filtered_test, \n",
    "    df_test['Power'], \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08db34-6ad5-4a89-8f0a-354d7ef6d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 56.56,\n",
    "#  'cv_rmse': 79.71,\n",
    "#  'cv_mape': 6.81,\n",
    "#  'test_mae': 68.48,\n",
    "#  'test_rmse': 118.48,\n",
    "#  'test_mape': 8.85,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44ad18-1fd8-41d0-846e-0187bfb23040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is baseline\n",
    "# 'cv_mae': 55.25,\n",
    "#  'cv_rmse': 80.82,\n",
    "#  'cv_mape': 6.58,\n",
    "#  'test_mae': 64.44,\n",
    "#  'test_rmse': 114.03,\n",
    "#  'test_mape': 8.3,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53fa4d-cfc4-4036-b2eb-a9777bdc3aba",
   "metadata": {},
   "source": [
    "We see that we have not got much of improvement.\n",
    "\n",
    "Moreover, we see pretty much the same performance as in out baseline.\n",
    "\n",
    "The most likely reason is that the data has a lot of noise and the transformations we applied focus on more or less the same - remove outliers and smooth the noise to extract the true signal.\n",
    "\n",
    "The biggest potential boost we have achieved was using the low pass FFT filter which however introduced a data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf451c-eb71-442a-9d77-9f765729d038",
   "metadata": {},
   "source": [
    "# Conclusions from deep data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ba128-ca6c-4c09-b777-612f5589c768",
   "metadata": {},
   "source": [
    "From all the analysis above, we can conclude that the future improvement can be achieved by better data denoising\n",
    "\n",
    "To make out pipeline more flexible for future development, we will introduce:\n",
    "- Filtering by difference to remove the outliers\n",
    "- Smoothing the noise with mean filter (can be substituted by Butterworth filter)\n",
    "- Adding lagged features - max value, but we will use the function that can add more features\n",
    "- Adding lagged features (except the target)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506bb20f-fab8-4944-8d0c-75ed8b4fe12f",
   "metadata": {},
   "source": [
    "# Model Selection: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2192cc-2faa-4e8f-ba65-c71ba91f471a",
   "metadata": {},
   "source": [
    "First, let's prepare the function that creates the final cleaning and feature engineering pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a6f48-8d6d-4d64-95b7-353ec814bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    abs_diff_thresholds: Dict[str, float],\n",
    "    smooth_window: int\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Clean and enrich train/test turbine datasets:\n",
    "    - remove outliers using absolute-difference thresholds,\n",
    "    - denoise selected signals via FFT low-pass filter,\n",
    "    - add lag features,\n",
    "    - add rolling statistical features.\n",
    "\n",
    "    Assumptions\n",
    "    ----------\n",
    "    - `df_train` and `df_test` contain a target column named 'Power'.\n",
    "    - `remove_diff_outliers` does NOT drop rows (it forward-fills / smooths values).\n",
    "    - Columns required for denoising and feature engineering are present:\n",
    "      ['GenRPM', 'GenPh1Temp', 'WindSpeed', 'WindDirAbs',\n",
    "       'WindDirRel', 'Pitch', 'RotorRPM'].\n",
    "    - Helper functions `remove_diff_outliers`, `fft_lowpass_filter`,\n",
    "      `add_lag_features`, and `add_rolling_features` are defined elsewhere.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pd.DataFrame\n",
    "        Training dataset with features and target ('Power').\n",
    "    df_test : pd.DataFrame\n",
    "        Test dataset with features and target ('Power').\n",
    "    abs_diff_thresholds : Dict[str, float]\n",
    "        Mapping from column name to absolute-difference threshold used by\n",
    "        `remove_diff_outliers` to smooth outliers.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : pd.DataFrame\n",
    "        Cleaned and feature-engineered training features (without 'Power').\n",
    "    y_train : pd.Series\n",
    "        Cleaned training target ('Power').\n",
    "    X_test : pd.DataFrame\n",
    "        Cleaned and feature-engineered test features (without 'Power').\n",
    "    y_test : pd.Series\n",
    "        Original test target ('Power'), not filtered by outlier logic.\n",
    "    \"\"\"\n",
    "    df_filtered_train = df_train.copy()\n",
    "\n",
    "    # Applying filter by difference\n",
    "    # Remove outliers on the train set (features + target)\n",
    "    for col, thr in abs_diff_thresholds.items():\n",
    "        df_filtered_train, removed_idx = remove_diff_outliers(df_filtered_train, col, thr)\n",
    "\n",
    "    # Remove outliers on the test set (features only)\n",
    "    filt_cols = [col for col in df_test.columns if col != \"Power\"]\n",
    "    x_filtered_test = df_test[filt_cols].copy()\n",
    "\n",
    "    for col, thr in abs_diff_thresholds.items():\n",
    "        if col != \"Power\":\n",
    "            x_filtered_test, removed_idx = remove_diff_outliers(x_filtered_test, col, thr)\n",
    "\n",
    "    # Denoising\n",
    "    smooth_cols = [col for col in df_filtered_train.columns if col != 'Power']\n",
    "\n",
    "    for col in smooth_cols:\n",
    "        df_filtered_train = smooth_signal(df_filtered_train, col, window=smooth_window, method=\"mean\")\n",
    "    \n",
    "    for col in smooth_cols:\n",
    "        x_filtered_test = smooth_signal(x_filtered_test, col, window=smooth_window, method=\"mean\")\n",
    "\n",
    "\n",
    "    # Add Lag Features in train and test datasets\n",
    "    df_filtered_train = add_lag_features(\n",
    "        df_filtered_train,\n",
    "        columns=[\"GenRPM\", \"GenPh1Temp\", 'WindSpeed'],\n",
    "        lags=[1, 2, 3],\n",
    "        drop_na=False,\n",
    "    )\n",
    "    x_filtered_test = add_lag_features(\n",
    "        x_filtered_test,\n",
    "        columns=[\"GenRPM\", \"GenPh1Temp\", 'WindSpeed'],\n",
    "        lags=[1, 2, 3],\n",
    "        drop_na=False,\n",
    "    )\n",
    "\n",
    "    # Add Statistical Features in the train dataset\n",
    "    df_filtered_train = add_rolling_features(\n",
    "        df_filtered_train,\n",
    "        window_sizes=[3, 6],  # e.g. [5, 10, 15]\n",
    "        columns=[\"GenRPM\", \"GenPh1Temp\", \"WindSpeed\",\n",
    "                 \"WindDirAbs\", \"WindDirRel\", \"Pitch\", \"RotorRPM\"],\n",
    "        stats=[\"max\"],\n",
    "        drop_na=False,\n",
    "    )\n",
    "\n",
    "    # Add Statistical Features in the test dataset\n",
    "    x_filtered_test = add_rolling_features(\n",
    "        x_filtered_test,\n",
    "        window_sizes=[3, 6],  # e.g. [5, 10, 15]\n",
    "        columns=[\"GenRPM\", \"GenPh1Temp\", \"WindSpeed\",\n",
    "                 \"WindDirAbs\", \"WindDirRel\", \"Pitch\", \"RotorRPM\"],\n",
    "        stats=[\"max\"],\n",
    "        drop_na=False,\n",
    "    )\n",
    "\n",
    "    X_train = df_filtered_train.drop(columns=\"Power\")\n",
    "    y_train = df_filtered_train[\"Power\"]\n",
    "    X_test = x_filtered_test\n",
    "    y_test = df_test[\"Power\"]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535b42b-dbea-4a57-8470-5d4bdb9e275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d44400-d327-47a8-847f-3d0abfa95eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_clean_data(df_train, df_test, abs_diff_thresholds, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e34af9-c55c-4d76-b490-4ee1c3124700",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results = eval_model(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    3,\n",
    "    'RF',\n",
    "    params\n",
    ")\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ee0c2-f9b2-4a43-83b1-aa0bf709af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 55.81,\n",
    "#  'cv_rmse': 78.73,\n",
    "#  'cv_mape': 6.72,\n",
    "#  'test_mae': 67.58,\n",
    "#  'test_rmse': 117.83,\n",
    "#  'test_mape': 8.72,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5b83b-642e-4867-83d3-430479118171",
   "metadata": {},
   "source": [
    "This seems to be the best model so far, even though the imporvement is not big compared to the model fitted just on cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74cdc4-ec79-4301-b1a2-e81b5ebe30f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_filtered_test, df_test['Power'], eval_results['y_pred_test'], error='mape', error_threshold=8.5, rolling_window=288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f5024-7a14-4f31-8817-966659396bcf",
   "metadata": {},
   "source": [
    "# Model Selection: CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edba988-5ccd-4f19-bf49-8161b8849340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "\n",
    "# Split the data\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f40cce-c3d5-4506-9245-49383b3b3295",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_clean_data(df_train, df_test, abs_diff_thresholds, smooth_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b17d5f-1c9a-494b-b0da-a5c1d50c8614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use default CatBoost Parameters\n",
    "params = {\n",
    "    \"iterations\": 1000,       # number of trees\n",
    "    \"learning_rate\": 0.03,    # shrinkage\n",
    "    \"depth\": 6,               # tree depth\n",
    "    \"l2_leaf_reg\": 3.0,       # L2 regularization\n",
    "    \"random_seed\": SEED,         # reproducibility\n",
    "    \"loss_function\": \"RMSE\",  # regression loss\n",
    "    \"verbose\": False          # silence logs\n",
    "}\n",
    "\n",
    "eval_results = eval_model(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    3,\n",
    "    'CatBoost',\n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81c5c51-988d-4940-ba6e-fcc91f9e94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_filtered_test, df_test['Power'], eval_results['y_pred_test'], error='mape', error_threshold=8.5, rolling_window=288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b973f8a-b957-4e6a-a937-01a186e61516",
   "metadata": {},
   "source": [
    "We see that CatBoost with default parameters gives slightly better performance (but not much improvement).\n",
    "\n",
    "It can potentially get better if we tune hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0dda90-64d8-49ff-ab58-5014feb0eefc",
   "metadata": {},
   "source": [
    "# Model Selection: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67840a44-fe17-4cb3-8ea3-361b2979cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Many-to-one LSTM regressor for time series prediction.\n",
    "\n",
    "    Architecture\n",
    "    ------------\n",
    "    - LSTM layers with configurable hidden size and number of layers\n",
    "    - Fully connected head: hidden_size -> hidden_size//2 -> 1\n",
    "    - Uses last hidden state from final LSTM layer for prediction\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        hidden_size: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize LSTM regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features : int\n",
    "            Number of input features per time step.\n",
    "        hidden_size : int, default=128\n",
    "            Hidden size of LSTM layers.\n",
    "        num_layers : int, default=2\n",
    "            Number of stacked LSTM layers.\n",
    "        dropout : float, default=0.0\n",
    "            Dropout rate applied between LSTM layers (only if num_layers > 1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        # a slightly richer head than just one Linear\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM regressor.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, seq_len, n_features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        out, (h_n, c_n) = self.lstm(x)      # h_n: (num_layers, B, H)\n",
    "        last_hidden = h_n[-1]               # (B, H)\n",
    "        return self.head(last_hidden)       # (B, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a96399-c442-4c6e-aa23-de5f014e9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(\n",
    "    x_np: np.ndarray,\n",
    "    y_np: np.ndarray,\n",
    "    seq_len: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create sliding window sequences for time series data.\n",
    "\n",
    "    Creates sequences where each input sequence contains `seq_len` consecutive\n",
    "    time steps, and the corresponding target is the value at the next time step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_np : np.ndarray\n",
    "        Input features array of shape (n_samples, n_features).\n",
    "    y_np : np.ndarray\n",
    "        Target values array of shape (n_samples,).\n",
    "    seq_len : int\n",
    "        Length of each input sequence (number of time steps to look back).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_seq : np.ndarray\n",
    "        Sequence inputs of shape (n_samples - seq_len, seq_len, n_features).\n",
    "    y_seq : np.ndarray\n",
    "        Sequence targets of shape (n_samples - seq_len,).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> x = np.array([[1], [2], [3], [4], [5]])\n",
    "    >>> y = np.array([10, 20, 30, 40, 50])\n",
    "    >>> X_seq, y_seq = make_sequences(x, y, seq_len=2)\n",
    "    >>> X_seq.shape\n",
    "    (3, 2, 1)\n",
    "    >>> y_seq\n",
    "    array([30, 40, 50])\n",
    "    \"\"\"\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(x_np) - seq_len):\n",
    "        X_seq.append(x_np[i:i + seq_len])\n",
    "        y_seq.append(y_np[i + seq_len])\n",
    "    return np.stack(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbdd20-b612-4a1b-94d8-36966baf1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(\n",
    "    x_train_df: pd.DataFrame,\n",
    "    y_train_ser: pd.Series,\n",
    "    x_val_df: pd.DataFrame,\n",
    "    y_val_ser: pd.Series,\n",
    "    model_params: Optional[Dict[str, Any]] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, nn.Module]:\n",
    "    \"\"\"\n",
    "    Fit a many-to-one LSTM on (x_train, y_train) and predict on x_val.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1. Standardize X with StandardScaler\n",
    "    2. Standardize y with StandardScaler\n",
    "    3. Create sequences of length seq_len\n",
    "    4. Build LSTM (hidden layers, dropout) on GPU if available\n",
    "    5. Train with MSE loss + Adam\n",
    "    6. Predict on validation data\n",
    "    7. Invert scaling for predictions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train_df : pd.DataFrame\n",
    "        Training features.\n",
    "    y_train_ser : pd.Series\n",
    "        Training target.\n",
    "    x_val_df : pd.DataFrame\n",
    "        Validation features.\n",
    "    y_val_ser : pd.Series\n",
    "        Validation target.\n",
    "    model_params : dict, optional\n",
    "        Dictionary containing hyperparameters:\n",
    "        - seq_len: int, default=48\n",
    "            Sequence length for LSTM input\n",
    "        - hidden_size: int, default=128\n",
    "            Hidden size of LSTM layers\n",
    "        - num_layers: int, default=2\n",
    "            Number of LSTM layers\n",
    "        - dropout: float, default=0.0\n",
    "            Dropout rate (only applied if num_layers > 1)\n",
    "        - lr: float, default=1e-3\n",
    "            Learning rate for Adam optimizer\n",
    "        - batch_size: int, default=128\n",
    "            Batch size for training\n",
    "        - epochs: int, default=40\n",
    "            Number of training epochs\n",
    "        - verbose: bool, default=True\n",
    "            Whether to print training progress\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_val_aligned : np.ndarray\n",
    "        Validation target (original scale, aligned with predictions).\n",
    "    y_pred_val : np.ndarray\n",
    "        Validation predictions (original scale).\n",
    "    model : nn.Module\n",
    "        Trained LSTM model.\n",
    "    \"\"\"\n",
    "\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "\n",
    "    # ---- hyperparams (with sensible defaults) ----\n",
    "    seq_len = model_params.get(\"seq_len\", 48)\n",
    "    hidden_size = model_params.get(\"hidden_size\", 128)\n",
    "    num_layers = model_params.get(\"num_layers\", 2)\n",
    "    dropout = model_params.get(\"dropout\", 0.0)\n",
    "    lr = model_params.get(\"lr\", 1e-3)\n",
    "    batch_size = model_params.get(\"batch_size\", 128)\n",
    "    epochs = model_params.get(\"epochs\", 40)\n",
    "    verbose = model_params.get(\"verbose\", True)\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---- scale X ----\n",
    "    x_scaler = StandardScaler()\n",
    "    x_train_scaled = x_scaler.fit_transform(x_train_df.values.astype(np.float32))\n",
    "    x_val_scaled = x_scaler.transform(x_val_df.values.astype(np.float32))\n",
    "\n",
    "    # ---- scale y ----\n",
    "    y_train = y_train_ser.values.astype(np.float32).reshape(-1, 1)\n",
    "    y_val = y_val_ser.values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train).flatten()\n",
    "    y_val_scaled = y_scaler.transform(y_val).flatten()\n",
    "\n",
    "    # ---- make sequences (in scaled space) ----\n",
    "    X_train_seq, y_train_seq = make_sequences(x_train_scaled, y_train_scaled, seq_len)\n",
    "    X_val_seq, y_val_seq = make_sequences(x_val_scaled, y_val_scaled, seq_len)\n",
    "\n",
    "    # We'll also keep the ORIGINAL-scale validation y for metrics later:\n",
    "    _, y_val_orig_seq = make_sequences(x_val_scaled, y_val.flatten(), seq_len)\n",
    "\n",
    "    train_ds = TensorDataset(\n",
    "        torch.from_numpy(X_train_seq),\n",
    "        torch.from_numpy(y_train_seq).view(-1, 1)\n",
    "    )\n",
    "    val_ds = TensorDataset(\n",
    "        torch.from_numpy(X_val_seq),\n",
    "        torch.from_numpy(y_val_seq).view(-1, 1)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ---- model ----\n",
    "    n_features = x_train_df.shape[1]\n",
    "    model = LSTMRegressor(\n",
    "        n_features=n_features,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # ---- training loop with progress ----\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_ds)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        val_loss /= len(val_ds)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch:03d}/{epochs} | train MSE (scaled): {train_loss:.4f} | \"\n",
    "                  f\"val MSE (scaled): {val_loss:.4f}\")\n",
    "\n",
    "    # ---- predict on validation ----\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            preds = model(X_batch).cpu().numpy().flatten()\n",
    "            preds_list.append(preds)\n",
    "\n",
    "    y_pred_val_scaled = np.concatenate(preds_list).reshape(-1, 1)\n",
    "    y_pred_val = y_scaler.inverse_transform(y_pred_val_scaled).flatten()\n",
    "\n",
    "    # original-scale y for metrics (same length as y_pred_val)\n",
    "    y_val_aligned = y_val_orig_seq\n",
    "\n",
    "    return y_val_aligned, y_pred_val, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aab2156-028b-438c-b148-026846a90af6",
   "metadata": {},
   "source": [
    "Let's add LSTM and MLP to the eval_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c3296-a3ff-4a61-8a66-df94b56a4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    x_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    n_splits: int = 3,\n",
    "    model_name: str = \"RF\",\n",
    "    model_params: Union[Mapping[str, Any], None] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a regression model with time-series CV, then refit on full train\n",
    "    and evaluate on test.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "          \"cv_mae\", \"cv_rmse\", \"cv_mape\",\n",
    "          \"test_mae\", \"test_rmse\", \"test_mape\",\n",
    "          \"y_pred_test\",\n",
    "          \"model\" \n",
    "        }\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_mae_list, cv_rmse_list, cv_mape_list = [], [], []\n",
    "\n",
    "    # ------------------ CV ------------------\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(x_train), 1):\n",
    "        x_train_cv = x_train.iloc[train_idx, :]\n",
    "        x_val_cv   = x_train.iloc[val_idx, :]\n",
    "        y_train_cv = y_train.iloc[train_idx]\n",
    "        y_val_cv   = y_train.iloc[val_idx]\n",
    "\n",
    "        if model_name == \"LSTM\":\n",
    "            y_val_aligned, y_pred_cv, model = fit_lstm(\n",
    "                x_train_cv, y_train_cv, x_val_cv, y_val_cv, model_params\n",
    "            )\n",
    "            mae_err, rmse_err, mape_err = compute_metrics(y_val_aligned, y_pred_cv)\n",
    "\n",
    "        elif model_name == \"MLP\":\n",
    "            y_val_aligned, y_pred_cv, model = fit_mlp(\n",
    "                x_train_cv, y_train_cv, x_val_cv, y_val_cv, model_params\n",
    "            )\n",
    "            mae_err, rmse_err, mape_err = compute_metrics(y_val_aligned, y_pred_cv)\n",
    "\n",
    "        else:\n",
    "            x_scaler = StandardScaler()\n",
    "            x_scaled_cv_train = x_scaler.fit_transform(x_train_cv)\n",
    "            x_scaled_cv_val   = x_scaler.transform(x_val_cv)\n",
    "\n",
    "            if model_name == \"RF\":\n",
    "                model = RF(**model_params)\n",
    "            elif model_name == \"LinReg\":\n",
    "                model = LinearRegression(**model_params)\n",
    "            elif model_name == \"CatBoost\":\n",
    "                params = dict(model_params)\n",
    "                params.setdefault(\"verbose\", False)\n",
    "                params.setdefault(\"random_seed\", SEED)\n",
    "                model = CatBoostRegressor(**params)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "            model.fit(x_scaled_cv_train, y_train_cv)\n",
    "            y_pred_cv = model.predict(x_scaled_cv_val)\n",
    "            mae_err, rmse_err, mape_err = compute_metrics(y_val_cv, y_pred_cv)\n",
    "\n",
    "        cv_mae_list.append(mae_err)\n",
    "        cv_rmse_list.append(rmse_err)\n",
    "        cv_mape_list.append(mape_err)\n",
    "\n",
    "    cv_mae  = float(np.mean(cv_mae_list))\n",
    "    cv_rmse = float(np.mean(cv_rmse_list))\n",
    "    cv_mape = float(np.mean(cv_mape_list))\n",
    "\n",
    "    # ----------- Final model on full training -------------\n",
    "    if model_name == \"LSTM\":\n",
    "        y_test_aligned, y_pred_test_aligned, model = fit_lstm(\n",
    "            x_train, y_train, x_test, y_test, model_params\n",
    "        )\n",
    "        seq_len = model_params.get(\"seq_len\", 48)\n",
    "\n",
    "        y_pred_test = np.full(len(y_test), np.nan, dtype=float)\n",
    "        y_pred_test[seq_len:] = y_pred_test_aligned\n",
    "\n",
    "        mae_err_test, rmse_err_test, mape_err_test = compute_metrics(\n",
    "            y_test.values[seq_len:], y_pred_test[seq_len:]\n",
    "        )\n",
    "\n",
    "    elif model_name == \"MLP\":\n",
    "        y_test_aligned, y_pred_test, model = fit_mlp(\n",
    "            x_train, y_train, x_test, y_test, model_params\n",
    "        )\n",
    "        mae_err_test, rmse_err_test, mape_err_test = compute_metrics(\n",
    "            y_test_aligned, y_pred_test\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        if model_name == \"RF\":\n",
    "            model = RF(**model_params)\n",
    "        elif model_name == \"LinReg\":\n",
    "            model = LinearRegression(**model_params)\n",
    "        elif model_name == \"CatBoost\":\n",
    "            params = dict(model_params)\n",
    "            params.setdefault(\"verbose\", False)\n",
    "            params.setdefault(\"random_seed\", SEED)\n",
    "            model = CatBoostRegressor(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "        x_scaler = StandardScaler()\n",
    "        x_scaled_train = x_scaler.fit_transform(x_train)\n",
    "        x_scaled_test  = x_scaler.transform(x_test)\n",
    "\n",
    "        model.fit(x_scaled_train, y_train)\n",
    "        y_pred_test = model.predict(x_scaled_test)\n",
    "\n",
    "        mae_err_test, rmse_err_test, mape_err_test = compute_metrics(\n",
    "            y_test, y_pred_test\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"cv_mae\": round(cv_mae, 2),\n",
    "        \"cv_rmse\": round(cv_rmse, 2),\n",
    "        \"cv_mape\": round(cv_mape, 2),\n",
    "        \"test_mae\": round(mae_err_test, 2),\n",
    "        \"test_rmse\": round(rmse_err_test, 2),\n",
    "        \"test_mape\": round(mape_err_test, 2),\n",
    "        \"y_pred_test\": y_pred_test,\n",
    "        \"model\": model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68fac37-a83e-42dd-8849-5386a1cf94a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lstm_params = {\n",
    "    \"seq_len\": 48,\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_layers\": 2,\n",
    "    \"dropout\": 0.0,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 5,\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "results_lstm = eval_model(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    n_splits=3,\n",
    "    model_name=\"LSTM\",\n",
    "    model_params=lstm_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc3a9d-e385-4420-aa3d-7f369708bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294cf4fa-64f0-40a3-b4d3-aaf332020dd4",
   "metadata": {},
   "source": [
    "We see that the first attempt with LSTM does not work well.\n",
    "\n",
    "In fact, it works way worse.\n",
    "\n",
    "It can of course be the case that by finding a better architecture and hyperparameters, we can achieve better results.\n",
    "\n",
    "However, based on the current results and given the data quality, the chances for this are not big.\n",
    "\n",
    "This is because LSTM would work well in less noisy data with clear patters over time.\n",
    "\n",
    "In our case, even though we clean data, for the model it can still be hard to learn the relationship and not overfit to the noise.\n",
    "\n",
    "Another reason can be that the generated features make it harder (not easier) for the model to predict the target.\n",
    "\n",
    "Last, it's better to make log transformation of the target to make it closer to the normalm distribution.\n",
    "\n",
    "However, this gives again some more of headache in production without providing very clear advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880c01fd-47db-49fd-8e27-1fe53c366406",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_filtered_test, df_test['Power'], results_lstm['y_pred_test'], error='mape', error_threshold=8.5, rolling_window=288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d0b2a-3ef1-4eb3-8c6e-86ba79709418",
   "metadata": {},
   "source": [
    "# Model Selection: MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f35e01-efc6-419b-a8ad-1c95c8a1af14",
   "metadata": {},
   "source": [
    "Let's try a simplier neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae1da4-87ea-42bd-bf48-3be4b66afee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, n_features, hidden_sizes=[128, 64], dropout=0.1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_features : int\n",
    "            Number of input features.\n",
    "        hidden_sizes : list[int]\n",
    "            Example: [256, 128, 64] => 3 hidden layers.\n",
    "        dropout : float\n",
    "            Dropout applied AFTER each hidden layer. Set 0 for no dropout.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = n_features\n",
    "\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            in_dim = h\n",
    "\n",
    "        # final output layer\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86818f11-5cd2-4032-983c-8a4adc94190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mlp(\n",
    "    x_train_df: pd.DataFrame,\n",
    "    y_train_ser: pd.Series,\n",
    "    x_test_df: pd.DataFrame,\n",
    "    y_test_ser: pd.Series,\n",
    "    model_params: Dict[str, Any] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, nn.Module]:\n",
    "    \"\"\"\n",
    "    Fit a feed-forward MLP for regression on (X_train, y_train) and\n",
    "    predict on X_test using PyTorch.\n",
    "\n",
    "    Steps\n",
    "    -----\n",
    "    1. Standardize X with StandardScaler\n",
    "    2. Standardize y with StandardScaler\n",
    "    3. Build MLP (hidden layers, dropout) on GPU if available\n",
    "    4. Train with MSE loss + Adam\n",
    "    5. Predict on test data\n",
    "    6. Invert scaling for predictions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train_df : pd.DataFrame\n",
    "        Training features.\n",
    "    y_train_ser : pd.Series\n",
    "        Training target.\n",
    "    x_test_df : pd.DataFrame\n",
    "        Test features.\n",
    "    y_test_ser : pd.Series\n",
    "        Test target.\n",
    "    model_params : dict, default=None\n",
    "        - hidden_sizes, dropout, lr, batch_size, epochs, verbose\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_test_orig : np.ndarray\n",
    "        Test target (original scale).\n",
    "    y_pred_test : np.ndarray\n",
    "        Test predictions (original scale).\n",
    "    model : nn.Module\n",
    "        Trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "\n",
    "    hidden_sizes = model_params.get(\"hidden_sizes\", [128, 64])\n",
    "    dropout = model_params.get(\"dropout\", 0.1)\n",
    "    lr = model_params.get(\"lr\", 1e-3)\n",
    "    batch_size = model_params.get(\"batch_size\", 128)\n",
    "    epochs = model_params.get(\"epochs\", 40)\n",
    "    verbose = model_params.get(\"verbose\", True)\n",
    "\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---- scale X ----\n",
    "    x_scaler = StandardScaler()\n",
    "    X_train = x_scaler.fit_transform(x_train_df.values.astype(np.float32))\n",
    "    X_test = x_scaler.transform(x_test_df.values.astype(np.float32))\n",
    "\n",
    "    # ---- scale y ----\n",
    "    y_train = y_train_ser.values.astype(np.float32).reshape(-1, 1)\n",
    "    y_test = y_test_ser.values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train).flatten()\n",
    "    y_test_scaled = y_scaler.transform(y_test).flatten()\n",
    "\n",
    "    # ---- tensors & loaders ----\n",
    "    X_train_t = torch.from_numpy(X_train)\n",
    "    y_train_t = torch.from_numpy(y_train_scaled).view(-1, 1)\n",
    "\n",
    "    X_test_t = torch.from_numpy(X_test)\n",
    "    y_test_t = torch.from_numpy(y_test_scaled).view(-1, 1)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ---- model ----\n",
    "    n_features = x_train_df.shape[1]\n",
    "    model = MLPRegressor(\n",
    "        n_features=n_features,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # ---- training ----\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        train_loss /= len(train_ds)\n",
    "\n",
    "        # evaluate on test\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                preds = model(X_batch)\n",
    "                loss = criterion(preds, y_batch)\n",
    "                test_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        test_loss /= len(test_ds)\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Epoch {epoch:03d}/{epochs} | \"\n",
    "                f\"train MSE (scaled): {train_loss:.4f} | \"\n",
    "                f\"test MSE (scaled): {test_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "    # ---- predict on test ----\n",
    "    model.eval()\n",
    "    preds_list = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            preds = model(X_batch).cpu().numpy().flatten()\n",
    "            preds_list.append(preds)\n",
    "\n",
    "    y_pred_test_scaled = np.concatenate(preds_list).reshape(-1, 1)\n",
    "    y_pred_test = y_scaler.inverse_transform(y_pred_test_scaled).flatten()\n",
    "\n",
    "    y_test_orig = y_test.flatten()\n",
    "    return y_test_orig, y_pred_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc528ea5-e7b6-4b66-b57e-bf0601dd525d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_params = {\n",
    "    \"hidden_sizes\": [128, 128, 128],\n",
    "    \"dropout\": 0.3, # 0.3,\n",
    "    \"lr\": 1e-3,\n",
    "    \"batch_size\": 128,\n",
    "    \"epochs\": 50, #100,\n",
    "    \"verbose\": True,\n",
    "}\n",
    "\n",
    "results_mlp = eval_model(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    n_splits=3,\n",
    "    model_name=\"MLP\",\n",
    "    model_params=mlp_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168ce0ef-6d3d-489c-a975-4c198c8afff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030b2927-7f2a-427d-83dd-bd14a4e940af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_filtered_test, df_test['Power'], results_mlp['y_pred_test'], error='mape', error_threshold=10, rolling_window=288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210ea2ba-1605-47dd-80a8-b133ff3069d0",
   "metadata": {},
   "source": [
    "We see that MLP gives a bit worse results in terms of the ML metrics but similar in terms of the business metric.\n",
    "\n",
    "For now, we can iterate more over CatBoost hyperparameter tuning, however MLP can also be tested.\n",
    "\n",
    "The problem with MLP though is that it has a much wider hyperparmaeter space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a7cfe-c925-4cf9-9156-c0df5a540863",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89da055-46cf-4715-b4c3-fda948be22b2",
   "metadata": {},
   "source": [
    "### **1. The real issue is noise, not the model**\n",
    "All meaningful improvements came from cleaning and smoothing the signal.\n",
    "This system is noise-limited, not model-limited.\n",
    "\n",
    "### **2. FFT looked great only because it leaked future data**\n",
    "Using the full sequence gave unrealistic results.  \n",
    "When we removed future information, we got worse performance.  \n",
    "\n",
    "### **3. Butterworth and mean filters gave more realistic results**\n",
    "Uses only past values, stable, fast, minimal lag, production-safe.  \n",
    "\n",
    "### **4. Difference-based outlier removal works well**\n",
    "We observed that Z-score filter misses spikes. Difference based filtering removes them better.  \n",
    "\n",
    "### **5. Extra features didn’t help because the model already had the signal**\n",
    "RPM + WindSpeed already capture almost everything.  \n",
    "New domain features were redundant for trees, even though showed good correlation with the target.\n",
    "\n",
    "### **6. Proposed production pipeline for cleaning**\n",
    "1. Diff-based outlier removal  \n",
    "2. Mean filtering (can be Butterworth)  \n",
    "3. Small rolling stats (optional)\n",
    "4. Lagged values for some features  \n",
    "5. Train RF/CatBoost on the cleaned data\n",
    "6. MLP can be tested if wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3b1218-7313-4cf8-999a-a8536d79714e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
