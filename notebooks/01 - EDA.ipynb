{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056f117-8b08-41d7-85e3-7d047101de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import math\n",
    "import shap\n",
    "from catboost import CatBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from ydata_profiling import ProfileReport\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.display import display\n",
    "plotly.io.renderers.default = \"notebook\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd70534-3cc8-4efa-b3bc-1236f8efad4a",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Feature Descriptions and Profiling](#Feature-Descriptions-and-Profiling)\n",
    "2. [Time Series Vizual Analysis](#Time-Series-Vizual-Analysis)\n",
    "3. [Feature Dependency Analysis](#Feature-Dependency-Analysis)\n",
    "4. [Data Cleaning and Grouping](#Data-Cleaning-and-Grouping)\n",
    "5. [Mahalanobis distance study](#Mahalanobis-distance-study)\n",
    "6. [Noise study](#Noise-study)\n",
    "7. [Conclusions](#Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b343bfd0-9ff8-42a4-aca6-28971b21bacb",
   "metadata": {},
   "source": [
    "# Feature Descriptions and Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f3369a-3342-4944-8a11-f14d19f2ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e0c682-4916-4c30-86a9-5c600d281158",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb94de-e724-4ff6-ba6d-bb19f16614e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999e6d7-8dd3-42d0-a4fa-f707cea643d5",
   "metadata": {},
   "source": [
    "The timestamps are taken every 10 mins.\n",
    "\n",
    "We see that the df index is not timestamps. For time series analysis, it's better to have it as a timestamp, so let's make it.\n",
    "\n",
    "This is especially importnt when plotting time series and understanding different phenomena over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3261baf2-8c78-4d3b-aa18-54722165f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cdbb89-6712-4597-9f9f-c2042302c8c1",
   "metadata": {},
   "source": [
    "Before doing any analysis, it's critical to understand what each raw feature means. So, let's describe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206e3ae6-20d2-4fa2-8fb5-078a8e2d0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911009f-0054-4cb7-ae01-86b35958df6d",
   "metadata": {},
   "source": [
    "| **Feature** | **Description** | **Typical Units** | **Category** |\n",
    "|--------------|----------------|-------------------|---------------|\n",
    "| **WindSpeed** | Mean wind speed measured over the recording interval by the anemometer on the nacelle. | m/s | Meteorological | \n",
    "| **WindDirAbs** | Absolute wind direction — measured with respect to geographic North. | ° (degrees) | Meteorological |\n",
    "| **WindDirRel** | Relative wind direction — difference between wind direction and nacelle yaw position (turbine facing direction). | ° | \n",
    "| **Power** | Electrical power output delivered by the generator during the sampling interval (average). | kW | Performance |\n",
    "| **Pitch** | Average blade pitch angle (rotation of blades around their longitudinal axis to control aerodynamic load). | ° | Control |\n",
    "| **GenRPM** | Generator rotational speed (after gearbox). | rpm | Mechanical |\n",
    "| **RotorRPM** | Rotor rotational speed (before gearbox). | rpm | Mechanical |\n",
    "| **EnvirTemp** | Ambient environmental temperature near the nacelle. | °C | Environmental |\n",
    "| **NacelTemp** | Temperature measured inside the nacelle (housing on top of the tower). | °C | Environmental |\n",
    "| **GearOilTemp** | Temperature of gearbox lubricating oil (indicator of mechanical load and thermal stress). | °C | Mechanical / Condition\n",
    "| **GearBearTemp** | Temperature of the main gearbox bearing. | °C | Mechanical / Condition Monitoring |\n",
    "| **GenPh1Temp** | Temperature of generator winding Phase 1. | °C | Electrical / Condition Monitoring |\n",
    "| **GenBearTemp** | Temperature of generator bearing (critical indicator of bearing wear or lubrication issues). | °C | Condition Monitoring |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec120b-05e5-4724-a581-9ac8bb436aff",
   "metadata": {},
   "source": [
    "## Let's make a quick dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cea5fc-48e6-4693-8a17-d04b51c98e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6a8ee-46e5-4e90-9766-8faadffddc1b",
   "metadata": {},
   "source": [
    "**We see that:**\n",
    "- There are weird negative values for min_values of some features.\n",
    "- Some parameters have the std / mean value high, so we have high variability within the data.\n",
    "\n",
    "However, basic description does not give much info.\n",
    "\n",
    "Let's use the Data Profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abab37-2db2-4019-946e-ca2c761d332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df)\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601ec168-aca0-4453-9fab-4d95476729c8",
   "metadata": {},
   "source": [
    "**We see that:**\n",
    "\n",
    "**Observations from values and distributions**\n",
    "- There are outliers including negative values for many parameters, definitely needs to be cleaned out.\n",
    "- A lot of the values of are zero, which means that a lot of time the turbine does don't work. It also means that these regime needs to be most likely cleaned out when analyzing the relatinsionships, correlations, etc. It can also be the turbine downtime.\n",
    "- Power - a very important parameter, is highly skewed If we gonna use it as a model target, it can be a problem.\n",
    "- GenRPM is relatuvely even distributed but also has some certain peaks which seem to be the main operating regimes.\n",
    "- The temperature seem to be moderate and does not have much of negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02996ba-78a8-4e4a-8e2f-3e1c6343f508",
   "metadata": {},
   "source": [
    "# Time Series Vizual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcaf55f-240d-4bdc-bbd6-5d5b572c7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(df, columns, step=10, rolling_window=None):\n",
    "    \"\"\"\n",
    "    Clean and fast Plotly plot:\n",
    "    - Subplots stacked vertically\n",
    "    - Optional rolling median trend (black)\n",
    "    - Only shows every `step`th tick\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Time series dataframe with a DateTimeIndex.\n",
    "    columns : list of str\n",
    "        List of column names to plot.\n",
    "    step : int\n",
    "        Subsampling step for faster plotting.\n",
    "    rolling_window : int or None\n",
    "        Window size for rolling median.\n",
    "        If None → no rolling median plotted.\n",
    "    \"\"\"\n",
    "\n",
    "    # Subsample for speed\n",
    "    df_small = df.iloc[::step]\n",
    "\n",
    "    # Create subplot layout\n",
    "    fig = make_subplots(\n",
    "        rows=len(columns),\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.01,\n",
    "        subplot_titles=columns\n",
    "    )\n",
    "\n",
    "    # Precompute rolling only if requested\n",
    "    if rolling_window is not None:\n",
    "        df_roll = (\n",
    "            df[columns]\n",
    "            .rolling(rolling_window, min_periods=1)\n",
    "            .median()\n",
    "            .iloc[::step]\n",
    "        )\n",
    "\n",
    "    for i, col in enumerate(columns, start=1):\n",
    "\n",
    "        # Original series\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_small.index,\n",
    "                y=df_small[col],\n",
    "                mode=\"lines\",\n",
    "                name=col,\n",
    "                line=dict(width=1)\n",
    "            ),\n",
    "            row=i, col=1\n",
    "        )\n",
    "\n",
    "        # Rolling trend only if rolling_window is given\n",
    "        if rolling_window is not None:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=df_roll.index,\n",
    "                    y=df_roll[col],\n",
    "                    mode=\"lines\",\n",
    "                    name=f\"{col} (rolling median)\",\n",
    "                    line=dict(width=1, color=\"black\"),\n",
    "                ),\n",
    "                row=i, col=1\n",
    "            )\n",
    "\n",
    "    fig.update_xaxes(tickmode=\"auto\")\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=250 * len(columns),\n",
    "        showlegend=False,\n",
    "        title_text=\"Time Series Overview\",\n",
    "        margin=dict(l=50, r=30, t=50, b=50)\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c14790-8919-4f2b-948a-2b7b331fa577",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f43c6-b8b4-4652-b59f-6397a04d806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = [\n",
    "    'Power', 'WindSpeed', 'GenRPM', 'RotorRPM', \n",
    "    'WindDirAbs', 'WindDirRel', 'Pitch',\n",
    "    'EnvirTemp', 'NacelTemp', 'GearOilTemp',\n",
    "    'GearBearTemp', 'GenPh1Temp', 'GenBearTemp'\n",
    "]\n",
    "plot_time_series(df, cols_to_plot, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d722f6-9067-41c2-a3ba-dd922bfe7441",
   "metadata": {},
   "source": [
    "- We see that most of the parameters have strong outlying values. This can be a problem when fitting the model.\n",
    "- We see that there is a data chunk where all the values are zero. This corresponds to the turbine shoutdown.\n",
    "- The operation of the turbine is unsteady which is expeted because the wind has turbulent and intermittent nature.\n",
    "- Many signals have quite A LOT of noise, at least visually. It might be a good idea to analyze it and denoise if possible.\n",
    "- Some signals have seasonality, especially tempearture-reated which makes sense.\n",
    "- We don't observe any strong weird anomaly behavior any time before the shoutdown. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4cb84f-441a-4f1e-8955-7caf6ea659ed",
   "metadata": {},
   "source": [
    "Let's also plot the time series with median rolling that helps us see the trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45be078-27cd-4222-83d4-670be63d5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = [\n",
    "    'Power', 'WindSpeed', 'GenRPM', 'RotorRPM', \n",
    "    'WindDirAbs', 'WindDirRel', 'Pitch',\n",
    "    'EnvirTemp', 'NacelTemp', 'GearOilTemp',\n",
    "    'GearBearTemp', 'GenPh1Temp', 'GenBearTemp'\n",
    "]\n",
    "plot_time_series(df, cols_to_plot, step=1, rolling_window=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1be7cd-6877-4c04-8a9b-7a1eaaa1d399",
   "metadata": {},
   "source": [
    "- Here, if zooming in, it's possible to see that there is a lot of noise in parameters that can be smoothed out.\n",
    "- Clearly,  in addition to noise, even the median parameter values have high variability.\n",
    "- We can also see that we can use the median filter as the outlier removal.\n",
    "- The smoothed values show better relationships betwen Power, Wind Speed, GenRMP and RotorRPM parameters, especially when there are big ups and downs. These parameters might have good feature <--> target relationships.\n",
    "- The same we can see for GenPh1Temp and GenBearTemp, NacelTemp and EnvirTemp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb9fc7-d2df-44b0-bdfa-43001aa44f9c",
   "metadata": {},
   "source": [
    "# Feature Dependency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2fe15-97d0-4e8a-9dab-e3eb24bb4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Plot heatmap with annotations\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0, square=True, annot_kws={\"size\": 12})\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c82a51-9b8c-4b26-b0f7-e7b82258d548",
   "metadata": {},
   "source": [
    "- We see that some parameters have VERY good correlations like Power <--> GenRPM corr=0.88\n",
    "- Some parameters are barely correlated with anything, e.g. Pitch.\n",
    "- There are many moderate to string correlation values.\n",
    "- What is weird is that on the time we have NOT seen such string relationships, especially taking into account the noise.\n",
    "\n",
    "Let's take Power as an example and sort correlations from highest to lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f24d1-c29b-4726-8885-fc519fd1baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of all columns with the target\n",
    "corr = df.corr()['Power'].drop('Power')\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "correlations_sorted = corr.reindex(corr.abs().sort_values(ascending=False).index)\n",
    "correlations_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5bf46-3bfc-482e-a347-a54dd4458050",
   "metadata": {},
   "source": [
    "From the Time Series plots, we haven't seen such a strong correlation. Let's check the scatter-like plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e816e-5c79-4c32-8e70-1713931fbd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_relationships(x, y, x_label='X', y_label='Y'):\n",
    "    \"\"\"\n",
    "    Plots x-y relationships in different formats (Regression, KDE and HexBin plots)\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Regression Plot (via seaborn)\n",
    "    sns.regplot(x=x, y=y, ax=axs[0], scatter_kws={'s': 20}, line_kws={'color': 'red'})\n",
    "    axs[0].set_title('Regression Plot')\n",
    "    axs[0].set_xlabel(x_label)\n",
    "    axs[0].set_ylabel(y_label)\n",
    "\n",
    "    # 2. KDE Plot (Seaborn joint density)\n",
    "    sns.kdeplot(x=x, y=y, fill=True, cmap=\"mako\", ax=axs[1], thresh=0.01)\n",
    "    axs[1].set_title('KDE Plot')\n",
    "    axs[1].set_xlabel(x_label)\n",
    "    axs[1].set_ylabel(y_label)\n",
    "\n",
    "    # 3. Hexbin Plot (via Matplotlib)\n",
    "    axs[2].hexbin(x, y, gridsize=30, cmap='viridis', mincnt=1)\n",
    "    axs[2].set_title('Hexbin Plot')\n",
    "    axs[2].set_xlabel(x_label)\n",
    "    axs[2].set_ylabel(y_label)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe91ac-5b3f-4881-95ae-ccca556b5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 'GenRPM'\n",
    "v2 = 'Power'\n",
    "n = 10\n",
    "df_local = df.copy() # df[(df[v1] > 800)]\n",
    "plot_relationships(df_local[v1][::n], df_local[v2][::n], v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e2527-b672-4911-bd1d-703f3f6a045e",
   "metadata": {},
   "source": [
    "These variables do NOT look like well-correlated.\n",
    "\n",
    "The correlation value is strongly effected by the outliers. But in this case making it high!\n",
    "\n",
    "For the outliers, we can see that there are some clouds of points which are separated from the main relationship.\n",
    "\n",
    "There is a big cloud of points around zero which drives big correlation.\n",
    "\n",
    "Let's check for some more variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04494509-b635-45d0-b18c-f5cab753e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 'WindSpeed'\n",
    "v2 = 'Power'\n",
    "n = 10\n",
    "df_local = df.copy()\n",
    "plot_relationships(df_local[v1][::n], df_local[v2][::n], v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733453d9-5e19-4d94-be01-bd52f20f8bd6",
   "metadata": {},
   "source": [
    "We see the same picture here. We also see that there are MANY points that looks like one point in zero. \n",
    "\n",
    "This is NOT possible to see in the scatter plot, but we can see it in the KDE and Hexbin plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad5d96-42df-42cb-9fde-8c9ba6b2396d",
   "metadata": {},
   "source": [
    "We clearly see that the correlation values are strongly influecned by the outliers, especially zeros.\n",
    "\n",
    "Let's first remove zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727d704-185c-49f0-92b4-5e14e0c860df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_zero = df[df['Power'] > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf7fcd5-c827-4c99-9e70-07b488abe601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation of all columns with the target\n",
    "corr = df_no_zero.corr()['Power'].drop('Power')\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "correlations_sorted = corr.reindex(corr.abs().sort_values(ascending=False).index)\n",
    "correlations_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30378c72-daa9-413f-ad0d-d534bebbfcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data correlations\n",
    "# GenRPM          0.879374\n",
    "# GenPh1Temp      0.828008\n",
    "# GearOilTemp     0.743265\n",
    "# WindSpeed       0.705276\n",
    "# RotorRPM        0.703314\n",
    "# GearBearTemp    0.703189\n",
    "# GenBearTemp     0.677292\n",
    "# WindDirAbs      0.419142\n",
    "# NacelTemp       0.313068\n",
    "# EnvirTemp       0.249844\n",
    "# Pitch           0.104456\n",
    "# WindDirRel     -0.015779"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d6a00-6280-41cb-abae-4ab3ac21fd2e",
   "metadata": {},
   "source": [
    "Now we see much smaller correlations.\n",
    "\n",
    "Let's check how it looks in scatters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e543da11-b027-4058-b7c3-201d75088cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 'GenRPM'\n",
    "v2 = 'Power'\n",
    "n = 10\n",
    "df_local = df_no_zero.copy()\n",
    "plot_relationships(df_local[v1][::n], df_local[v2][::n], v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570dae5-aeaa-423f-9eb6-2bc0bb9683a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 'RotorRPM'\n",
    "v2 = 'Power'\n",
    "n = 10\n",
    "df_local = df_no_zero.copy()\n",
    "plot_relationships(df_local[v1][::n], df_local[v2][::n], v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254debc8-2672-4e56-b89d-6d02ef1a7fe7",
   "metadata": {},
   "source": [
    "We still see the influence by the outlers. \n",
    "    \n",
    "And now it's hard to say iof the outliers increase or decrease correlations.\n",
    "\n",
    "But what is more important, it's hard to see the relationships clearly and trully understand the data.\n",
    "\n",
    "Let's check the distrobutions closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265060d-9552-47e2-af9f-a43f3e8e6bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select numeric columns only\n",
    "numeric_cols = df_no_zero.select_dtypes(include=np.number).columns\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(numeric_cols) / n_cols))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.histplot(df_no_zero[col].dropna(), bins=50, kde=True)\n",
    "    plt.title(col, fontsize=10)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15729d4d-dd9a-4ad6-91d6-dcef7f10f73d",
   "metadata": {},
   "source": [
    "We can see long tails in most of the features, but the number of data points is not that big.\n",
    "\n",
    "Let's also check if we can see in the multidimentional space reduced to 2 dmentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c2660-058d-4e8b-904a-fe8bd017d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df) # df_no_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964faba-3daf-42f1-b249-a8c498615eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=len(df.columns))\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create DataFrame of first 2 components\n",
    "pca_df = pd.DataFrame(data=X_pca[:, :2], columns=[\"PC1\", \"PC2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f449c-74cb-4d96-aa4f-b4a6a2d9465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: PCA Scatter Plot (2D)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=\"PC1\", y=\"PC2\", data=pca_df[::1])\n",
    "plt.title(\"PCA: First 2 Principal Components\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce509f-d8d5-49f0-8858-99526584aac1",
   "metadata": {},
   "source": [
    "We can see that there are outlying values.\n",
    "\n",
    "If we check this for **df_no_zero**, we will not see them.\n",
    "\n",
    "So, this means that these PCA outlying values are zeros.\n",
    "\n",
    "Also, let's check how representative the first two principal components are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420722db-2576-448a-a05b-08cf60be8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Explained Variance (Scree Plot)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "cumulative_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a04825-8ec0-4a9c-bcb6-0d65f2f3a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Number of Principal Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Cumulative Explained Variance\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0369478a-8c5c-4576-8571-32d3f4e5104a",
   "metadata": {},
   "source": [
    "Also, we see that a lot of information missing, so PCA plots might not be very representative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1f083b-2a09-4833-8086-1f9a181e1d35",
   "metadata": {},
   "source": [
    "# Data Cleaning and Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4150461c-979b-43e3-a117-03d9c8d38a6b",
   "metadata": {},
   "source": [
    "### Z-Score Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1f81e4-c13d-4666-993d-8510d1034490",
   "metadata": {},
   "source": [
    "Let's try to remove the outliers with a simple Z-score filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298391a-ba65-4bf9-a8a6-3fac03c88f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_zscore(df, threshold=3, nan_treatment='ffill'):\n",
    "    \"\"\"\n",
    "    Replace outliers (based on z-score) with NaN for each numeric column\n",
    "    and report how many values were replaced.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame (numeric or mixed).\n",
    "    threshold : float, optional\n",
    "        Z-score threshold. Default = 3.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_masked : pd.DataFrame\n",
    "        DataFrame with outlier values replaced by NaN.\n",
    "    \"\"\"\n",
    "    df_masked = df.copy()\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "    total_replaced = 0\n",
    "    replaced_per_column = {}\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std(ddof=0)\n",
    "        z_score = np.abs((df[col] - mean) / std)\n",
    "\n",
    "        outlier_mask = z_score > threshold\n",
    "        n_replaced = outlier_mask.sum()\n",
    "\n",
    "        df_masked.loc[outlier_mask, col] = np.nan\n",
    "\n",
    "        replaced_per_column[col] = n_replaced\n",
    "        total_replaced += n_replaced\n",
    "\n",
    "    if nan_treatment == 'ffill':\n",
    "        df_masked = df_masked.ffill()\n",
    "    elif nan_treatment =='drop':\n",
    "        df_masked = df_masked.dropna()\n",
    "    else:\n",
    "        raise ValueError(f'{nan_treatment} nan_treatment is not recognized')\n",
    "\n",
    "    print(f\"Replaced {total_replaced} values total (|z| > {threshold}).\")\n",
    "    print(\"Per column replacements:\")\n",
    "    for col, n in replaced_per_column.items():\n",
    "        print(f\"  {col}: {n}\")\n",
    "\n",
    "    return df_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf1e62-5b0b-4308-bec1-12745f68b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = remove_outliers_zscore(df_no_zero, nan_treatment='ffill', threshold=3) # Check with 2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e1000-760f-4170-94cd-f8cdfee0b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns only\n",
    "numeric_cols = df_clean.select_dtypes(include=np.number).columns\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(numeric_cols) / n_cols))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.histplot(df_clean[col].dropna(), bins=25, kde=True)\n",
    "    plt.title(col, fontsize=10)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf9b88-6afe-480f-85ca-86b10cd88c87",
   "metadata": {},
   "source": [
    "We see that we cut the outliers quite well. Now, let's check the X-Y plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c62dac-247b-402a-b496-ac9cb2be546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 'GenRPM'\n",
    "v2 = 'Power'\n",
    "n = 10\n",
    "df_local = df_clean.copy()\n",
    "plot_relationships(df_local[v1][::n], df_local[v2][::n], v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d42fce-6d34-47ab-89b0-dcb8eeae1346",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 'WindSpeed'\n",
    "v2 = 'Power'\n",
    "n = 10\n",
    "df_local = df_clean.copy()\n",
    "plot_relationships(df_local[v1][::n], df_local[v2][::n], v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf7760-25d8-43f9-bc6f-2bd7a04b04f6",
   "metadata": {},
   "source": [
    "Now, we can better see the relationships. \n",
    "\n",
    "However, due to the noisy nature of the signals, it's still hard.\n",
    "\n",
    "Let's try one trick - let's plot grouped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3173ce10-40d8-4172-bf22-bfe03250727d",
   "metadata": {},
   "source": [
    "## Grouped data plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd94468-0bd4-409c-9351-f91574d60f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gr = df_clean.resample('1D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93820b3-89b7-432a-87a9-ac1005d1a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 'GenRPM'\n",
    "v2 = 'Power'\n",
    "n = 1\n",
    "df_local = df_gr.copy()\n",
    "plot_relationships(df_local[v1][::n], df_local[v2][::n], v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ca287d-ea1a-45c0-a7be-66d79290696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 'WindSpeed'\n",
    "v2 = 'Power'\n",
    "n = 1\n",
    "df_local = df_gr.copy()\n",
    "plot_relationships(df_local[v1][::n], df_local[v2][::n], v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8233f24-92f1-4287-9ca7-129dd343f3ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v2 = \"Power\"         # fixed y-variable\n",
    "df_local = df_gr.copy()\n",
    "n = 1                # subsampling step\n",
    "\n",
    "for col in df_local.columns:\n",
    "    if col == v2:\n",
    "        continue     # skip Power itself\n",
    "    print(f\"Plotting: {col} vs {v2}\")\n",
    "    plot_relationships(\n",
    "        df_local[col][::n],\n",
    "        df_local[v2][::n],\n",
    "        col,\n",
    "        v2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbdfbe-ce6a-4fc4-98b7-6448b937c88a",
   "metadata": {},
   "source": [
    "### PCA on Grouped Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26447cf7-f509-4c61-b2a3-3bd90459e809",
   "metadata": {},
   "source": [
    "Now we can see that ON AVERAGE for some variables like WindSpeed or GenRPM vs Power the relationships are quite linear.\n",
    "\n",
    "We can take this into account because maybe it can be useful when creating the model and identifying the time horizon for the model prediction.\n",
    "\n",
    "Soome varibleas are close to linear dependency, but the variable fo the relationship is very high, e.g. EnvirTemp vs Power.\n",
    "\n",
    "Pitch has a very strange relationship with Power, however, in some data ranges it can be a useful predictor.\n",
    "\n",
    "Let's see how it looks as a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720905e9-9a03-434f-a11a-13b0625a0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_plot = [\n",
    "    'Power', 'WindSpeed', 'GenRPM', 'RotorRPM', \n",
    "    'WindDirAbs', 'WindDirRel', 'Pitch',\n",
    "    'EnvirTemp', 'NacelTemp', 'GearOilTemp',\n",
    "    'GearBearTemp', 'GenPh1Temp', 'GenBearTemp'\n",
    "]\n",
    "\n",
    "plot_time_series(df_gr, cols_to_plot, step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc5aa2-ba93-498d-bb8c-0e2045ca7481",
   "metadata": {},
   "source": [
    "We can see and study the time series more clearly after grouping.\n",
    "\n",
    "We can see how big ups and downs for high correlation features match ups and downs of Power.\n",
    "\n",
    "However, we can't really see any specific behavior before the the downtime.\n",
    "\n",
    "We need to do something else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84256020-750f-4097-91d2-d5c438dd5c1e",
   "metadata": {},
   "source": [
    "Let's apply PCA to the grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799eca72-afc7-47c0-89ff-3aa37725e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = df_gr.copy().dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_pca)\n",
    "\n",
    "pca = PCA(n_components=len(df_pca.columns))\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "pca_df = pd.DataFrame(\n",
    "    data=X_pca[:, :2],\n",
    "    index=df_pca.index,\n",
    "    columns=[\"PC1\", \"PC2\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5620e6-2735-4e7a-acb1-7fda2f31e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: PCA Scatter Plot (2D)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=\"PC1\", y=\"PC2\", data=pca_df[::1])\n",
    "plt.title(\"PCA: First 2 Principal Components\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc2dea3-0af5-4afd-ad2a-5d8d414f6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Explained Variance (Scree Plot)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance.cumsum()\n",
    "cumulative_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff8c52-cc6d-4564-9fcc-e5303b6f190d",
   "metadata": {},
   "source": [
    "- We see that even after grouping, first 2 PC still do not explain eniough of variance.\n",
    "- We can see some \"cluster\" of points for PC_1 > 5, but it's not very well separated.\n",
    "- However, let's plot it on time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fc6d5-c8fc-4b49-a9d5-baf83b1a8d68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pc1_thr = 4\n",
    "pc1 = pca_df[\"PC1\"]\n",
    "\n",
    "mask_red = pc1 > pc1_thr   # positions where PC1 is \"high\"\n",
    "\n",
    "cols_to_plot = [\n",
    "    'Power', 'WindSpeed', 'GenRPM', 'RotorRPM', \n",
    "    'WindDirAbs', 'WindDirRel', 'Pitch',\n",
    "    'EnvirTemp', 'NacelTemp', 'GearOilTemp',\n",
    "    'GearBearTemp', 'GenPh1Temp', 'GenBearTemp'\n",
    "]\n",
    "\n",
    "for col in cols_to_plot:\n",
    "    plt.figure(figsize=(10, 3))\n",
    "\n",
    "    # base time series (grey line)\n",
    "    plt.plot(\n",
    "        df_pca.index,\n",
    "        df_pca[col],\n",
    "        color=\"grey\",\n",
    "        linewidth=1,\n",
    "        label=col\n",
    "    )\n",
    "\n",
    "    # mark PC1 > threshold in red\n",
    "    plt.scatter(\n",
    "        df_pca.index[mask_red],\n",
    "        df_pca.loc[mask_red, col],\n",
    "        color=\"red\",\n",
    "        s=10,\n",
    "        label=f\"PC1 > {pc1_thr}\"\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{col} – points where PC1 > {pc1_thr} in red\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(col)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd367a4-9563-418a-a123-04d2874f56bd",
   "metadata": {},
   "source": [
    "- We see that PCA \"outliers\" are just the points with highest values of Power, WindSpeed, GenRPM, etc.\n",
    "- We see that these values happened far before the downtime and quite quickly after downtime.\n",
    "- So, these points can hardly be considered as anomalies in terms of downtime detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821f8b4e-bff2-4922-ba14-5d60024f6644",
   "metadata": {},
   "source": [
    "# Mahalanobis distance study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c467c41-4879-43c8-b3e0-4059cccabf69",
   "metadata": {},
   "source": [
    "Mahalanobis Distance measures how far a point is from the center of a multivariate distribution while accounting for correlations between features.\n",
    "\n",
    "Unlike Euclidean distance, it adjusts for:\n",
    "- different variances in each feature\n",
    "- correlations between features\n",
    "- scale differences across dimensions\n",
    "\n",
    "\n",
    "Mahalanobis Distance is defined as:\n",
    "\n",
    "$$\n",
    "D_M(x) = \\sqrt{(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x$ is the data point  \n",
    "- $\\mu$ is the mean vector  \n",
    "- $\\Sigma^{-1}$ is the inverse covariance matrix  \n",
    "\n",
    "\n",
    "**Mahalanobis Distance answers:**\n",
    "\n",
    "\"How many multivariate standard deviations away is this point from the center?\"\n",
    "\n",
    "It is commonly used for:\n",
    "- multivariate anomaly detection\n",
    "- identifying outliers in high-dimensional data\n",
    "- measuring how unusual a point is relative to a distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac013c2-be7b-452a-88da-f050021dec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Split\n",
    "split_point = 30000\n",
    "df_mah_train = df_clean.iloc[:split_point].dropna().copy()\n",
    "df_mah_test  = df_clean.iloc[split_point:].dropna().copy()\n",
    "\n",
    "# 2) Arrays\n",
    "x_train_raw = df_mah_train.values\n",
    "x_test_raw  = df_mah_test.values\n",
    "\n",
    "# 3) Scale\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train_raw)\n",
    "x_test  = scaler.transform(x_test_raw)\n",
    "\n",
    "# 4) Covariance\n",
    "mu = np.mean(x_train, axis=0)\n",
    "cov = np.cov(x_train, rowvar=False)\n",
    "cov_inv = np.linalg.pinv(cov)\n",
    "\n",
    "# 5) Mahalanobis\n",
    "diff = x_test - mu\n",
    "precision_proj = diff @ cov_inv\n",
    "mah_components = precision_proj * diff\n",
    "\n",
    "d2 = np.sum(mah_components, axis=1)\n",
    "df_mah_test[\"mahalanobis_distance\"] = np.sqrt(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f88003-b0f0-4ad5-be70-d9b7bb08c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_mah_test.index,\n",
    "    y=df_mah_test[\"mahalanobis_distance\"],\n",
    "    mode=\"lines\",\n",
    "    name=\"Mahalanobis distance\",\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_mah_test.index,\n",
    "    y=df_mah_test[\"mahalanobis_distance\"].rolling(50).median(),\n",
    "    mode=\"lines\",\n",
    "    name=\"Rolling median (50)\",\n",
    "    line=dict(color=\"black\", width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Mahalanobis distance (test segment)\",\n",
    "    xaxis_title=\"Time\",\n",
    "    yaxis_title=\"Distance\",\n",
    "    template=\"plotly_white\",\n",
    "    legend=dict(\n",
    "        x=0.01,\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        yanchor=\"top\",\n",
    "        bgcolor=\"rgba(255,255,255,0)\",\n",
    "        bordercolor=\"rgba(0,0,0,0)\",\n",
    "        font=dict(size=11),\n",
    "        orientation=\"h\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b688e03-368b-4256-84a3-db2b78a8e0e0",
   "metadata": {},
   "source": [
    "- Finally, we can see some anomaly signs around the downtime.\n",
    "- Around the end of may, the distance starts rising quite a lot. This indicates, that these data points are gettung away from the normal range INCLUDING the covariance between the features.\n",
    "- So, the main conclusion is that THERE ARE some anomalies around the downtime.\n",
    "- It's tempting to use Mahalanobis distance to monitor anomalie in production, however, this method can be numerically unstable.\n",
    "- Also, we do not really understand which parameters influenced the distance (and the Turbine) to behave abnormally.\n",
    "\n",
    "Let's try to explain what contribute the most to the distance rise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80347f71-e187-4f0d-9093-f018fc09d005",
   "metadata": {},
   "source": [
    "### Computing distances per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eba73b-dfca-494e-8a36-186da93a84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that above the Mahalanobis distance threshold, the data are abnormal\n",
    "threshold = 4.5\n",
    "anom_mask = df_mah_test[\"mahalanobis_distance\"] >= threshold\n",
    "\n",
    "# Select anomaly points\n",
    "mah_components_anom = mah_components[anom_mask]     # (n_anom, n_features)\n",
    "\n",
    "feature_cols = df_mah_test.columns.drop(\"mahalanobis_distance\")\n",
    "\n",
    "df_contrib_anom = pd.DataFrame(\n",
    "    mah_components_anom,\n",
    "    columns=feature_cols,\n",
    "    index=df_mah_test.loc[anom_mask].index\n",
    ")\n",
    "\n",
    "df_contrib_anom.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1394d8f-e6ec-4bc6-af7b-0b6e1917f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importance across anomalies (mean absolute contribution)\n",
    "feature_importance = (\n",
    "    df_contrib_anom.abs().mean().sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Feature contribution ranking:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367ea79f-954b-4eca-9137-72a9e6e99e3d",
   "metadata": {},
   "source": [
    "- Nice! We clearly see 3 dominating features that contributed the most to the anomaly region.\n",
    "- We have to notice, however, that these features are correlated, so this can be the reason why all of them dominate at the same time.\n",
    "- Before drawing conclusions, let's try some more tricks to explain the Mahalanobis distance rise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e49ac-be73-4c9c-84f1-6bd8eb25367e",
   "metadata": {},
   "source": [
    "### Explanation using ML Model Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ba80bc-f163-47fb-95f3-3f701c374e3a",
   "metadata": {},
   "source": [
    "Let's fit a CatBoost model with default parameters where:\n",
    "\n",
    "- **Features** - the distances per feature that we computed above.\n",
    "- **Target** - Mahalanobis distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd556c3d-82b3-4437-9d62-762b5c33b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use true per-feature Mahalanobis contributions as model features\n",
    "df_contrib_all = pd.DataFrame(\n",
    "    mah_components,\n",
    "    index=df_mah_test.index,\n",
    "    columns=feature_cols\n",
    ")\n",
    "\n",
    "X = df_contrib_all\n",
    "y = df_mah_test[\"mahalanobis_distance\"]\n",
    "\n",
    "# Select anomaly subset\n",
    "threshold = 4.5\n",
    "anom_mask = y >= threshold\n",
    "\n",
    "X_anom = X[anom_mask]\n",
    "y_anom = y[anom_mask]\n",
    "\n",
    "# Train CatBoost on all test data\n",
    "# (Model learns mapping: per-feature-contrib → anomaly score)\n",
    "\n",
    "model = CatBoostRegressor(\n",
    "    depth=6,\n",
    "    learning_rate=0.05,\n",
    "    iterations=600,\n",
    "    loss_function=\"RMSE\",\n",
    "    verbose=False,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247b48e-c73b-497c-b0e9-71e7dda08513",
   "metadata": {},
   "source": [
    "First, let's check defaul feature importance of CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae793e-4146-4d06-808b-b897c25b2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model.get_feature_importance()\n",
    "df_catboost_importance = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"importance\": feature_importance\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(df_catboost_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683357c-706c-46e2-903f-25925098f5e7",
   "metadata": {},
   "source": [
    "Interestingly, we see the same TOP features, however, now Power is the TOP 1.\n",
    "\n",
    "Now, let's try to use SHAP values.\n",
    "\n",
    "We will check SHAP values for:\n",
    "- Anomaly data (Mahalanobis distace > 4.5)\n",
    "- The entire test dataset.\n",
    "\n",
    "**First, let's check anomalies only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98467d1-378b-4c27-a2a9-855b07156803",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_anom)   # explain anomalies only\n",
    "\n",
    "# Global ranking\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "shap_ranking = pd.Series(shap_importance, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nSHAP Feature Importance (Anomaly Region):\")\n",
    "print(shap_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483818c4-1697-41fb-99f6-052ba29c0976",
   "metadata": {},
   "source": [
    "**Now, let's check the entire data used to train CatBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1138ef8-d54c-4acf-a38c-0a60ed4ecbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Compute SHAP for entire dataset\n",
    "shap_values_all = explainer.shap_values(X)\n",
    "\n",
    "# Global SHAP importance (full dataset)\n",
    "shap_importance_all = np.abs(shap_values_all).mean(axis=0)\n",
    "\n",
    "shap_ranking_all = (\n",
    "    pd.Series(shap_importance_all, index=X.columns)\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Global SHAP Feature Importance (Full Dataset) ===\\n\")\n",
    "print(shap_ranking_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac597f-4a41-4ad8-86ca-cdf2225fc336",
   "metadata": {},
   "source": [
    "We see similar importances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd720cc-31a3-4ba5-ae3e-1a9bc9b36371",
   "metadata": {},
   "source": [
    "## Mahalanobis study conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416149a-03f7-44d1-b355-26d8b03d3438",
   "metadata": {},
   "source": [
    "- We have seen that using 4 different approaches, we identified that Power, GenRPM and GenPh1TEmp controbute the most to the anomalious region.\n",
    "- When using the SHAP values explainer of a CatBoost model, Power feature dominates.\n",
    "- From the technological perspective, Power is the resulting variable from the Wind Turbine work.\n",
    "- **From both data-driven approach and technologcala point of view, it's proposed to monitor Power as the main signal to identify the anomaly using regression models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e7a9f-dd9b-407f-937b-aeb9868dbf8d",
   "metadata": {},
   "source": [
    "# Noise study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffa0481-8fde-4cf6-9e71-dfe2b7648908",
   "metadata": {},
   "source": [
    "During the entire EDA, we observed that the data seems to have quite a lot of noise, so this section studies the noise in more detail using Fast Fourier Transformations.\n",
    "\n",
    "FFT (Fast Fourier Transform) is an algorithm that converts a time-series signal into its frequency components.\n",
    "Instead of looking at how the signal changes over time, FFT shows which frequencies are present and how strong they are.\n",
    "\n",
    "This is useful because:\n",
    "\n",
    "Noise often appears as high-frequency components\n",
    "\n",
    "Trends and slow variations appear as low-frequency components\n",
    "\n",
    "Periodic behavior becomes clearly visible in the frequency domain\n",
    "\n",
    "By analyzing the frequency spectrum, we can better understand the underlying structure of the signal and identify which parts of the variation come from meaningful patterns versus random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202b47c-edfe-43f4-a858-5108212b5878",
   "metadata": {},
   "source": [
    "To quicly get an idea, let's create a sigmal of 2 sin waves of different frequencies + noise.\n",
    "\n",
    "We need to define a compute of parameters.\n",
    "\n",
    "**dt = sampling interval** - The time between two samples, expressed in any unit you choose (seconds, minutes, hours)\n",
    "\n",
    "If we want to express units is hours and we have data points every 10 mins, we get:\n",
    "\n",
    "dt = 10 mins / 60 mins = 0.1666 h\n",
    "\n",
    "**f = Frequency** - Number of cycles per unit time (e.g., per minuite or per hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c692b73-ead0-4da7-a47f-43047b53a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 10/60          # 10 minutes in hours\n",
    "t = np.arange(0, 480, dt)   # 48 hours\n",
    "\n",
    "freq1 = 0.5     # slow wave\n",
    "freq2 = 1.5     # fast wave\n",
    "\n",
    "signal = (\n",
    "    3 * np.sin(2*np.pi*freq1*t) +\n",
    "    1 * np.sin(2*np.pi*freq2*t)\n",
    ")\n",
    "\n",
    "noise = 1.5 * np.random.randn(len(t))\n",
    "\n",
    "df_demo = pd.DataFrame({\n",
    "    \"demo_signal\": signal + noise\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef0c26-71be-4f78-b004-739a4797fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_demo[\"demo_signal\"][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53180dcc-c445-4d45-86a5-06f7a0c6d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fft(df, col, dt=10/60):\n",
    "    \"\"\"\n",
    "    Pure FFT-based power spectrum.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df  : DataFrame with the signal\n",
    "    col : column name (string)\n",
    "    dt  : sampling interval for the selected unit (e.g. 10/60 for 10 min in hours)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    freqs : array, frequencies in 1/dt units (e.g. cycles/hour)\n",
    "    power : array, power at each frequency (|FFT|^2 / N)\n",
    "    \"\"\"\n",
    "    x = df[col].astype(float).interpolate().bfill().ffill().values\n",
    "    x = x - np.mean(x)          # remove DC offset (zero-frequency component - mean value)\n",
    "\n",
    "    N = len(x)\n",
    "    fft_vals = np.fft.rfft(x)   # one-sided FFT\n",
    "    freqs = np.fft.rfftfreq(N, d=dt)\n",
    "\n",
    "    power = (np.abs(fft_vals) ** 2) / N   # simple power spectrum\n",
    "\n",
    "    return freqs, power, fft_vals, np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a9ca1-8383-4ab3-851d-855e3730635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs, power, fft_vals, x_mean = compute_fft(df_demo, \"demo_signal\", dt=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984414ed-35e1-4bbf-ac6c-060beaa018d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=freqs,\n",
    "    y=power,\n",
    "    mode=\"lines\",\n",
    "    line=dict(width=2),\n",
    "    name=\"FFT Power\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"FFT Power Spectrum\",\n",
    "    xaxis_title=\"Frequency (cycles per hour)\",\n",
    "    yaxis_title=\"Power\",\n",
    "    height=500,\n",
    "    width=900\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c07c5a-78cd-43c4-a3b9-9fa8f4ea130a",
   "metadata": {},
   "source": [
    "Here, we see that after transforming the signal to the frequency space, we can clearly identify that there are 2 main frequencies in the signal, 0.5 and 1.5.\n",
    "\n",
    "If zooming in, we can see that other frequencies are covered with small noise fluctuations.\n",
    "\n",
    "What is the most beautiful, we can take the inverse transformation and reconstruct the original signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76619a-838c-42a1-bed4-ea298b12edcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inverse = np.fft.irfft(fft_vals, n=df_demo.shape[0]) + x_mean   # inverse FFT (no filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9204b3e-588f-46ad-87f5-3c4eee07dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_demo['demo_signal'][:100], label=\"Original\", linewidth=2)\n",
    "plt.plot(x_inverse[:100], \"o\", markersize=10, alpha=0.6, label=\"Inverse FFT signal\")\n",
    "plt.title(\"Original vs Inverse FFT Reconstruction (No Filtering)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae880f08-fc0c-446f-89df-f439f3fa5bcc",
   "metadata": {},
   "source": [
    "This gives us the opportunity to denoiuse the signal.\n",
    "\n",
    "What we can simply do is we keep only the frequenices (or power spectrum) that we want and cut the rest which we think is noise.\n",
    "\n",
    "This then can be inverted to the original domain and ideally it should represent the signal with no to little noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ea8c88-bbde-4360-922e-56b804053aa2",
   "metadata": {},
   "source": [
    "### Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e527c2d-79f8-4e7f-8df9-2371c4c1a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fft_power(df, col, dt=10/60):\n",
    "    \"\"\"\n",
    "    Pure FFT-based power spectrum.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df  : DataFrame with the signal\n",
    "    col : column name (string)\n",
    "    dt  : sampling interval for the selected unit (e.g. 10/60 for 10 min in hours)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    freqs : array, frequencies in 1/dt units (e.g. cycles/hour)\n",
    "    power : array, power at each frequency (|FFT|^2 / N)\n",
    "    \"\"\"\n",
    "    x = df[col].astype(float).interpolate().bfill().ffill().values\n",
    "    x = x - np.mean(x)          # remove DC\n",
    "\n",
    "    N = len(x)\n",
    "    fft_vals = np.fft.rfft(x)   # one-sided FFT\n",
    "    freqs = np.fft.rfftfreq(N, d=dt)\n",
    "\n",
    "    power = (np.abs(fft_vals) ** 2) / N   # simple power spectrum\n",
    "\n",
    "    return freqs, power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d8bc7-1f25-4fe7-8a2c-15b311c843a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 10/60  # 10 minutes in hours (cycles/hour)\n",
    "fft_dict = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    freqs, power = compute_fft_power(df_no_zero, col, dt=dt)\n",
    "    fft_dict[col] = {\"freqs\": freqs, \"power\": power}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec04d5-ec51-4de4-bbdb-e8dcaac506a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"WindSpeed\"   # pick the column you want\n",
    "\n",
    "freqs = fft_dict[col][\"freqs\"]\n",
    "power = fft_dict[col][\"power\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=freqs,\n",
    "    y=power,\n",
    "    mode=\"lines\",\n",
    "    line=dict(width=2),\n",
    "    name=col\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"FFT Power Spectrum: {col}\",\n",
    "    xaxis_title=\"Freq (cycles/hour)\",\n",
    "    yaxis_title=\"Power\",\n",
    "    height=500,\n",
    "    width=900,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1befe3-cd88-41bc-abfc-e43482b246c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 3\n",
    "n_plots = len(df.columns)\n",
    "n_rows = math.ceil(n_plots / n_cols)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=n_rows,\n",
    "    cols=n_cols,\n",
    "    subplot_titles=df.columns,\n",
    "    vertical_spacing=0.05,\n",
    "    horizontal_spacing=0.05\n",
    ")\n",
    "\n",
    "# ---- Add each FFT plot ----\n",
    "for idx, col in enumerate(df.columns):\n",
    "    row = idx // n_cols + 1\n",
    "    col_pos = idx % n_cols + 1\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=fft_dict[col][\"freqs\"],\n",
    "            y=fft_dict[col][\"power\"],\n",
    "            mode=\"lines\",\n",
    "            name=col\n",
    "        ),\n",
    "        row=row, col=col_pos\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(title_text=\"Freq (cycles/hour)\", row=row, col=col_pos)\n",
    "    fig.update_yaxes(title_text=\"Power\", row=row, col=col_pos)\n",
    "\n",
    "\n",
    "# ---- Set global layout ----\n",
    "fig.update_layout(\n",
    "    height=350 * n_rows,\n",
    "    width=1200,\n",
    "    showlegend=False,\n",
    "    title_text=\"FFT Power Spectra for All Columns\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5dcb9-958d-4445-88b8-436ded5cf450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = \"WindSpeed\"   # pick the column you want\n",
    "\n",
    "freqs = fft_dict[col][\"freqs\"]\n",
    "power = fft_dict[col][\"power\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=freqs,\n",
    "    y=power,\n",
    "    mode=\"lines\",\n",
    "    line=dict(width=2),\n",
    "    name=col\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"FFT Power Spectrum: {col}\",\n",
    "    xaxis_title=\"Freq (cycles/hour)\",\n",
    "    yaxis_title=\"Power\",\n",
    "    height=500,\n",
    "    width=900,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb36b97-e6c3-4f12-9a29-21b50784191b",
   "metadata": {},
   "source": [
    "Now, let's create a functio that cuts off the frequencies that are above some values. This is called low-pass filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a14959a-4fdb-4568-8169-b0dfd1a61e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_lowpass_filter(x, dt, cutoff):\n",
    "    \"\"\"\n",
    "    Apply a low-pass FFT filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        Raw time-series signal.\n",
    "    dt : float\n",
    "        Sampling interval in chosen time units (e.g. 10/60 for cycles/hour).\n",
    "    cutoff : float\n",
    "        Cutoff frequency in same units as FFT output (e.g. cycles/hour).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_filtered : np.array\n",
    "        Filtered time-series (mean added back).\n",
    "    freqs : np.array\n",
    "        Frequency axis.\n",
    "    fft_filtered : np.array\n",
    "        Filtered FFT values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure numpy array\n",
    "    x_clean = np.asarray(x, dtype=float)\n",
    "\n",
    "    # Fill NaNs if needed\n",
    "    if np.isnan(x_clean).any():\n",
    "        nans = np.isnan(x_clean)\n",
    "        x_clean[nans] = np.interp(np.flatnonzero(nans),\n",
    "                                  np.flatnonzero(~nans),\n",
    "                                  x_clean[~nans])\n",
    "\n",
    "    # Store original mean\n",
    "    mean_val = np.mean(x_clean)\n",
    "\n",
    "    # Detrend (remove mean for FFT)\n",
    "    x_detrended = x_clean - mean_val\n",
    "\n",
    "    N = len(x_detrended)\n",
    "\n",
    "    # FFT\n",
    "    fft_vals = np.fft.rfft(x_detrended)\n",
    "    freqs = np.fft.rfftfreq(N, d=dt)\n",
    "\n",
    "    # Low-pass mask\n",
    "    mask = freqs <= cutoff\n",
    "    fft_filtered = fft_vals * mask\n",
    "\n",
    "    # Inverse FFT + ADD MEAN BACK\n",
    "    x_filtered = np.fft.irfft(fft_filtered, n=N) + mean_val\n",
    "\n",
    "    return x_filtered, freqs, fft_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653a22d0-6d83-41fb-8ae9-0e51afc4e4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 1\n",
    "col = \"WindSpeed\"\n",
    "\n",
    "x = df_no_zero[col].values\n",
    "\n",
    "x_filt, freqs, fft_filt = fft_lowpass_filter(x, dt, cutoff)\n",
    "df_no_zero.loc[:, col + \"_filt\"] = x_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c88c6c-635a-4888-964c-ad213f4a46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_no_zero[col][:100], label='Raw Signal')\n",
    "plt.plot(df_no_zero[col + \"_filt\"][:100], label='FFT filtered signal')\n",
    "# plt.plot(df_no_zero[col][:100].rolling(3).mean(), label='Rolling mean signal')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feea731e-a293-4ef6-a4c3-ed0aaf35a50e",
   "metadata": {},
   "source": [
    "Let's compute the correlations before and after filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7447c8-b780-4ee6-9299-f0cd1c55a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_zero['Power'].corr(df_no_zero['WindSpeed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f50d73-07fc-469e-abe6-9eac05fe6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_zero['Power'].corr(df_no_zero['WindSpeed_filt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd7680-7e1f-4b87-adc1-9bbfa2ab309b",
   "metadata": {},
   "source": [
    "We see that the correlation improved because we removed the noise from the signal.\n",
    "\n",
    "This is a promising approach for data cleaning later and ML model improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4be3a0-790d-40a3-807d-84a57b552aac",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ad7cf8-c402-4fb0-9767-9f3351e8a172",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "This report analyzes multivariate behavior of a wind turbine using correlations, time-series exploration, PCA, Anomaly analysis (Mahalanobis distance, and model-based explainability (CatBoost + SHAP)) and Noise study.\n",
    "\n",
    "What we have found is that, despite heavy noise, three variables consistently dominate anomaly formation:\n",
    "\n",
    "**Key Drivers of Abnormal Behavior**\n",
    "- GenRPM  \n",
    "- GenPh1Temp  \n",
    "- Power  \n",
    "\n",
    "These features jointly describe mechanical–thermal stress states inside the turbine. Environmental variables (WindSpeed, WindDirAbs/Rel, Pitch) do not explain anomaly formation, indicating faults are internal rather than wind-driven.\n",
    "\n",
    "**As the next step, it's decided to proceed with modeling Power parameter asn the target in a regression model and then analyzing the deviation of the predicted values from the observed values.**\n",
    "\n",
    "---\n",
    "\n",
    "# More detailed conclusions.\n",
    "\n",
    "1. Many parameters contain strong outliers, including negative values and long tails, which must be cleaned before analysis or modeling.\n",
    "\n",
    "2. Large block of zeros represents full turbine shutdown periods; these distort correlations, PCA structure, and feature distributions, so they must be excluded from relationship analysis.\n",
    "\n",
    "3. KDE and hexbin plots reveal dense point clouds at zero values that cannot be seen in scatter plots but heavily distort distribution shapes and correlations.\n",
    "\n",
    "4. Noise is present across many signals, especially temperature and RPM, and visually overwhelms real structure; denoising significantly clarifies relationships.\n",
    "\n",
    "5. After FFT smoothing, outlier removal and zeros removal, correlations become physically meaningful, showing clearer dependencies between Power, WindSpeed, GenRPM, and RotorRPM. This is also confirmed after resampling the data with daily mean values.\n",
    "\n",
    "6. Some variables show partially linear relationships with Power (e.g., WindSpeed, GenRPM), while others behave inconsistently (e.g., Pitch), suggesting differing predictive value.\n",
    "\n",
    "7. Despite noise reduction using median filter, several variables still show high variability even in median values, indicating multiple operational regimes and non-stationarity.\n",
    "\n",
    "8. PCA reveals outlying values driven primarily by zeros rather than true anomalies, and the first two components do not explain enough variance for reliable representation.\n",
    "\n",
    "9. PCA applied to grouped data still fails to separate anomalies clearly; high PCA scores correspond mostly to high operational values rather than true abnormal behavior.\n",
    "\n",
    "10. Time-series inspection shows that ups and downs of highly correlated features follow Power closely, but no clear anomaly pattern appears before shutdowns using raw or grouped data.\n",
    "\n",
    "11. Mahalanobis distance clearly rises before the downtime, showing true multivariate deviation that includes covariance structure, unlike PCA or raw correlations.\n",
    "\n",
    "12. Across four independent approaches (feature contributions, smoothed relationships, Mahalanobis decomposition, CatBoost+SHAP) applied to Mahalanobis distance, the same three features dominate anomaly formation: Power, GenRPM, and GenPh1Temp.\n",
    "\n",
    "13. These three features are correlated, so their dominance likely represents a joint mechanical–thermal stress regime rather than three independent root causes.\n",
    "\n",
    "14. Outlier and noise removal is essential for reliable modeling because raw data distort the physical relationships between variables; only after cleaning do true dependencies become visible and usable for predictive modeling.\n",
    "\n",
    "15. Low-pass filtering (FFT cutoff)can be used to isolate the low-frequency structure from high-frequency noise, revealing clearer physical relationships between Power, RPM, and temperature variables and improving the interpretability of correlations and trends."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
