{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140e80b-ef71-4b56-a656-95c663f63d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "from typing import Tuple, Union, Literal, List, Dict, Optional, Any\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor as RF\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import plotly.graph_objects as go\n",
    "from utils import plot_time_series\n",
    "import warnings\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de89931-b574-4a0d-bf43-a071786b44cb",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Preparing baseline data](#Preparing-baseline-data)\n",
    "2. [Linear Model Baseline](#Linear-Model-Baseline)\n",
    "3. [Random Forest Baseline](#Random-Forest-Baseline)\n",
    "4. [Data Cleaning](#Data-Cleaning)\n",
    "5. [Analysis of RF predictions](#Analysis-of-RF-predictions)\n",
    "6. [Feature Engineering: Lagged features including Target](#Feature-Engineering:-Lagged-features-including-Target)\n",
    "7. [Feature Engineering: Lagged WITHOUT including Target](#Feature-Engineering:-Lagged-features-WITHOUT-Target)\n",
    "8. [Statistical rolling features](#Statistical-rolling-features)\n",
    "9. [Treating Regression Error as Anomaly](#Treating-Regression-Error-as-Anomaly)\n",
    "10. [Conclusions](#Conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f367bda-b4ff-4813-bdcc-5f8b0c3aa087",
   "metadata": {},
   "source": [
    "# Preparing baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765aed0e-439e-48f0-9d29-3d8409524afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "SEED = 42\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42462d27-d743-456a-94a7-7b7bd0152c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series(df, ['Power'], step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed9ac7-7342-442c-87d6-c78e9206de71",
   "metadata": {},
   "source": [
    "It's a good practice to start modeling ASAP and check what kind of performance we can expect if we use pure raw data and simple models without any tuning.\n",
    "\n",
    "It also helps us to understand the level of data cleaning and feature engineering we likely need to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873bda5-8f18-417f-b07a-b500f4d5700f",
   "metadata": {},
   "source": [
    "**Drop the downtime**\n",
    "\n",
    "There is no point using downtime in the dataset, so we just skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ebf24-b6e8-4207-9c47-9941f19c48e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Power'] > 20].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529971c-df25-4f3d-a1d1-9e3a281e68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46625931-66bd-4292-bb20-459344b35ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314284f-34aa-4865-8c43-23e76a3d0d99",
   "metadata": {},
   "source": [
    "What we will do is that we split the dataset into the train and test part.\n",
    "\n",
    "The train part will be considered as \"NORMAL\".\n",
    "\n",
    "This means that we assume that there are no anomalies in that region.\n",
    "\n",
    "Then the idea is that if the deviation of the measured Power values from the predictions are high, then this indicates that this region is likely to be an anomaly because it's drifting from the \"NORMAL\" training data.\n",
    "\n",
    "We will take the first 30k points as training data. This value can be selected different but it seems to be a good compromise between having enough data from training and being far enough from the downtime where we observed the anomaly happens (which makes sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3cafe-e419-4c0d-8109-5aefa789d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(df: pd.DataFrame, target: str, split_index=30_000) -> Tuple:\n",
    "    \"\"\"\n",
    "    Prepares train and test datasets\n",
    "    \"\"\"\n",
    "    df_train = df[:split_index].copy()\n",
    "    df_test = df[split_index:].copy()\n",
    "    \n",
    "    # Splitting x and y\n",
    "    x_train, y_train = df_train.drop(columns=[target]), df_train[target]\n",
    "    x_test, y_test = df_test.drop(columns=[target]), df_test[target]\n",
    "    \n",
    "    return (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a0987-dc8f-4a5d-a3a1-37989d26b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, \n",
    " y_train, \n",
    " x_test, \n",
    " y_test\n",
    ") = get_datasets(df, 'Power', 30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78e99e-dc9d-40e1-bf2a-468db5ed0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(y_train, label='Train set')\n",
    "plt.plot(y_test, label='Test set')\n",
    "plt.legend(fontsize=16)\n",
    "plt.title('Train Test Split', fontsize=20)\n",
    "plt.ylabel('Power', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51449999-ff95-439b-8412-aeacbc18b592",
   "metadata": {},
   "source": [
    "**Metrics**\n",
    "\n",
    "For time series, it's a golden rule that for the validation/test sets we take values from the future, so we never should split data randomly.\n",
    "\n",
    "During training and validation, we split the training set to 3 validation folds using Sliding Time Series Cross Validation. Sowe always training on the past data and validation on the future data.\n",
    "\n",
    "We then compute both cross validation errors and the test set errors.\n",
    "\n",
    "For the metrics, we will use MAE, RMSE and MAPE error to have different views and decide which main metric wil will use later one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a727e0-b14f-4baa-be46-590f6cf32d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Compute MAE, RMSE, and MAPE for regression predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        Ground truth values.\n",
    "    y_pred : array-like\n",
    "        Predicted values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of floats\n",
    "        (mae, rmse, mape)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-8, None))) * 100\n",
    "    return mae, rmse, mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ae8f7-a2ec-481e-b07c-a4cfc9bf86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    x_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    n_splits: int = 3,\n",
    "    model_name: Literal[\"RF\", \"LinReg\"] = \"RF\",\n",
    "    model_params: Optional[Dict[str, Any]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Evaluate a time series model using TimeSeriesSplit cross-validation\n",
    "    on the training set, then refit on the full train data and evaluate on test.\n",
    "    \"\"\"\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    if model_params is None:\n",
    "        model_params = {}\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_mae_list: list[float] = []\n",
    "    cv_rmse_list: list[float] = []\n",
    "    cv_mape_list: list[float] = []\n",
    "    \n",
    "    # --- Cross-validation ---\n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(x_train), 1):\n",
    "        x_train_cv = x_train.iloc[train_idx, :].copy()\n",
    "        x_val_cv = x_train.iloc[val_idx, :].copy()\n",
    "        y_train_cv = y_train.iloc[train_idx].copy().values.ravel()\n",
    "        y_val_cv = y_train.iloc[val_idx].copy().values.ravel()\n",
    "\n",
    "        x_scaler = StandardScaler()\n",
    "        x_scaled_cv_train = x_scaler.fit_transform(x_train_cv)\n",
    "        x_scaled_cv_val = x_scaler.transform(x_val_cv)\n",
    "\n",
    "        if model_name == \"RF\":\n",
    "            model = RF(**model_params)\n",
    "        elif model_name == \"LinReg\":\n",
    "            model = Ridge(**model_params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "        model.fit(x_scaled_cv_train, y_train_cv)\n",
    "        y_pred_cv = model.predict(x_scaled_cv_val)\n",
    "\n",
    "        mae_err, rmse_err, mape_err = compute_metrics(y_val_cv, y_pred_cv)\n",
    "        cv_mae_list.append(float(mae_err))\n",
    "        cv_rmse_list.append(float(rmse_err))\n",
    "        cv_mape_list.append(float(mape_err))\n",
    "\n",
    "    cv_mae = float(np.mean(cv_mae_list))\n",
    "    cv_rmse = float(np.mean(cv_rmse_list))\n",
    "    cv_mape = float(np.mean(cv_mape_list))\n",
    "\n",
    "    # --- Final model training ---\n",
    "    if model_name == \"RF\":\n",
    "        model = RF(**model_params)\n",
    "    elif model_name == \"LinReg\":\n",
    "        model = Ridge(**model_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "    x_scaler = StandardScaler()\n",
    "    x_scaled_train = x_scaler.fit_transform(x_train)\n",
    "    x_scaled_test = x_scaler.transform(x_test)\n",
    "\n",
    "    model.fit(x_scaled_train, y_train.values.ravel())\n",
    "\n",
    "    y_pred_test = model.predict(x_scaled_test)\n",
    "    mae_err_test, rmse_err_test, mape_err_test = compute_metrics(y_test, y_pred_test)\n",
    "\n",
    "    return {\n",
    "        \"cv_mae\": round(cv_mae, 2),\n",
    "        \"cv_rmse\": round(cv_rmse, 2),\n",
    "        \"cv_mape\": round(cv_mape, 2),\n",
    "        \"test_mae\": round(float(mae_err_test), 2),\n",
    "        \"test_rmse\": round(float(rmse_err_test), 2),\n",
    "        \"test_mape\": round(float(mape_err_test), 2),\n",
    "        \"y_pred_test\": y_pred_test,\n",
    "        \"model\": model,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683d26d-e575-4d04-82d7-385406b9b9c7",
   "metadata": {},
   "source": [
    "# Linear Model Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f2207-3cec-4212-bb30-29a7bd0b799d",
   "metadata": {},
   "source": [
    "First, we start with the simpliest model possible - Linear Model. \n",
    "\n",
    "If this model well, it would be a great baseline and maybe even a production model.  Let's see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57432c34-861b-4cd9-b061-3b8e964fe772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "(x_train, \n",
    " y_train, \n",
    " x_test, \n",
    " y_test\n",
    ") = get_datasets(df, 'Power', 30_000)\n",
    "\n",
    "# Set params\n",
    "params = {\n",
    "    'alpha': 0\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    x_train,\n",
    "    y_train, \n",
    "    x_test, \n",
    "    y_test, \n",
    "    3, \n",
    "    'LinReg', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a9d09-56e5-4c78-aaab-9ebbffd73410",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(y_test.values[:1000], label='True')\n",
    "plt.plot(eval_results['y_pred_test'][:1000], label='Prediction')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ec2e0-6e96-43b2-8530-1819356f39fb",
   "metadata": {},
   "source": [
    "- In general, the predictions do not look good. It seems be overfitted and having high variance.\n",
    "- Also, as we already have seen in EDA, our taregt has outliers, so it can be a big problem, especially for a linear model.\n",
    "- ALWAYS, to make better conclusions, zoom in. Let's do with Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b02fd-57b8-4fd1-af7a-2ee3617dc951",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_predictions(y_true: pd.Series, y_pred: pd.Series) -> None:\n",
    "    \"\"\"\n",
    "    Plots interactive predictions with Plotly\n",
    "    \"\"\"\n",
    "    # Assume y_test and y_pred are pandas Series with a datetime index\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add actual values\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=y_true.index, y=y_true.values,\n",
    "        mode='lines',\n",
    "        name='Actual',\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "    \n",
    "    # Add predicted values\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=y_true.index, y=y_pred,\n",
    "        mode='lines',\n",
    "        name='Predicted',\n",
    "        line=dict(width=1)\n",
    "    ))\n",
    "    \n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        title='Actual vs Predicted Values',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Value',\n",
    "        legend=dict(x=0, y=1),\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7fbed-e2cb-4168-befb-f5352ae430b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, eval_results['y_pred_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b13651-d4d5-40c0-94a2-542756163ffa",
   "metadata": {},
   "source": [
    "Let's try to add some regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a857933-9590-4193-a091-d8fc333636fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "(x_train, \n",
    " y_train, \n",
    " x_test, \n",
    " y_test\n",
    ") = get_datasets(df, 'Power', 30_000)\n",
    "\n",
    "# Set params\n",
    "params = {\n",
    "    'alpha': 10\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    x_train,\n",
    "    y_train, \n",
    "    x_test, \n",
    "    y_test, \n",
    "    3, \n",
    "    'LinReg', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bf200-2a9e-43c6-843b-788b8e07ed0d",
   "metadata": {},
   "source": [
    "We see that adding regularization does not help much.\n",
    "\n",
    "It seems that with such noisy but dense data, it will be hard to fit a linear model, at least, without an extensive cleaning.\n",
    "\n",
    "It can also be the case that our problem is non-linear.\n",
    "\n",
    "This actually makes sense because we saw visually and even during PCA, the first 2 components did not explain even 80% of the variance which also indicates the problem non-linearity.\n",
    "\n",
    "Let's fit a non-linear model, e.g. Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02cfcd5-f82e-4bd4-90ca-6141ae114e27",
   "metadata": {},
   "source": [
    "# Random Forest Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9921f-c557-482a-9a8b-5eb8bed153df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "(x_train, \n",
    " y_train, \n",
    " x_test, \n",
    " y_test\n",
    ") = get_datasets(df, 'Power', 30_000)\n",
    "\n",
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    x_train,\n",
    "    y_train, \n",
    "    x_test, \n",
    "    y_test, \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6ed37-a3bf-4e7f-a2ff-095dc142f28a",
   "metadata": {},
   "source": [
    "We see a sudden improvement. That's great! Let's see what the predictions look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa45e9-3ef4-4551-9d45-32afd5d7ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, eval_results['y_pred_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ab9a5-36b0-4c6d-b3f4-abc0c3d3126a",
   "metadata": {},
   "source": [
    "We still see high variance but the predictions are way more stable now.\n",
    "\n",
    "Also, we can easily see that the predictions do not go below the bottom level out of the box but the linear model did not manage to do that.\n",
    "\n",
    "Let's also check how the built-in feature importance looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9936f08-c546-44c9-8db9-a0fd7009bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values\n",
    "importances = eval_results['model'].feature_importances_\n",
    "cols = x_train.columns\n",
    "\n",
    "# Combine into DataFrame\n",
    "feat_imp = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": cols,\n",
    "        \"importance\": importances\n",
    "    })\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "feat_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4081a8-cf48-46e7-8f79-db2f5a67624e",
   "metadata": {},
   "source": [
    "Among the TOP 2 features, we see the same ones that we observed in our EDA analysis.\n",
    "\n",
    "We also see that many features are WAY less important. Let's keep it in mind for further modelling.\n",
    "\n",
    "Among the two baselines, it's obvious we need to go with Random Forest and explore it further because:\n",
    "- It has smaller error\n",
    "- It doesn't produce values below the low limit like Linear Regression.\n",
    "\n",
    "We also see that the spikes are not predicted which is a good sign - the model does not get overfitted (it could be different in case of Gradient Boosting or Neural Network, for example).\n",
    "\n",
    "However, in EDA we saw that features and the target itself have outliers which can negatively impact the model accuracy.\n",
    "\n",
    "Let's see if basic outlier removal would help."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391171e7-4d90-44c5-8120-a4e32482ac0d",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc2ead-afdf-4732-9963-ea7ab3db401e",
   "metadata": {},
   "source": [
    "First, it's crucial to deal with outliers in target.\n",
    "\n",
    "In general, by properly cleaning the target, we can achieve one of the highest boosts in model performance.\n",
    "\n",
    "This is especially the case in this problem, because measurements are very noisy and have many outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18cc82-9d90-4340-ba5e-14a8a828b930",
   "metadata": {},
   "source": [
    "## Removing Outliers in target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1529bc-9f95-4bde-b2b9-571c2b5c20a3",
   "metadata": {},
   "source": [
    "For quick iterations between different experiments, it's better to read the data again in each section.\n",
    "\n",
    "This reduces a change to operate with the dataframe that has already been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d94d49-e630-415f-a3ec-e26ba0c9e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "df = df[df['Power'] > 20].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ef140-d56b-4c25-8bae-45e593791ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, \n",
    " y_train,  \n",
    " x_test, \n",
    " y_test\n",
    ") = get_datasets(df, 'Power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c42cf32-3007-4afc-bf2a-77eab59f5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_train)\n",
    "plt.ylim(0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120eabfc-9535-42e6-af88-883eb0c053e3",
   "metadata": {},
   "source": [
    "We clearly see that there are outliers. Let's remove them using z-score filtering.\n",
    "\n",
    "For now, we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22dd73b-cacd-4ae1-9a19-643f65961572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score visualization\n",
    "mean = df['Power'].mean()\n",
    "std = df['Power'].std(ddof=0)\n",
    "z_score = np.abs((df['Power'] - mean) / std)\n",
    "sns.histplot(z_score)\n",
    "plt.xlim(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9795922-caff-41c3-bab0-aae7df041afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_zscore(\n",
    "    df: pd.DataFrame,\n",
    "    threshold: float = 3.0,\n",
    "    nan_treatment: str = 'ffill',\n",
    "    stats: Optional[Dict[str, Tuple[float, float]]] = None\n",
    ") -> Tuple[pd.DataFrame, Dict[str, Tuple[float, float]]]:\n",
    "    \"\"\"\n",
    "    Replace outliers (per-column z-score > threshold) with NaN.\n",
    "    Optionally forward-fills or drops the resulting NaNs.\n",
    "    If stats are provided, uses them; otherwise computes and returns them\n",
    "    (so the same parameters can be applied to test data).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input data (numeric/mixed).\n",
    "    threshold : float\n",
    "        |z| cutoff.\n",
    "    nan_treatment : {'ffill','drop'}\n",
    "        How to handle NaNs.\n",
    "    stats : dict or None\n",
    "        Precomputed {col: (mean, std)}.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_masked : pd.DataFrame\n",
    "        DataFrame with outliers replaced and NaN treatment applied.\n",
    "    fit_stats : Dict[str,(float,float)]\n",
    "        Mean and std used (save for test).\n",
    "    \"\"\"\n",
    "    df_masked = df.copy()\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    fit_stats: Dict[str, Tuple[float, float]] = {}\n",
    "\n",
    "    if stats is None:\n",
    "        for col in numeric_cols:\n",
    "            fit_stats[col] = (df[col].mean(), df[col].std(ddof=0))\n",
    "    else:\n",
    "        fit_stats = stats\n",
    "\n",
    "    total_changed = 0\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        mean, std = fit_stats[col]\n",
    "        if std == 0:\n",
    "            print(f\"{col}: std==0, skipped\")\n",
    "            continue\n",
    "\n",
    "        z = np.abs((df[col] - mean) / std)\n",
    "        changed = int((z > threshold).sum())\n",
    "        total_changed += changed\n",
    "\n",
    "        print(f\"{col}: {changed} replaced\")\n",
    "\n",
    "        df_masked.loc[z > threshold, col] = np.nan\n",
    "\n",
    "    print(f\"TOTAL replaced: {total_changed}\")\n",
    "\n",
    "    if nan_treatment == 'ffill':\n",
    "        df_masked = df_masked.ffill()\n",
    "    elif nan_treatment == 'drop':\n",
    "        df_masked = df_masked.dropna()\n",
    "    else:\n",
    "        raise ValueError(f\"nan_treatment '{nan_treatment}' not recognized.\")\n",
    "\n",
    "    return df_masked, fit_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fcbe7-4b26-44b3-a74f-e7fa70f52c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_clean, _ = remove_outliers_zscore(pd.DataFrame(y_train), nan_treatment='ffill', threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec0ecf2-4b3f-40aa-9217-105e25f1089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "117 / y_train.shape[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a87e7-e81d-4e8e-93da-3b56ae9d0ce1",
   "metadata": {},
   "source": [
    "So, we replaced only 117 values (or 0.39%).\n",
    "\n",
    "Let's see how this influences the model performance>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a5918-6ebd-4452-964c-8fb1e8d1334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_train, label='Raw Data')\n",
    "sns.histplot(y_train_clean.values.ravel(), label='Cleaned Data')\n",
    "plt.legend()\n",
    "# plt.ylim(0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fb1f4-adc9-4976-b3d5-8ff2038028be",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    x_train,\n",
    "    y_train_clean, \n",
    "    x_test, \n",
    "    y_test, \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb079134-b5bd-462a-abeb-611e7fe817af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 70.0,\n",
    "#  'cv_rmse': 113.31,\n",
    "#  'cv_mape': 8.15,\n",
    "#  'test_mae': 74.51,\n",
    "#  'test_rmse': 122.62,\n",
    "#  'test_mape': 9.76,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e83019-a00b-4001-a94b-b3623a88e106",
   "metadata": {},
   "source": [
    "We see that we reduced the Cross-Validation errors a lot which makes sense because we don't compute the errors for the outliers now.\n",
    "\n",
    "But what is also important to notice is that we reduced the test errors qute a lot EVEN THOUGH we haven't removed the outliers there.\n",
    "\n",
    "So, removing outliers that cover 0.5% of the data is crucial and can produce better results that endless model tuning.\n",
    "\n",
    "Now, let's check outliers in features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425be9f6-a4a6-4dcc-9621-aad757f6eefc",
   "metadata": {},
   "source": [
    "## Outliers removal in features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c7deaf-861e-48c9-9d1d-c8fdff87a4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "df = df[df['Power'] > 20].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be785a4-a064-48a4-9ba9-4175cac2dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, \n",
    " y_train,  \n",
    " x_test, \n",
    " y_test\n",
    ") = get_datasets(df, 'Power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e67710-516c-4361-8900-5b2b29808d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns only\n",
    "numeric_cols = x_train.select_dtypes(include=np.number).columns\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(numeric_cols) / n_cols))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.histplot(x_train[col].dropna(), bins=20, kde=True)\n",
    "    plt.title(col, fontsize=10)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db72f6-56fb-4667-90b1-777e7759edf9",
   "metadata": {},
   "source": [
    "For simplicity and comparison purposes, we will clean the outliers only in the train set.\n",
    "\n",
    "However, if the whole idea of removing outliers work, we would need to clean these outliers in the test (aka production) data too.\n",
    "\n",
    "This is because when the model deployed, we want to have the same data preprocessing that we use to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9c343-3b02-4370-9bef-eab550397775",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_clean, _ = remove_outliers_zscore(pd.DataFrame(y_train), nan_treatment='ffill', threshold=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842d7617-a90e-48ed-98a2-d314847ed405",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_clean, z_score_stats = remove_outliers_zscore(x_train, nan_treatment='ffill', threshold=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba05fdf5-639f-4a87-a262-0d68125f5be6",
   "metadata": {},
   "source": [
    "Since we are cleaning features, it's essential to use the same cleaning on both training and test sets.\n",
    "\n",
    "In this particular filter, we save the mean and std statistics from the training set and then apply them on the test set.\n",
    "\n",
    "**If we do not do that, we would introduce data leackage because we would use the infromation from the future on the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b29765-81e4-456c-a09e-c3c42c0cc44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate drop as well\n",
    "x_test_clean, _ = remove_outliers_zscore(x_test, nan_treatment='ffill', threshold=3, stats=z_score_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625235b6-c31f-4c9a-8042-c3726730d699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select numeric columns only\n",
    "numeric_cols = x_train_clean.select_dtypes(include=np.number).columns\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(numeric_cols) / n_cols))\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.histplot(x_train_clean[col].dropna(), bins=20, kde=True)\n",
    "    plt.title(col, fontsize=10)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f59fd-98f5-419c-a485-357329411455",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    x_train_clean,\n",
    "    y_train_clean, \n",
    "    x_test_clean, \n",
    "    y_test, \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdc3e5-5507-4c1a-972a-e792b49e328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 65.02,\n",
    "#  'cv_rmse': 95.33,\n",
    "#  'cv_mape': 7.64,\n",
    "#  'test_mae': 71.88,\n",
    "#  'test_rmse': 120.91,\n",
    "#  'test_mape': 9.29,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6f2b3-07f9-4c16-a315-5775efffc973",
   "metadata": {},
   "source": [
    "We see that the error is not influenced much in this case.\n",
    "\n",
    "In fact, it became slightly worse.\n",
    "\n",
    "This is true for this particular dataset.\n",
    "\n",
    "However, this might NOT be true for many-many other datasets.\n",
    "\n",
    "So, removing outliers in features IN ADDITION to the target is a MUST step to check in any time series-based analysis.\n",
    "\n",
    "In this case, we WILL implement outlier removal in features as well, so you know how to implement this in production and then later in your work projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574eecfa-4940-4760-8c4c-ba21011685e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, eval_results['y_pred_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff2950c-d762-4ef8-94b7-37dd6cd21fd0",
   "metadata": {},
   "source": [
    "### Outlier dropping vs ffilling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb676dc6-1523-4976-94b4-ff8767a19313",
   "metadata": {},
   "source": [
    "Pure data dropping is usually not good because it's harder to maintain in production (making sure that the indicies are aligned, etc) and we lose information.\n",
    "\n",
    "Unless we get a very good improvement, it's better to stick with some filling or interpolation options.\n",
    "\n",
    "On the other hand, forward filling might not be an ideal option because it create data that in realily never existed.\n",
    "\n",
    "You can fill with other options but:\n",
    "\n",
    "1. It's gives headache in production\n",
    "2. Again, you will create data that never existed.\n",
    "\n",
    "For now, let's just make forward fill because then we don't need to make sure that X and y indicies match (twice!).\n",
    "\n",
    "However, it's a good execise to test dropping the indicies for the outliying values and see how the model behaves, so feel free!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b7eb7-0fb1-427b-9bcc-6273ea8bbc7b",
   "metadata": {},
   "source": [
    "# Analysis of RF predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef011e3-7bb3-435c-afa4-353b64254b7d",
   "metadata": {},
   "source": [
    "Ok, now as we have built the first robust baseline, we can start analyzing the predictions in more detail.\n",
    "\n",
    "Let's prepare the function that plots the actual values VS ML model predictions.\n",
    "\n",
    "Also, since our dataset is very noisy and we don't clean (for now) the test set, the erro value can be noisy as well.\n",
    "\n",
    "So, we introduce the median rolling to see the trends better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc193c4-86c7-4b5a-b958-3d0535cdfc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(\n",
    "    x_true: pd.DataFrame,\n",
    "    y_true: Union[pd.Series, pd.DataFrame],\n",
    "    y_pred: Union[np.ndarray, list],\n",
    "    error: Literal[\"mae\", \"mape\"],\n",
    "    error_threshold: float,\n",
    "    rolling_window: int\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot prediction errors with detected anomalies and threshold lines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_true : pd.DataFrame\n",
    "        Original input data (with datetime index).\n",
    "    y_true : pd.Series or pd.DataFrame\n",
    "        True target values.\n",
    "    y_pred : array-like\n",
    "        Predicted target values.\n",
    "    error : {\"mae\", \"mape\"}\n",
    "        Type of error to visualize.\n",
    "    error_threshold : float\n",
    "        Threshold above which points are flagged as anomalies.\n",
    "    rolling_window: int\n",
    "        Rollin window for the error rolling aggregation\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a DataFrame with true values and compute the error\n",
    "    y_test_err = pd.DataFrame(y_true)\n",
    "    if error == \"mae\":\n",
    "        y_test_err['Error'] = abs(y_test_err['Power'] - y_pred)\n",
    "    elif error == 'rmse':\n",
    "        y_test_err['Error'] = np.sqrt((y_test_err['Power'] - y_pred)**2)\n",
    "    elif error == 'mape':\n",
    "        y_test_err['Error'] = abs(y_test_err['Power'] - y_pred) / y_test_err['Power'] * 100\n",
    "\n",
    "    # Ensure both y_test_err and x_true have datetime indices\n",
    "    y_test_err.index = pd.to_datetime(y_test_err.index)\n",
    "    x_true.index = pd.to_datetime(x_true.index)\n",
    "\n",
    "    # Initialize Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Plot the error over time\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=y_test_err.index, \n",
    "        y=y_test_err['Error'].values,\n",
    "        mode='lines',\n",
    "        name='Error',\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "\n",
    "    # Add horizontal line for static 95th percentile threshold\n",
    "    fig.add_hline(\n",
    "        y=error_threshold,\n",
    "        line_color=\"black\",\n",
    "        annotation_text=\"Upper Threshold\",\n",
    "        annotation_position=\"bottom right\"\n",
    "    )\n",
    "\n",
    "    # Plot rolling median error for smoothed trend\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=y_test_err.index,\n",
    "        y=y_test_err['Error'].rolling(rolling_window).median(),\n",
    "        mode='lines',\n",
    "        name='Rolling Median Error',\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "\n",
    "    # Configure layout settings\n",
    "    fig.update_layout(\n",
    "        title='Prediction Error and Anomaly Detection',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Error',\n",
    "        legend=dict(x=0, y=1),\n",
    "        height=500,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace97fe5-05c3-441b-96cd-a9f278426ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mae', error_threshold=100, rolling_window=228) # 72, 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e0ceb-3d18-4b81-b8df-747c80a66532",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=10, rolling_window=228) # 72, 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916386e-003c-4216-a65a-d554d48b47de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_error = abs(y_test - eval_results['y_pred_test'])\n",
    "mae_error_rol = mae_error.rolling(144).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3fa7d-57f5-4bf6-9920-28a1f4089a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_error = abs(y_test - eval_results['y_pred_test']) / y_test * 100\n",
    "mape_error_rol = mape_error.rolling(144).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e01ee-dac7-431a-8f04-0ed4312b4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(mape_error, label='Raw MAPE')\n",
    "sns.histplot(mape_error_rol, label='Rolling MAPE')\n",
    "plt.legend()\n",
    "plt.xlim(0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9593c960-79a6-4d80-99cd-87e712be56bd",
   "metadata": {},
   "source": [
    "We see that the rolling value is much more stable and it;s also easier to select t=a threshold above which we can consider the errors to show anomalies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1adcc8-fc28-4319-b6e8-bba080f5e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(mape_error_rol.dropna(), 0.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c578123-d409-4b29-a1e5-9b6456b1f706",
   "metadata": {},
   "source": [
    "Generally, we can have 2 options:\n",
    "1. Fix the threshold value.\n",
    "2. Compute it as a quantile.\n",
    "\n",
    "For simplicity, we can keep just the value and then switch if we want to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34c9ed-9e17-4eb3-9772-9ca8dd78d69b",
   "metadata": {},
   "source": [
    "**Preliminary Conclusions**\n",
    "\n",
    "We can see that we can identify the anomaly region about 18-19 days in advance which is quite good.\n",
    "\n",
    "The number of days to value is out business metric and THIS is the metric we need to optimize.\n",
    "\n",
    "For experiments, we can still for now leave Cross Validation ML metrics with the assumption that the better metrics, the mode stable and accurate predictions of out business metrics are.\n",
    "\n",
    "Let's iterate using the ML metrics and for some promising models compute the Time-to-Failure metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8cb917-8584-4747-b08c-b45c657eb28c",
   "metadata": {},
   "source": [
    "# Feature Engineering: Lagged features including Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1a5f6-7c99-46d7-9740-352c10305a76",
   "metadata": {},
   "source": [
    "Before computing features, it's highly recommended that we remove the outliers BEFORE it.\n",
    "\n",
    "Otherwise, we would need to remove outliers twice which can introduce even more noise to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f5751a-4121-466a-964c-f7cdb2424bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "# First, we need to get the training dataset\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda811d-9c17-4203-bc2e-0a9e0a640690",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean_train, z_score_stats = remove_outliers_zscore(df_train, threshold=3, nan_treatment='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df53a34-3cb9-4619-bdc5-ad596d0b6b8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaning the features in the test set\n",
    "features = [col for col in df.columns if col not in ['Power']]\n",
    "x_clean_test, _ = remove_outliers_zscore(df_test[features], threshold=3, nan_treatment='ffill', stats=z_score_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed0a0a-9cf5-42a9-815c-e50dcf8ca4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_test = pd.concat([x_clean_test, df_test['Power']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed50459-9879-4218-a427-3215de7a60e2",
   "metadata": {},
   "source": [
    "Now let's add lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba62119-1fed-4bc0-971e-685b099cc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(df: pd.DataFrame, columns:List[str]=None, lags: List[int]=[1], drop_na=True):\n",
    "    \"\"\"\n",
    "    Add lag features to DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - columns: list of column names (default: all numeric)\n",
    "    - lags: int or list of lag periods (default: 1)\n",
    "    - drop_na: bool, drop NaN rows (default: True)\n",
    "    \n",
    "    Returns: DataFrame with lag features\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for col in columns:\n",
    "        for lag in lags:\n",
    "            df_result[f\"{col}_lag{lag}\"] = df_result[col].shift(lag)\n",
    "    \n",
    "    if drop_na:\n",
    "        return df_result.dropna()\n",
    "    else:\n",
    "        return df_result.bfill()  # Backward fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba868dd8-1fdd-4f14-ab24-206419bd7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding lagged features\n",
    "df_train_feat = add_lag_features(df_clean_train, columns=['Power', 'GenRPM'], lags=[1, 2, 3], drop_na=False)\n",
    "x_test_feat = add_lag_features(df_clean_test, columns=['Power', 'GenRPM'], lags=[1, 2, 3], drop_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a591d-eef9-42b9-a282-bf16e7ce99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(df_clean_train['Power'][:200], label='Raw Signal')\n",
    "plt.plot(df_train_feat['Power_lag1'][:200], label='Lagged Signal')\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8ef48-d89a-49c6-83ef-9b8821eeff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume y_test and y_pred are pandas Series with a datetime index\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add actual values\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_train.index, y=df_train['Power'].values,\n",
    "    mode='lines',\n",
    "    name='Actual',\n",
    "    line=dict(width=2)\n",
    "))\n",
    "\n",
    "# Add predicted values\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_train.index, y=df_clean_train['Power'].values,\n",
    "    mode='lines',\n",
    "    name='Predicted',\n",
    "    line=dict(width=2)\n",
    "))\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title='Actual vs Cleaned Values',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Value',\n",
    "    legend=dict(x=0, y=1),\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf33f443-6d82-4371-a225-4d6db027ecdd",
   "metadata": {},
   "source": [
    "It seems that the outliers are cleaned quite well.\n",
    "\n",
    "However, let's look at the cleaned data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a134f440-f3e3-491a-99a1-40da402b3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_series(df_clean_train, ['Power'], step=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf6b5d-9644-4ea2-acf5-5e89bcae80ca",
   "metadata": {},
   "source": [
    "Even though the outliers are similar in nature, z-score filter cannot capture it.\n",
    "\n",
    "This is important because we add lagged features, so if the data is not cleaned well, we create additional outlying values to the training data.\n",
    "\n",
    "We will consider this fact when we do more in-depth cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd71711-bf1b-497c-831c-250f91a88794",
   "metadata": {},
   "source": [
    "Note how we add the features only AFTER the data split into the train and test to:\n",
    "\n",
    "1. Avoid data leakage into the future.\n",
    "2. Slowly preparing the code and flow that we will then put to the production pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c696980-4ae8-4447-9494-038dd308ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in df_train_feat.columns if col != 'Power'] # Note that we keep Power_lagged features\n",
    "# Train\n",
    "x_clean_train = df_train_feat[features].copy()\n",
    "y_clean_train = df_train_feat['Power'].copy()\n",
    "\n",
    "# Test\n",
    "x_clean_test = x_test_feat[features].copy()\n",
    "y_test = df_test['Power'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b576c-caa0-4241-bbbf-6a81db89c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    x_clean_train,\n",
    "    y_clean_train, \n",
    "    x_clean_test, \n",
    "    y_test, \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d709ada5-9de1-47fa-98d9-c794f2c98108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 65.08,\n",
    "#  'cv_rmse': 95.42,\n",
    "#  'cv_mape': 7.65,\n",
    "#  'test_mae': 71.44,\n",
    "#  'test_rmse': 120.68,\n",
    "#  'test_mape': 9.22,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c054db2-dc10-4ca3-b548-e990827049a0",
   "metadata": {},
   "source": [
    "Nice! We see that we significanlty the errors.\n",
    "\n",
    "Let's see now how this looks in terms of Time to Failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758908d4-7609-4f17-a8ae-aadb9f3fa4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mae', error_threshold=40, rolling_window=288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63a947f-b674-412a-b436-b6ebaca62146",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=6, rolling_window=288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2ae9a-6e12-4ea7-9767-870c9dc976a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, eval_results['y_pred_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bcf43b-af34-4012-a438-9e23f867ee76",
   "metadata": {},
   "source": [
    "Very often adding lagged features help a lot.\n",
    "\n",
    "In this case, it also helped to reduce the error.\n",
    "\n",
    "However, in our case, even though the predictions are better, the problem is that we start including the \"degraded\" parameter into the model as the feature.\n",
    "\n",
    "This results in the fact that it becomes HARDER to see the anomaly region and distinguish it from the normal region.\n",
    "\n",
    "All this can make it confusing to interpret the model behavior.\n",
    "\n",
    "Let's NOT include Power as a feature and instead include some other parameters.\n",
    "\n",
    "**THIS SHOWS AN EXAMPLE WHEN BETTER ML METRICS DO NOT RESULT IN BETTER BUSINESS METRICS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0eb98-63cc-44e6-b202-ea0bd99abad2",
   "metadata": {},
   "source": [
    "# Feature Engineering: Lagged features WITHOUT Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48d2663-d9d6-406f-8e2d-7e9a9b2aba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "# First, we need to get the training dataset\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d0f08-4d35-4b52-a436-8493f3fe1d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train, z_score_stats = remove_outliers_zscore(df_train, threshold=3, nan_treatment='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481daee8-8a6a-464d-ba55-b6c6fdb264ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the features in the test set\n",
    "features = [col for col in df.columns if col not in ['Power']]\n",
    "x_clean_test, _ = remove_outliers_zscore(df_test[features], threshold=3, nan_treatment='ffill', stats=z_score_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52755593-47c0-4e0b-ab7f-a10b3024c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_test = pd.concat([x_clean_test, df_test['Power']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8c1ae6-0ff7-4b87-a8dd-9f19af7f5716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding lagged features\n",
    "df_train_feat = add_lag_features(df_clean_train, columns=['GenRPM', 'GenPh1Temp'], lags=[1, 2, 3], drop_na=False)\n",
    "x_test_feat = add_lag_features(df_clean_test, columns=['GenRPM', 'GenPh1Temp'], lags=[1, 2, 3], drop_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c462d0-38e0-41d0-8f9a-4a0f08aedca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in df_train_feat.columns if col != 'Power']\n",
    "# Train\n",
    "x_clean_train = df_train_feat[features].copy()\n",
    "y_clean_train = df_train_feat['Power'].copy()\n",
    "\n",
    "# Test\n",
    "x_clean_test = x_test_feat[features].copy()\n",
    "y_test = df_test['Power'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0304a4-8c9b-4af9-9705-a53880ac9ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': 100, \n",
    "    'random_state': SEED,\n",
    "    'n_jobs':-1\n",
    "}\n",
    "\n",
    "eval_results  = eval_model(\n",
    "    x_clean_train,\n",
    "    y_clean_train, \n",
    "    x_clean_test, \n",
    "    y_test, \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cee22d-3216-48a6-ae36-5e1b04577dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 65.02,\n",
    "#  'cv_rmse': 95.33,\n",
    "#  'cv_mape': 7.64,\n",
    "#  'test_mae': 71.88,\n",
    "#  'test_rmse': 120.91,\n",
    "#  'test_mape': 9.29,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18dc91-7bb1-4820-95ca-4b66f690eeb5",
   "metadata": {},
   "source": [
    "Cool! We see that we still reduced the error but hopefully we have NOT deteriorated the anomaly class close to the downtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37658d-6df0-4c4a-8423-65978780560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=8, rolling_window=228)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069cfd2-79e0-40c3-a4ea-2ce400e895d3",
   "metadata": {},
   "source": [
    "We see tht we can still clearly distiguish the anomaly class AND we reduced the cross-validation error.\n",
    "\n",
    "It seems that this is a perfect mix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b047f9-c11a-4a21-8156-6d0861ddb720",
   "metadata": {},
   "source": [
    "# Statistical rolling features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af2b30-f7bf-45b2-ab89-953f6975f314",
   "metadata": {},
   "source": [
    "Now, let's add basic statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53d7d0-7aa7-48a7-9721-b80532720888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_features(df, columns, window_sizes=7, stats=['mean', 'median'], drop_na=True):\n",
    "    \"\"\"\n",
    "    Add rolling features to DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - columns: list of column names\n",
    "    - window_sizes: int or list of window sizes (default: 7)\n",
    "    - stats: list of statistics ['mean', 'median', 'std', 'min', 'max', 'skew', 'kurt']\n",
    "    - drop_na: bool, drop NaN rows (default: True)\n",
    "    \n",
    "    Returns: DataFrame with rolling features\n",
    "    \"\"\"\n",
    "    df_result = df.copy()\n",
    "    \n",
    "    # Convert single values to lists\n",
    "    if isinstance(window_sizes, int):\n",
    "        window_sizes = [window_sizes]\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    \n",
    "    # Create rolling features\n",
    "    for col in columns:\n",
    "        for window in window_sizes:\n",
    "            rolling = df_result[col].rolling(window)\n",
    "            \n",
    "            for stat in stats:\n",
    "                if stat == 'mean':\n",
    "                    df_result[f\"{col}_roll{window}_mean\"] = rolling.mean()\n",
    "                elif stat == 'median':\n",
    "                    df_result[f\"{col}_roll{window}_median\"] = rolling.median()\n",
    "                elif stat == 'std':\n",
    "                    df_result[f\"{col}_roll{window}_std\"] = rolling.std()\n",
    "                elif stat == 'min':\n",
    "                    df_result[f\"{col}_roll{window}_min\"] = rolling.min()\n",
    "                elif stat == 'max':\n",
    "                    df_result[f\"{col}_roll{window}_max\"] = rolling.max()\n",
    "                elif stat == 'skew':\n",
    "                    df_result[f\"{col}_roll{window}_skew\"] = rolling.skew()\n",
    "                elif stat == 'kurt':\n",
    "                    df_result[f\"{col}_roll{window}_kurt\"] = rolling.kurt()\n",
    "    if drop_na:\n",
    "        return df_result.dropna()\n",
    "    else:\n",
    "        return df_result.bfill()  # Backward fill NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaa3e5-8d1b-48f5-8abb-bcb4631ff240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read again for reproducibility\n",
    "df = pd.read_parquet('../data/01_raw/df_train_test.parquet')\n",
    "df = df[df['Power'] > 20].copy()\n",
    "df.index = pd.to_datetime(df['Timestamps'])\n",
    "df.drop(columns=['Timestamps'], inplace=True)\n",
    "# First, we need to get the training dataset\n",
    "df_train = df[:30_000]\n",
    "df_test = df[30_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673fe15e-24ab-40cf-b43b-bd233a4cd64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean_train, z_score_stats = remove_outliers_zscore(df_train, threshold=3, nan_treatment='ffill')\n",
    "features = [col for col in df.columns if col not in ['Power']]\n",
    "x_clean_test, _ = remove_outliers_zscore(df_test[features], threshold=3, nan_treatment='ffill', stats=z_score_stats)\n",
    "df_clean_test = pd.concat([x_clean_test, df_test['Power']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc72b14-80df-4a80-95ec-1d3d9a1be58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding lagged features\n",
    "df_clean_train = add_lag_features(df_clean_train, columns=['GenRPM', 'GenPh1Temp'], lags=[1, 2, 3], drop_na=False)\n",
    "df_clean_test = add_lag_features(df_clean_test, columns=['GenRPM', 'GenPh1Temp'], lags=[1, 2, 3], drop_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2bc60-8272-4a39-8f62-5938aaa23f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_feat =  add_rolling_features(\n",
    "    df_clean_train,\n",
    "    window_sizes=[5, 10, 15],\n",
    "    columns=['GenRPM', 'WindSpeed', 'GenPh1Temp'], \n",
    "    stats=['median', 'std', 'min', 'max'],\n",
    "    drop_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2835d-bc52-4e98-8642-b2d14642aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_feat =  add_rolling_features(\n",
    "    df_clean_test,\n",
    "    window_sizes=[5, 10, 15],\n",
    "    columns=['GenRPM', 'WindSpeed', 'GenPh1Temp'],\n",
    "    stats=['median', 'std', 'min', 'max'],\n",
    "    drop_na=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554e342-da89-4ace-b13c-eda97781ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in df_train_feat.columns if col != 'Power']\n",
    "# Train\n",
    "x_clean_train = df_train_feat[features].copy()\n",
    "y_clean_train = df_train_feat['Power'].copy()\n",
    "\n",
    "# Test\n",
    "x_clean_test = x_test_feat[features].copy()\n",
    "y_test = df_test['Power'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8111a-802b-4886-8ee7-8b3df445723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results  = eval_model(\n",
    "    x_clean_train,\n",
    "    y_clean_train, \n",
    "    x_clean_test, \n",
    "    y_test, \n",
    "    3, \n",
    "    'RF', \n",
    "    params\n",
    ")\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd3fd8-8334-450c-ba23-b93230b1796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'cv_mae': 56.32,\n",
    "#  'cv_rmse': 82.79,\n",
    "#  'cv_mape': 6.64,\n",
    "#  'test_mae': 65.32,\n",
    "#  'test_rmse': 115.11,\n",
    "#  'test_mape': 8.39,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d15bd-28e0-4b07-aa56-b3d31d81bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=8, rolling_window=288)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1727eb1-9b3b-4989-ac17-66be40a4a0c6",
   "metadata": {},
   "source": [
    "We see that overall, it seems we improved the model performance.\n",
    "\n",
    "This can potentially confirm the fact that the noise in features make our model performance worse and when adding rolling features, we reduce this noise and improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb249b9-764c-4cd7-abc8-9bc2821accd7",
   "metadata": {},
   "source": [
    "# Treating Regression Error as Anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44e4968-4dd9-4e7c-8243-58b495bd57d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(y_test, eval_results['y_pred_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6735a3-f13f-47bc-b603-476dd396b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, eval_results['y_pred_test'], error='mape', error_threshold=8, rolling_window=288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008d0dc-9fa9-4cf2-8b3b-7319e4f88f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_error = abs(y_test - eval_results['y_pred_test']) / y_test * 100\n",
    "mape_error_rol = mape_error.rolling(288).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112796a-0ddd-4a35-8ad1-6e4a768051d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(mape_error_rol)\n",
    "plt.xlim(0, 30)\n",
    "plt.xlabel('MAPE TEST SET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273a18f1-9848-4fe0-84d3-a179706d3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(mape_error_rol.dropna(), 0.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7151cb8e-e4b3-4db8-8c85-2a4b76956add",
   "metadata": {},
   "source": [
    "### Let's compute how much time in advance we can detect the anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880e2bb9-8d8e-4183-81e7-fd20dc16202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomaly(\n",
    "    y_true: np.ndarray | pd.Series,\n",
    "    y_pred: np.ndarray | pd.Series,\n",
    "    threshold: float,\n",
    "    waiting_time: int,\n",
    "    rolling_window: int,\n",
    "    error_type: Literal['raw','rolling'] = 'raw'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Detect anomalies based on prediction error exceeding a threshold.\n",
    "    Optionally applies a rolling-median smoothing to detect slow deviations.\n",
    "    Requires a minimum number of consecutive violations (waiting_time)\n",
    "    before confirming an anomaly.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "        Ground-truth target values.\n",
    "    y_pred : array-like\n",
    "        Model predictions.\n",
    "    threshold : float\n",
    "        Error value above which a point is considered suspicious.\n",
    "    waiting_time : int\n",
    "        Number of consecutive threshold violations needed to flag anomaly.\n",
    "    rolling_window : int\n",
    "        Window size for rolling-median smoothing (used when error_type='rolling').\n",
    "    error_type : {'raw','rolling'}, default='raw'\n",
    "        - 'raw'     : use absolute error directly\n",
    "        - 'rolling' : use rolling median of absolute error to detect slow drift\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    anomalies : np.ndarray of bool\n",
    "        Boolean array marking anomaly positions.\n",
    "    \"\"\"\n",
    "\n",
    "    # absolute error\n",
    "    error = np.abs(np.asarray(y_true) - np.asarray(y_pred))/np.asarray(y_true) * 100\n",
    "\n",
    "    if error_type == 'rolling':\n",
    "        # rolling median (slow drift detection)\n",
    "        error = (\n",
    "            pd.Series(error)\n",
    "            .rolling(rolling_window, min_periods=1)\n",
    "            .median()\n",
    "            .to_numpy()\n",
    "        )\n",
    "\n",
    "    above = error > threshold\n",
    "    anomalies = np.zeros_like(above, dtype=bool)\n",
    "\n",
    "    # consecutive violations\n",
    "    count = 0\n",
    "    for i, flag in enumerate(above):\n",
    "        count = count + 1 if flag else 0\n",
    "        if count >= waiting_time:\n",
    "            anomalies[i] = True\n",
    "\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4d67f-28c7-4867-9461-c9550816aa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make delay 1 days: 10 mins * 6 points * 24\n",
    "anomalies = detect_anomaly(\n",
    "    y_true=y_test, \n",
    "    y_pred=eval_results['y_pred_test'],\n",
    "    threshold=8.5,\n",
    "    waiting_time=72, \n",
    "    rolling_window=288, \n",
    "    error_type='rolling'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc27ce3-f4b7-4172-b644-257d7e13b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0d88c-9639-415f-a534-61495daf76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_with_anomalies(\n",
    "    x_true,\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    error,\n",
    "    error_threshold,\n",
    "    rolling_window,\n",
    "    anomalies=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot prediction errors with detected anomalies and threshold line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_true : pd.DataFrame\n",
    "        Original input data (with index aligned to y_true).\n",
    "    y_true : pd.Series or pd.DataFrame\n",
    "        True target values (must contain 'Power' column if DataFrame).\n",
    "    y_pred : array-like\n",
    "        Predicted target values (same length as y_true).\n",
    "    error : str\n",
    "        Type of error to visualize. Either 'mae' or 'mape'.\n",
    "    error_threshold : float\n",
    "        Static threshold value to draw as a horizontal line.\n",
    "    rolling_window: int\n",
    "        Rolling windown for the error\n",
    "    anomalies : array-like of bool, optional\n",
    "        Boolean mask (same length as y_true) where True indicates anomaly.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Build y_test_err DataFrame with 'Power' + 'Error' ---\n",
    "\n",
    "    # Ensure y_true is DataFrame with column 'Power'\n",
    "    if isinstance(y_true, pd.Series):\n",
    "        y_test_err = y_true.to_frame(name='Power').copy()\n",
    "    else:\n",
    "        y_test_err = y_true[['Power']].copy()\n",
    "\n",
    "    # Compute error\n",
    "    if error == \"mae\":\n",
    "        y_test_err['Error'] = (y_test_err['Power'] - y_pred).abs()\n",
    "    elif error == 'mape':\n",
    "        y_test_err['Error'] = (y_test_err['Power'] - y_pred).abs() / (y_test_err['Power'] + 1e-8) * 100\n",
    "    else:\n",
    "        raise ValueError(\"error must be 'mae' or 'mape'\")\n",
    "\n",
    "    # --- Handle anomalies mask robustly ---\n",
    "\n",
    "    if anomalies is not None:\n",
    "        anomalies = np.asarray(anomalies).astype(bool)\n",
    "\n",
    "        # Align length by trimming from the front if needed\n",
    "        if len(anomalies) > len(y_test_err):\n",
    "            anomalies = anomalies[-len(y_test_err):]\n",
    "        elif len(anomalies) < len(y_test_err):\n",
    "            # pad with False at the beginning\n",
    "            pad = len(y_test_err) - len(anomalies)\n",
    "            anomalies = np.concatenate([np.zeros(pad, dtype=bool), anomalies])\n",
    "\n",
    "        # Now lengths match exactly\n",
    "        y_test_err['Anomaly'] = anomalies\n",
    "    else:\n",
    "        y_test_err['Anomaly'] = False\n",
    "\n",
    "    print(f\"Number of anomalies: {y_test_err['Anomaly'].sum()}\")\n",
    "\n",
    "    # --- Build figure ---\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # 1) Error line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=y_test_err.index,\n",
    "        y=y_test_err['Error'].values,\n",
    "        mode='lines',\n",
    "        name='Error',\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "\n",
    "    # 2) Threshold line\n",
    "    fig.add_hline(\n",
    "        y=error_threshold,\n",
    "        line_color=\"black\",\n",
    "        annotation_text=\"Upper Threshold\",\n",
    "        annotation_position=\"bottom right\"\n",
    "    )\n",
    "\n",
    "    # 3) Rolling median error\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=y_test_err.index,\n",
    "        y=y_test_err['Error'].rolling(rolling_window, min_periods=1).median(),\n",
    "        mode='lines',\n",
    "        name='Rolling Median Error',\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "\n",
    "    # 4) Anomaly markers\n",
    "    anomaly_mask = y_test_err['Anomaly']\n",
    "    if anomaly_mask.any():\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=y_test_err.index[anomaly_mask],\n",
    "            y=y_test_err.loc[anomaly_mask, 'Error'].rolling(rolling_window, min_periods=1).median(),\n",
    "            mode='markers',\n",
    "            name='Anomaly',\n",
    "            marker=dict(size=8, color='red', symbol='circle-open')\n",
    "        ))\n",
    "\n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        title='Prediction Error and Anomaly Detection',\n",
    "        xaxis_title='Index',\n",
    "        yaxis_title='Error',\n",
    "        legend=dict(x=0, y=1),\n",
    "        height=500,\n",
    "        template='plotly_white'\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2e9b5-c9ea-48e5-9eb2-623670d82c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 8.5\n",
    "rolling_window = 288\n",
    "waiting_time = 72\n",
    "\n",
    "anomalies = detect_anomaly(y_test, eval_results['y_pred_test'], threshold, waiting_time, rolling_window, error_type='rolling')\n",
    "\n",
    "plot_prediction_with_anomalies(\n",
    "    x_true=x_test,\n",
    "    y_true=y_test,\n",
    "    y_pred=eval_results['y_pred_test'],\n",
    "    error='mape',\n",
    "    error_threshold=threshold,\n",
    "    rolling_window=rolling_window,\n",
    "    anomalies=anomalies\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42f925-75ee-4138-8f1c-da7f0b5f386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_timestamps = y_test.iloc[anomalies].index\n",
    "detected_anomaly_start = anomaly_timestamps[0]\n",
    "stoppage_time = pd.Timestamp('2008-06-05 15:00:00')\n",
    "stoppage_time - detected_anomaly_start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c19ae65-ccad-4d99-ad36-49b98cf79bef",
   "metadata": {},
   "source": [
    "So, we see that with out baseline model, we can **ROBUSTLY** estimate the anomaly with Time-to-Failure of 19.5 days. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d02779-c4c7-4328-9ef7-01b2a11621e4",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795bdec-4aed-4688-ba3e-1feb98fe45d6",
   "metadata": {},
   "source": [
    "1. We created a good baseline model AND approach for detecting anomalies based on Random Forest Regression model.\n",
    "2. We improved model performance through basic data cleaning of features and feature engineering.\n",
    "3. The estimated Time-to-Failure on the test set is 19.5 days.\n",
    "4. Data cleaning and basic feature engineering improves the model performance a lot comapred to the model fitted on the raw data.\n",
    "5. The linear model got bad performance, so we did not proceed with it.\n",
    "6. The next step is to find the best model and start registering hyperparameter tuning and model performance in MLflow for reproducibility. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
