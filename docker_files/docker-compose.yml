version: '3.8'

services:
  # =============================================================================
  # MLFLOW TRACKING SERVER
  # =============================================================================
  # This service provides MLflow for experiment tracking and model registry
  # Uses local file storage only (no database, stores everything in files)
  mlflow:
    build:
      context: ..                    # Build context is the project root (parent of docker_files/)
      dockerfile: docker_files/Dockerfile.mlflow
    command: >
      sh -c "mlflow server 
      --host 0.0.0.0 
      --port 5001 
      --default-artifact-root file:///app/mlflow/mlartifacts
      --allowed-hosts mlflow,mlflow:5001,localhost,localhost:5001,127.0.0.1,127.0.0.1:5001,0.0.0.0,0.0.0.0:5001
      --cors-allowed-origins '*'"
    ports:
      - "5001:5001"                 # Expose port 5001 for MLflow UI (5000 is used by macOS AirPlay)
    volumes:
      # Mount shared folders to persist MLflow data locally
      - ../mlflow:/app/mlflow       # MLflow runs and artifacts (file-based storage)
      - ../data:/app/data           # Access to data if needed
      - ../conf:/app/conf           # Configuration files
    networks:
      - ml-app-network
    restart: unless-stopped

  # =============================================================================
  # ML MODEL TRAINING SERVICE
  # =============================================================================
  # This service trains the ML model using the training data
  # It runs once to create the model, then the inference service uses it
  app-ml-train:
    build:
      context: ..                    # Build context is the project root
      dockerfile: docker_files/Dockerfile.ml
    command: ["python", "entrypoint/training.py"]  # Run the training script
    volumes:
      # Mount shared folders so data persists between container restarts
      - ../data:/app/data           # Training and production data
      - ../conf:/app/conf           # Configuration files
      - ../mlflow:/app/mlflow       # MLflow tracking and artifacts
      - ../src:/app/src             # Source code
      - ../entrypoint:/app/entrypoint # Entrypoint scripts
    environment:
      - KEDRO_ENV=local
      - MLFLOW_TRACKING_URI=http://mlflow:5001
    networks:
      - ml-app-network
    restart: "no"                   # Only run once, not continuously
    depends_on:
      - mlflow                      # Wait for MLflow to be ready

  # =============================================================================
  # ML INFERENCE SERVICE
  # =============================================================================
  # This service runs inference on new data after training completes
  # It monitors for new data and runs inference pipeline automatically
  app-ml-inference:
    build:
      context: ..                    # Build context is the project root
      dockerfile: docker_files/Dockerfile.ml
    command: ["python", "entrypoint/inference_real_time.py"]  # Run inference on stream data
    volumes:
      # Mount shared folders to access data, models, and config
      - ../data:/app/data           # Production data and predictions
      - ../conf:/app/conf           # Configuration files
      - ../mlflow:/app/mlflow       # MLflow tracking and model registry
      - ../src:/app/src             # Source code
      - ../entrypoint:/app/entrypoint # Entrypoint scripts
    environment:
      - KEDRO_ENV=local
      - MLFLOW_TRACKING_URI=http://mlflow:5001
    networks:
      - ml-app-network
    restart: unless-stopped
    depends_on:
      - mlflow                      # Wait for MLflow to be ready
      - app-ml-train                # Wait for training to complete first

  # =============================================================================
  # KEDRO VISUALIZATION SERVER
  # =============================================================================
  # This service provides Kedro Viz for pipeline visualization
  # It shows the data pipeline structure and dependencies
  kedro-viz:
    build:
      context: ..                    # Build context is the project root
      dockerfile: docker_files/Dockerfile.ml  # Uses ML Dockerfile (includes Kedro)
    command: ["kedro", "viz", "--host", "0.0.0.0", "--port", "4141"]
    ports:
      - "4141:4141"                 # Expose port 4141 for Kedro Viz UI
    volumes:
      - ..:/app                     # Mount entire project for Kedro Viz
    working_dir: /app
    environment:
      - KEDRO_ENV=local
    networks:
      - ml-app-network
    restart: unless-stopped
    depends_on:
      - mlflow                      # Wait for MLflow to be ready

  # =============================================================================
  # DATA STREAMING SERVICE
  # =============================================================================
  # This service streams data point-by-point to the database
  # It simulates real-time data ingestion for inference
  app-stream-data:
    build:
      context: ..                    # Build context is the project root
      dockerfile: docker_files/Dockerfile.data
    command: ["python", "entrypoint/app_stream_data.py"]  # Run the streaming script
    volumes:
      # Mount shared folders to access data and config
      - ../data:/app/data           # Production data and database
      - ../conf:/app/conf           # Configuration files
      - ../src:/app/src             # Source code
      - ../entrypoint:/app/entrypoint # Entrypoint scripts
    environment:
      - KEDRO_ENV=local
    networks:
      - ml-app-network
    restart: unless-stopped
    depends_on:
      - mlflow                      # Wait for MLflow to be ready

  # =============================================================================
  # WEB USER INTERFACE SERVICE
  # =============================================================================
  # This service provides the web dashboard for visualizing predictions
  # It displays real-time predictions, error metrics, and model information
  app-ui:
    build:
      context: ..                    # Build context is the project root
      dockerfile: docker_files/Dockerfile.ui
    command: ["python", "entrypoint/app_ui.py"]  # Run the Dash UI app
    ports:
      - "8050:8050"                 # Expose port 8050 for web access
    volumes:
      # Mount shared folders to access data and config
      - ../data:/app/data           # Access to production data and predictions
      - ../conf:/app/conf           # Configuration files
      - ../src:/app/src             # Source code
      - ../mlflow:/app/mlflow       # MLflow data for model info
    environment:
      - KEDRO_ENV=local
      - MLFLOW_TRACKING_URI=http://mlflow:5001
      - DEBUG=False                 # Disable debug mode in production
    networks:
      - ml-app-network
    restart: unless-stopped
    depends_on:
      - mlflow                      # Wait for MLflow to be ready

# =============================================================================
# NETWORK CONFIGURATION
# =============================================================================
networks:
  ml-app-network:
    driver: bridge                  # Bridge network for service communication

# =============================================================================
# DOCKER COMPOSE EXPLANATION
# =============================================================================
# This file defines 6 services with optimized Dockerfiles:
#
# 1. mlflow: Uses Dockerfile.mlflow (minimal - just MLflow server)
# 2. app-ml-train: Uses Dockerfile.ml (ML stack - Kedro, MLflow, CatBoost, etc.)
# 3. app-ml-inference: Uses Dockerfile.ml (ML stack)
# 4. app-stream-data: Uses Dockerfile.data (minimal - just pandas)
# 5. app-ui: Uses Dockerfile.ui (Dash, MLflow, plotly)
# 6. kedro-viz: Uses Dockerfile.ml (needs Kedro)
#
# Benefits:
# - Each service only installs the dependencies it needs
# - Faster builds (smaller images)
# - Smaller image sizes (more efficient)
#
# To use:
# 1. First, add the optional dependencies from pyproject.toml.optional-deps to your main pyproject.toml
# 2. Run: uv lock (to update the lock file with new optional dependencies)
# 3. From project root: docker-compose -f docker_files/docker-compose.yml up
# 4. Or from docker_files/: docker-compose -f docker-compose.yml up (context is parent dir)
#
# Key concepts:
# - volumes: Shared folders between host and containers (use ../ for parent dir)
# - ports: Expose container ports to host machine
# - depends_on: Service startup order
# - environment: Configuration variables for Docker networking
# - networks: Allows services to communicate using service names
#
# To run: docker-compose -f docker_files/docker-compose.yml up
# To stop: docker-compose -f docker_files/docker-compose.yml down
# To rebuild: docker-compose -f docker_files/docker-compose.yml up --build


